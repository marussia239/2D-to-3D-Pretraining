{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e11ee1-790d-47c3-841e-5fc237a45b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marus\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\marus\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "C:\\Users\\marus\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23bf741b-622a-4b47-ac2b-0cba71bf9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(file_path, has_auc=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if has_auc:\n",
    "        metrics = {\n",
    "        'accuracy': df.loc[df.index[-4], 'precision'],\n",
    "        'precision': df.loc[df.index[-3], 'precision'],\n",
    "        'recall': df.loc[df.index[-3], 'recall'],\n",
    "        'f1_macro': df.loc[df.index[-3], 'f1-score'],\n",
    "        'auc': df.loc[df.index[-1], 'precision']\n",
    "    }\n",
    "    else:\n",
    "        metrics = {\n",
    "        'accuracy': df.loc[df.index[-3], 'precision'],\n",
    "        'precision': df.loc[df.index[-2], 'precision'],\n",
    "        'recall': df.loc[df.index[-2], 'recall'],\n",
    "        'f1_macro': df.loc[df.index[-2], 'f1-score']\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd1a8e19-8ce4-4bce-bfcb-62f39cb553a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_random_pretrained', 'coronal_random_pretrained', \n",
    "               'sagittal_random_pretrained', 'combined_pretrained_random',\n",
    "               'weighted_pretrained_random', 'balanced_pretrained_random']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"reports/{model}*.csv\") for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "431a467a-dde2-4512-a317-9a09d8f2966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_pretrained_random', \n",
    "                                                           'weighted_pretrained_random',\n",
    "                                                           'balanced_pretrained_random']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'recall', 'precision', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-3]],\n",
    "        'combined': summary['combined_pretrained_random']['mean'][metric],\n",
    "        'weighted': summary['weighted_pretrained_random']['mean'][metric],\n",
    "        'balanced': summary['balanced_pretrained_random']['mean'][metric]\n",
    "    }\n",
    "    \n",
    "\n",
    "if 'auc' in summary['combined_pretrained_random']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_pretrained_random']['mean']['auc'],\n",
    "        'weighted': summary['weighted_pretrained_random']['mean']['auc'],\n",
    "        'balanced': summary['balanced_pretrained_random']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7164808f-db41-4d06-ac27-413628dd4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_random_pretrained\n",
      "Mean:\n",
      "accuracy     0.831148\n",
      "precision    0.854023\n",
      "recall       0.850242\n",
      "f1_macro     0.848143\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.014965\n",
      "precision    0.009397\n",
      "recall       0.009318\n",
      "f1_macro     0.009477\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_random_pretrained\n",
      "Mean:\n",
      "accuracy     0.888934\n",
      "precision    0.901600\n",
      "recall       0.897297\n",
      "f1_macro     0.897999\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.008399\n",
      "precision    0.010795\n",
      "recall       0.010027\n",
      "f1_macro     0.010279\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_random_pretrained\n",
      "Mean:\n",
      "accuracy     0.761066\n",
      "precision    0.783351\n",
      "recall       0.765608\n",
      "f1_macro     0.769336\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.017369\n",
      "precision    0.017706\n",
      "recall       0.020716\n",
      "f1_macro     0.018475\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_pretrained_random\n",
      "Mean:\n",
      "accuracy     0.838525\n",
      "precision    0.861601\n",
      "recall       0.856445\n",
      "f1_macro     0.856498\n",
      "auc          0.987775\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.017631\n",
      "precision    0.015056\n",
      "recall       0.016359\n",
      "f1_macro     0.015652\n",
      "auc          0.001097\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_pretrained_random\n",
      "Mean:\n",
      "accuracy     0.839754\n",
      "precision    0.863930\n",
      "recall       0.854693\n",
      "f1_macro     0.856928\n",
      "auc          0.987300\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.006042\n",
      "precision    0.006838\n",
      "recall       0.008757\n",
      "f1_macro     0.006970\n",
      "auc          0.000804\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: balanced_pretrained_random\n",
      "Mean:\n",
      "accuracy     0.795902\n",
      "precision    0.836142\n",
      "recall       0.817724\n",
      "f1_macro     0.818363\n",
      "auc          0.980725\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.007392\n",
      "precision    0.007429\n",
      "recall       0.005157\n",
      "f1_macro     0.009390\n",
      "auc          0.000988\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8311475409836065, 0.8889344262295082, 0.7610655737704918]\n",
      "Combined: 0.8385245901639344\n",
      "Weighted: 0.8397540983606557\n",
      "Balanced: 0.7959016393442622\n",
      "========================================\n",
      "Metric: recall\n",
      "Before combined: [0.8502423756201948, 0.8972971692339281, 0.7656080085921984]\n",
      "Combined: 0.8564454881706685\n",
      "Weighted: 0.8546926567968186\n",
      "Balanced: 0.8177235623788751\n",
      "========================================\n",
      "Metric: precision\n",
      "Before combined: [0.8540234167016392, 0.9015996369893889, 0.7833512166518001]\n",
      "Combined: 0.8616010045486427\n",
      "Weighted: 0.8639299228784166\n",
      "Balanced: 0.8361420412376528\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8481434776334118, 0.8979991384689163, 0.7693357339812326]\n",
      "Combined: 0.8564983311577257\n",
      "Weighted: 0.8569284833105637\n",
      "Balanced: 0.8183634542462519\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9877750000000001\n",
      "Weighted: 0.9873000000000001\n",
      "Balanced: 0.9807250000000001\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(f\"Balanced: {values['balanced']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4121534-a549-41fc-8b64-ff2c64ce46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_random_unpretrained', 'coronal_random_unpretrained', \n",
    "               'sagittal_random_unpretrained', 'combined_unpretrained_random',\n",
    "               'weighted_unpretrained_random', 'balanced_unpretrained_random']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"reports/{model}*.csv\")\n",
    "    for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8362e97c-a5a7-44f6-be9c-b61f36d7ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_unpretrained_random', \n",
    "                                                           'weighted_unpretrained_random',\n",
    "                                                           'balanced_unpretrained_random']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'recall', 'precision', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-3]],\n",
    "        'combined': summary['combined_unpretrained_random']['mean'][metric],\n",
    "        'weighted': summary['weighted_unpretrained_random']['mean'][metric],\n",
    "        'balanced': summary['balanced_unpretrained_random']['mean'][metric]\n",
    "    }\n",
    "\n",
    "if 'auc' in summary['combined_unpretrained_random']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_unpretrained_random']['mean']['auc'],\n",
    "        'weighted': summary['weighted_unpretrained_random']['mean']['auc'],\n",
    "        'balanced': summary['weighted_unpretrained_random']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f504866a-951f-4120-9576-213f5b607de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_random_unpretrained\n",
      "Mean:\n",
      "accuracy     0.829098\n",
      "precision    0.855832\n",
      "recall       0.848292\n",
      "f1_macro     0.846654\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.007733\n",
      "precision    0.010961\n",
      "recall       0.009560\n",
      "f1_macro     0.012129\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_random_unpretrained\n",
      "Mean:\n",
      "accuracy     0.246721\n",
      "precision    0.267198\n",
      "recall       0.241287\n",
      "f1_macro     0.225915\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.007146\n",
      "precision    0.047879\n",
      "recall       0.017088\n",
      "f1_macro     0.025552\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_random_unpretrained\n",
      "Mean:\n",
      "accuracy     0.252049\n",
      "precision    0.285371\n",
      "recall       0.224872\n",
      "f1_macro     0.216803\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.034319\n",
      "precision    0.080010\n",
      "recall       0.033514\n",
      "f1_macro     0.036888\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_unpretrained_random\n",
      "Mean:\n",
      "accuracy     0.924180\n",
      "precision    0.936605\n",
      "recall       0.934197\n",
      "f1_macro     0.934447\n",
      "auc          0.996925\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.010486\n",
      "precision    0.007716\n",
      "recall       0.010338\n",
      "f1_macro     0.009663\n",
      "auc          0.000680\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_unpretrained_random\n",
      "Mean:\n",
      "accuracy     0.923770\n",
      "precision    0.936535\n",
      "recall       0.932960\n",
      "f1_macro     0.933876\n",
      "auc          0.996875\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.008087\n",
      "precision    0.006456\n",
      "recall       0.008543\n",
      "f1_macro     0.007320\n",
      "auc          0.000395\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: balanced_unpretrained_random\n",
      "Mean:\n",
      "accuracy     0.918033\n",
      "precision    0.931207\n",
      "recall       0.930212\n",
      "f1_macro     0.930024\n",
      "auc          0.995700\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.004637\n",
      "precision    0.004997\n",
      "recall       0.004506\n",
      "f1_macro     0.004700\n",
      "auc          0.000821\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8290983606557377, 0.24672131147540982, 0.2520491803278688]\n",
      "Combined: 0.9241803278688525\n",
      "Weighted: 0.9237704918032786\n",
      "Balanced: 0.918032786885246\n",
      "========================================\n",
      "Metric: recall\n",
      "Before combined: [0.8482918161045341, 0.24128742198667796, 0.2248721072773619]\n",
      "Combined: 0.9341971211768934\n",
      "Weighted: 0.9329595305879892\n",
      "Balanced: 0.930211647803487\n",
      "========================================\n",
      "Metric: precision\n",
      "Before combined: [0.85583201221502, 0.2671978481688286, 0.28537058279376293]\n",
      "Combined: 0.9366046611584831\n",
      "Weighted: 0.9365348716544405\n",
      "Balanced: 0.931206840747038\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8466536139951888, 0.22591515849863475, 0.21680279255528]\n",
      "Combined: 0.9344469813170645\n",
      "Weighted: 0.9338755591922137\n",
      "Balanced: 0.9300242024985398\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9969250000000001\n",
      "Weighted: 0.9968750000000001\n",
      "Balanced: 0.9968750000000001\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(f\"Balanced: {values['balanced']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4a4d7-ab77-455d-80d3-f16df7cbbf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d9555-958f-4cbb-8663-8885247618e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9adb059-0c2b-4134-8efd-87335f3102eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_full_pretrained', 'coronal_full_pretrained', \n",
    "               'sagittal_full_pretrained', 'combined_pretrained_full',\n",
    "               'weighted_pretrained_full', 'balanced_pretrained_full']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"reports/{model}*.csv\") for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18af7eda-412b-448c-823f-5e9c5d3eb00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'axial_full_unpretrained':    accuracy  precision    recall  f1_macro\n",
       " 0  0.821311   0.841906  0.848408  0.841066\n",
       " 1  0.896721   0.916842  0.907353  0.909124\n",
       " 2  0.806557   0.828554  0.838548  0.826866\n",
       " 3  0.888525   0.908622  0.900943  0.903817\n",
       " 4  0.814754   0.825987  0.843088  0.825980,\n",
       " 'coronal_full_unpretrained':    accuracy  precision    recall  f1_macro\n",
       " 0  0.247541   0.265856  0.232274  0.193288\n",
       " 1  0.242623   0.221216  0.219676  0.190706\n",
       " 2  0.204918   0.173563  0.176822  0.144535\n",
       " 3  0.265574   0.212840  0.256392  0.212830\n",
       " 4  0.277049   0.296502  0.275583  0.260774,\n",
       " 'sagittal_full_unpretrained':    accuracy  precision    recall  f1_macro\n",
       " 0  0.540984   0.558642  0.518659  0.514355\n",
       " 1  0.557377   0.545211  0.520494  0.501937\n",
       " 2  0.580328   0.559684  0.567404  0.550104\n",
       " 3  0.478689   0.497995  0.433547  0.428878\n",
       " 4  0.583607   0.600653  0.581673  0.580001,\n",
       " 'combined_unpretrained_full':    accuracy  precision    recall  f1_macro     auc\n",
       " 0  0.852459   0.877665  0.872523  0.873026  0.9907\n",
       " 1  0.895082   0.911137  0.902403  0.903059  0.9937\n",
       " 2  0.877049   0.898884  0.894839  0.895207  0.9904\n",
       " 3  0.891803   0.910660  0.905249  0.907279  0.9928\n",
       " 4  0.842623   0.867189  0.860698  0.862339  0.9900,\n",
       " 'weighted_unpretrained_full':    accuracy  precision    recall  f1_macro     auc\n",
       " 0  0.852459   0.876786  0.869528  0.871410  0.9885\n",
       " 1  0.888525   0.901564  0.899058  0.898745  0.9932\n",
       " 2  0.862295   0.885550  0.877574  0.880096  0.9890\n",
       " 3  0.878689   0.896977  0.892258  0.892137  0.9927\n",
       " 4  0.842623   0.862604  0.860716  0.859942  0.9871,\n",
       " 'balanced_unpretrained_full':    accuracy  precision    recall  f1_macro    auc\n",
       " 0  0.881967   0.903693  0.899312  0.900677  0.992}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b8b278a-a492-4e89-93bf-956f1549f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_pretrained_full', \n",
    "                                                           'weighted_pretrained_full',\n",
    "                                                           'balanced_pretrained_full']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'recall', 'precision', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-3]],\n",
    "        'combined': summary['combined_pretrained_full']['mean'][metric],\n",
    "        'weighted': summary['weighted_pretrained_full']['mean'][metric],\n",
    "        'balanced': summary['balanced_pretrained_full']['mean'][metric]\n",
    "    }\n",
    "    \n",
    "\n",
    "if 'auc' in summary['combined_pretrained_full']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_pretrained_full']['mean']['auc'],\n",
    "        'weighted': summary['weighted_pretrained_full']['mean']['auc'],\n",
    "        'balanced': summary['balanced_pretrained_full']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08e7d714-a16a-48b4-beff-52ac0e80b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_full_pretrained\n",
      "Mean:\n",
      "accuracy     0.871475\n",
      "precision    0.891261\n",
      "recall       0.891058\n",
      "f1_macro     0.888182\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.016981\n",
      "precision    0.018751\n",
      "recall       0.013736\n",
      "f1_macro     0.016026\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_full_pretrained\n",
      "Mean:\n",
      "accuracy     0.781311\n",
      "precision    0.808118\n",
      "recall       0.809013\n",
      "f1_macro     0.805876\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.014083\n",
      "precision    0.016007\n",
      "recall       0.018552\n",
      "f1_macro     0.017650\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_full_pretrained\n",
      "Mean:\n",
      "accuracy     0.760328\n",
      "precision    0.796643\n",
      "recall       0.785522\n",
      "f1_macro     0.784956\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.025381\n",
      "precision    0.024293\n",
      "recall       0.023968\n",
      "f1_macro     0.023288\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_pretrained_full\n",
      "Mean:\n",
      "accuracy     0.822951\n",
      "precision    0.854209\n",
      "recall       0.846026\n",
      "f1_macro     0.845538\n",
      "auc          0.985640\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.012213\n",
      "precision    0.013733\n",
      "recall       0.013049\n",
      "f1_macro     0.012565\n",
      "auc          0.002990\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_pretrained_full\n",
      "Mean:\n",
      "accuracy     0.824590\n",
      "precision    0.858830\n",
      "recall       0.845256\n",
      "f1_macro     0.846261\n",
      "auc          0.985380\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.016678\n",
      "precision    0.017338\n",
      "recall       0.016006\n",
      "f1_macro     0.016605\n",
      "auc          0.003168\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: balanced_pretrained_full\n",
      "Mean:\n",
      "accuracy     0.800000\n",
      "precision    0.830714\n",
      "recall       0.828057\n",
      "f1_macro     0.822988\n",
      "auc          0.971900\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    NaN\n",
      "precision   NaN\n",
      "recall      NaN\n",
      "f1_macro    NaN\n",
      "auc         NaN\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8714754098360654, 0.781311475409836, 0.7603278688524591]\n",
      "Combined: 0.8229508196721312\n",
      "Weighted: 0.8245901639344263\n",
      "Balanced: 0.8\n",
      "========================================\n",
      "Metric: recall\n",
      "Before combined: [0.8910583913490215, 0.8090127132419622, 0.785522202357822]\n",
      "Combined: 0.8460256189598203\n",
      "Weighted: 0.8452563889648289\n",
      "Balanced: 0.8280569426698233\n",
      "========================================\n",
      "Metric: precision\n",
      "Before combined: [0.8912607525837732, 0.8081175348293096, 0.7966426530672033]\n",
      "Combined: 0.8542090205971489\n",
      "Weighted: 0.8588303904176033\n",
      "Balanced: 0.8307137433471865\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8881820450709098, 0.8058760445067431, 0.7849564386177429]\n",
      "Combined: 0.8455383666294487\n",
      "Weighted: 0.8462608344596955\n",
      "Balanced: 0.8229884341176077\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9856400000000001\n",
      "Weighted: 0.9853799999999999\n",
      "Balanced: 0.9719\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(f\"Balanced: {values['balanced']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c5cbe8a-ef5b-4a4d-a95b-d31d4dc6d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_full_unpretrained', 'coronal_full_unpretrained', \n",
    "               'sagittal_full_unpretrained', 'combined_unpretrained_full',\n",
    "               'weighted_unpretrained_full', 'balanced_unpretrained_full']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"reports/{model}*.csv\")\n",
    "    for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41629d0b-9eaf-42b5-ba08-f74811ca48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_unpretrained_full', \n",
    "                                                           'weighted_unpretrained_full',\n",
    "                                                           'balanced_unpretrained_full']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'recall', 'precision', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-3]],\n",
    "        'combined': summary['combined_unpretrained_full']['mean'][metric],\n",
    "        'weighted': summary['weighted_unpretrained_full']['mean'][metric],\n",
    "        'balanced': summary['balanced_unpretrained_full']['mean'][metric]\n",
    "    }\n",
    "\n",
    "if 'auc' in summary['combined_unpretrained_full']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_unpretrained_full']['mean']['auc'],\n",
    "        'weighted': summary['weighted_unpretrained_full']['mean']['auc'],\n",
    "        'balanced': summary['weighted_unpretrained_full']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e433c88-9aa0-4de6-bf80-a4f81cdab85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_full_unpretrained\n",
      "Mean:\n",
      "accuracy     0.845574\n",
      "precision    0.864382\n",
      "recall       0.867668\n",
      "f1_macro     0.861371\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.043364\n",
      "precision    0.044644\n",
      "recall       0.033561\n",
      "f1_macro     0.041646\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_full_unpretrained\n",
      "Mean:\n",
      "accuracy     0.247541\n",
      "precision    0.233996\n",
      "recall       0.232149\n",
      "f1_macro     0.200427\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.027554\n",
      "precision    0.047920\n",
      "recall       0.037719\n",
      "f1_macro     0.042027\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_full_unpretrained\n",
      "Mean:\n",
      "accuracy     0.548197\n",
      "precision    0.552437\n",
      "recall       0.524355\n",
      "f1_macro     0.515055\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.042585\n",
      "precision    0.036851\n",
      "recall       0.057948\n",
      "f1_macro     0.057108\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_unpretrained_full\n",
      "Mean:\n",
      "accuracy     0.871803\n",
      "precision    0.893107\n",
      "recall       0.887142\n",
      "f1_macro     0.888182\n",
      "auc          0.991520\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.023426\n",
      "precision    0.019854\n",
      "recall       0.019577\n",
      "f1_macro     0.019576\n",
      "auc          0.001630\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_unpretrained_full\n",
      "Mean:\n",
      "accuracy     0.864918\n",
      "precision    0.884696\n",
      "recall       0.879827\n",
      "f1_macro     0.880466\n",
      "auc          0.990100\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy     0.018749\n",
      "precision    0.015706\n",
      "recall       0.015817\n",
      "f1_macro     0.015602\n",
      "auc          0.002699\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: balanced_unpretrained_full\n",
      "Mean:\n",
      "accuracy     0.881967\n",
      "precision    0.903693\n",
      "recall       0.899312\n",
      "f1_macro     0.900677\n",
      "auc          0.992000\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    NaN\n",
      "precision   NaN\n",
      "recall      NaN\n",
      "f1_macro    NaN\n",
      "auc         NaN\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8455737704918033, 0.24754098360655732, 0.5481967213114753]\n",
      "Combined: 0.8718032786885246\n",
      "Weighted: 0.8649180327868853\n",
      "Balanced: 0.8819672131147541\n",
      "========================================\n",
      "Metric: recall\n",
      "Before combined: [0.8676680794211602, 0.23214938901290907, 0.5243553338856757]\n",
      "Combined: 0.8871423252404419\n",
      "Weighted: 0.8798265417447004\n",
      "Balanced: 0.8993115347800306\n",
      "========================================\n",
      "Metric: precision\n",
      "Before combined: [0.8643820662538417, 0.23399571236439765, 0.5524371624766047]\n",
      "Combined: 0.8931071484039329\n",
      "Weighted: 0.8846962053890379\n",
      "Balanced: 0.9036926901679038\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8613705475407917, 0.20042662410844864, 0.5150549274888242]\n",
      "Combined: 0.88818205740521\n",
      "Weighted: 0.8804660265944928\n",
      "Balanced: 0.9006767719725318\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9915200000000001\n",
      "Weighted: 0.9901\n",
      "Balanced: 0.9901\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(f\"Balanced: {values['balanced']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d68ae-7fa3-40a7-a3d0-8bf474d0fd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
