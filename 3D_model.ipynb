{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Classification with MONAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2441\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: cf815ed4e44a5b8ce67e894ab0bc2765279a1a59\n",
      "MONAI __file__: /mnt/hdd/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.17.1\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.46.2\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import shutil\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import decollate_batch, DataLoader, Dataset\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121, resnet, resnet18\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    RandGaussianNoise,\n",
    "    RandAdjustContrast,\n",
    "    ScaleIntensity, \n",
    "    Transform,\n",
    "    ToTensor,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue working with OrganMNIST3d 64x64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'organmnist3d'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', download=download, size=64)\n",
    "val_dataset = DataClass(split='val', download=download, size=64)\n",
    "test_dataset = DataClass(split='test', download=download, size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/hdd/marina/.medmnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset OrganMNIST3D of size 64 (organmnist3d_64)\n",
       "    Number of datapoints: 971\n",
       "    Root location: /mnt/hdd/marina/.medmnist\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 1\n",
       "    Meaning of labels: {'0': 'liver', '1': 'kidney-right', '2': 'kidney-left', '3': 'femur-right', '4': 'femur-left', '5': 'bladder', '6': 'heart', '7': 'lung-right', '8': 'lung-left', '9': 'spleen', '10': 'pancreas'}\n",
       "    Number of samples: {'train': 971, 'val': 161, 'test': 610}\n",
       "    Description: The source of the OrganMNIST3D is the same as that of the Organ{A,C,S}MNIST. Instead of 2D images, we directly use the 3D bounding boxes and process the images into 28×28×28 to perform multi-class classification of 11 body organs. The same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the source training set are used as training and validation set, respectively, and the same 70 CT scans as the Organ{A,C,S}MNIST from the source test set are treated as the test set.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),\n",
    "        RandGaussianNoise(prob=0.5, mean=0.0, std=0.05),\n",
    "        RandAdjustContrast(gamma=(0.7, 1.3), prob=0.5),\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        ToTensor(),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "y_pred_trans = Compose([Activations(softmax=True)])\n",
    "y_trans = Compose([AsDiscrete(to_onehot=n_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _3D_Dataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return {'images': data, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ = _3D_Dataset(train_dataset, transform=train_transforms)\n",
    "val_dataset_ = _3D_Dataset(val_dataset, transform=val_transforms)\n",
    "test_dataset_ = _3D_Dataset(test_dataset, transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False, spatial_dims=3, n_input_channels=1, num_classes=n_classes).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.00005)\n",
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:02<?, ?it/s, train_loss=2.45]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<01:22,  2.74s/it, train_loss=2.45]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:03<01:22,  2.74s/it, train_loss=2.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:50,  1.74s/it, train_loss=2.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:04<00:50,  1.74s/it, train_loss=2.19]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:38,  1.36s/it, train_loss=2.19]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:05<00:38,  1.36s/it, train_loss=2.27]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.18s/it, train_loss=2.27]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:31,  1.18s/it, train_loss=2.18]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.18s/it, train_loss=2.18]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.18s/it, train_loss=2.13]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=2.13]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:27,  1.10s/it, train_loss=1.99]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=1.99]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:25,  1.05s/it, train_loss=1.92]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.00it/s, train_loss=1.92]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:22,  1.00it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.02it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:21,  1.02it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:19,  1.07it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:19,  1.07it/s, train_loss=1.6] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:18,  1.10it/s, train_loss=1.6]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:18,  1.10it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:17,  1.09it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:17,  1.09it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.11it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:16,  1.11it/s, train_loss=1.91]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.11it/s, train_loss=1.91]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:15,  1.11it/s, train_loss=1.71]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.08it/s, train_loss=1.71]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:14,  1.08it/s, train_loss=1.46]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.05it/s, train_loss=1.46]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.05it/s, train_loss=1.6] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.03it/s, train_loss=1.6]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.03it/s, train_loss=1.7]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.01s/it, train_loss=1.7]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.01s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=1.4] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.13s/it, train_loss=1.4]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=1.58]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.10s/it, train_loss=1.58]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.10s/it, train_loss=1.29]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.04s/it, train_loss=1.29]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=1.3] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.04s/it, train_loss=1.3]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.05s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.05s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.05s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.05s/it, train_loss=1.38]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.08s/it, train_loss=1.38]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.08s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.07s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=1.11]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.02s/it, train_loss=1.11]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.02s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.05s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=1.85]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.16it/s, train_loss=1.85]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average loss: 1.6798\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [00:34<56:19, 34.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.5794 current accuracy: 0.1118 best AUC: 0.5794 at epoch: 1\n",
      "----------\n",
      "epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.01it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.01it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=1.05]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.05it/s, train_loss=1.05]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.05it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.02it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=1.22]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=1.22]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=1.17]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.11s/it, train_loss=1.17]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.11s/it, train_loss=0.9] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.9]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.11s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=1.02]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.15s/it, train_loss=1.02]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.921]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.17s/it, train_loss=0.921]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.17s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:23,  1.24s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:23,  1.24s/it, train_loss=1.04] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:22,  1.24s/it, train_loss=1.04]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:22,  1.24s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.19s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.19s/it, train_loss=0.955]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.23s/it, train_loss=0.955]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.23s/it, train_loss=0.84] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.20s/it, train_loss=0.84]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.20s/it, train_loss=0.788]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.18s/it, train_loss=0.788]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.18s/it, train_loss=0.751]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.751]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.807]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.807]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.623]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.623]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.07s/it, train_loss=0.722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.04s/it, train_loss=0.739]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.00s/it, train_loss=0.739]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.00s/it, train_loss=0.83] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.83]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.05s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.08s/it, train_loss=0.927]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.927]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.10s/it, train_loss=0.61] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.61]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.18s/it, train_loss=0.501]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.18s/it, train_loss=0.501]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.18s/it, train_loss=0.856]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.17s/it, train_loss=0.856]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.17s/it, train_loss=0.747]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.747]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.15s/it, train_loss=0.829]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.13s/it, train_loss=0.829]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.13s/it, train_loss=0.535]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.07it/s, train_loss=0.535]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average loss: 0.8960\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [01:10<57:45, 35.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.9498 current accuracy: 0.5590 best AUC: 0.9498 at epoch: 2\n",
      "----------\n",
      "epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.945]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.12s/it, train_loss=0.945]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.12s/it, train_loss=0.683]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.14s/it, train_loss=0.683]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.14s/it, train_loss=0.661]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.661]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.863]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.12s/it, train_loss=0.863]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.12s/it, train_loss=0.897]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.12s/it, train_loss=0.897]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.12s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.09s/it, train_loss=0.708]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.16s/it, train_loss=0.708]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.16s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.12s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.12s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.619]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.619]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.09s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.703]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.16s/it, train_loss=0.703]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.16s/it, train_loss=0.868]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.17s/it, train_loss=0.868]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.17s/it, train_loss=0.635]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.16s/it, train_loss=0.635]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.16s/it, train_loss=0.613]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.20s/it, train_loss=0.613]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.20s/it, train_loss=0.636]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:18,  1.33s/it, train_loss=0.636]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:21<00:18,  1.33s/it, train_loss=0.471]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.30s/it, train_loss=0.471]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:16,  1.30s/it, train_loss=0.834]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.24s/it, train_loss=0.834]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.24s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.14s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.14s/it, train_loss=0.854]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.854]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.984]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.01s/it, train_loss=0.984]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.01s/it, train_loss=0.695]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.06it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:07,  1.06it/s, train_loss=0.735]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.07it/s, train_loss=0.735]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:06,  1.07it/s, train_loss=0.604]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.01s/it, train_loss=0.604]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.01s/it, train_loss=0.624]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.624]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:04,  1.03it/s, train_loss=0.645]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.645]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.549]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.13s/it, train_loss=0.549]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.13s/it, train_loss=0.649]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.17s/it, train_loss=0.649]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.17s/it, train_loss=0.693]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.19s/it, train_loss=0.693]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.19s/it, train_loss=0.938]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.05it/s, train_loss=0.938]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 average loss: 0.7023\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 3/100 [01:46<58:04, 35.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 3 current AUC: 0.9904 current accuracy: 0.6957 best AUC: 0.9904 at epoch: 3\n",
      "----------\n",
      "epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.14s/it, train_loss=0.599]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.14s/it, train_loss=0.618]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.21s/it, train_loss=0.618]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.21s/it, train_loss=0.446]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.446]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.599]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.15s/it, train_loss=0.599]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.15s/it, train_loss=0.528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.774]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.774]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.13s/it, train_loss=0.553]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.13s/it, train_loss=0.553]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.13s/it, train_loss=0.878]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.878]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.10s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.15s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.15s/it, train_loss=0.615]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.615]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.659]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.13s/it, train_loss=0.659]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.13s/it, train_loss=0.69] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.69]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.672]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.17s/it, train_loss=0.672]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.17s/it, train_loss=0.571]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.17s/it, train_loss=0.571]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.17s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.16s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.16s/it, train_loss=0.781]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.781]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.08s/it, train_loss=0.946]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.946]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.09s/it, train_loss=0.52] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.06s/it, train_loss=0.52]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.06s/it, train_loss=0.486]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.486]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.07s/it, train_loss=0.862]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.11s/it, train_loss=0.862]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.11s/it, train_loss=0.47] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.13s/it, train_loss=0.47]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.851]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.851]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.875]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.875]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.08s/it, train_loss=0.614]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.614]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.11s/it, train_loss=0.782]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.782]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.573]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.10s/it, train_loss=0.573]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.10s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.06s/it, train_loss=0.674]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.10s/it, train_loss=0.674]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.10s/it, train_loss=0.468]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.12s/it, train_loss=0.468]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.12s/it, train_loss=0.644]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.11it/s, train_loss=0.644]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 average loss: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 4/100 [02:21<56:41, 35.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 4 current AUC: 0.9694 current accuracy: 0.6335 best AUC: 0.9904 at epoch: 3\n",
      "----------\n",
      "epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.702]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.702]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.71] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.16it/s, train_loss=0.71]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.16it/s, train_loss=0.545]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.03s/it, train_loss=0.545]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.03s/it, train_loss=0.584]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.11s/it, train_loss=0.584]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.11s/it, train_loss=0.595]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.13s/it, train_loss=0.595]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.13s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.13s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.09s/it, train_loss=0.434]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.434]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.15s/it, train_loss=0.538]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.15s/it, train_loss=0.538]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.15s/it, train_loss=0.592]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.17s/it, train_loss=0.592]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.17s/it, train_loss=1.21] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.15s/it, train_loss=1.21]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.15s/it, train_loss=0.363]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.363]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.576]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.576]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.05it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.05it/s, train_loss=0.894]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:13,  1.09it/s, train_loss=0.894]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:13,  1.09it/s, train_loss=0.7]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.08it/s, train_loss=0.7]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:12,  1.08it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.06it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.06it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.03it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.03it/s, train_loss=0.382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.01s/it, train_loss=0.382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.614]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.04it/s, train_loss=0.614]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.04it/s, train_loss=0.554]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.02s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.02s/it, train_loss=0.807]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.02it/s, train_loss=0.807]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.02it/s, train_loss=0.667]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.05it/s, train_loss=0.667]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.05it/s, train_loss=0.561]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.09it/s, train_loss=0.561]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.09it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.07it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.07it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.04it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.04it/s, train_loss=0.633]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.07it/s, train_loss=0.633]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.07it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.09it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.09it/s, train_loss=1.09] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.35it/s, train_loss=1.09]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 average loss: 0.6060\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 5/100 [02:53<54:19, 34.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 5 current AUC: 0.9954 current accuracy: 0.8075 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.15it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.15it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.14it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.14it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.17it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.17it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.12it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.12it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.11it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.11it/s, train_loss=0.64] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.08it/s, train_loss=0.64]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.08it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.04it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.04it/s, train_loss=0.442]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:23,  1.04s/it, train_loss=0.442]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.04s/it, train_loss=0.831]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:23,  1.05s/it, train_loss=0.831]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.667]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.15s/it, train_loss=0.667]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.493]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.12s/it, train_loss=0.493]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.442]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.11s/it, train_loss=0.442]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.461]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.12s/it, train_loss=0.461]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.12s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.742]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.12s/it, train_loss=0.742]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.625]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.10s/it, train_loss=0.625]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.10s/it, train_loss=0.675]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.14s/it, train_loss=0.675]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.54] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.11s/it, train_loss=0.54]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.11s/it, train_loss=0.541]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.07s/it, train_loss=0.541]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.689]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.14s/it, train_loss=0.689]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.14s/it, train_loss=0.491]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.10s/it, train_loss=0.491]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.10s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.409]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.12s/it, train_loss=0.409]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.506]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.12s/it, train_loss=0.506]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.12s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.419]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.419]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.02s/it, train_loss=0.65] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.05s/it, train_loss=0.65]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.05s/it, train_loss=0.374]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.07s/it, train_loss=0.374]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.06s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.06s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.552]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.552]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 average loss: 0.5079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 6/100 [03:26<53:02, 33.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 6 current AUC: 0.9858 current accuracy: 0.7143 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.43]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.11s/it, train_loss=0.43]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.11s/it, train_loss=0.421]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.421]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.453]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.02s/it, train_loss=0.453]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.02s/it, train_loss=0.375]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.06it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.06it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.09it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.09it/s, train_loss=0.68] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.04it/s, train_loss=0.68]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.04it/s, train_loss=0.499]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.04it/s, train_loss=0.499]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.04it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.02it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.02it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.02s/it, train_loss=0.24]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.513]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.01it/s, train_loss=0.513]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.01it/s, train_loss=0.445]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.02s/it, train_loss=0.445]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.407]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.407]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.729]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.10s/it, train_loss=0.729]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.495]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.05s/it, train_loss=0.495]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.358]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.12s/it, train_loss=0.358]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.441]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.13s/it, train_loss=0.441]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.10s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.383]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.13s/it, train_loss=0.383]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.13s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.08s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.08s/it, train_loss=0.319]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.10s/it, train_loss=0.319]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.467]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.467]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.12s/it, train_loss=0.34] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.34]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.10s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.526]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.526]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.03s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.03s/it, train_loss=0.529]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.06s/it, train_loss=0.529]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.18s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.18s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.11s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.11s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.675]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.10it/s, train_loss=0.675]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 average loss: 0.4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 7/100 [04:00<52:09, 33.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 7 current AUC: 0.9855 current accuracy: 0.7764 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.27s/it, train_loss=0.337]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.27s/it, train_loss=0.4]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.21s/it, train_loss=0.4]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.21s/it, train_loss=0.364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.11s/it, train_loss=0.364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.11s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.317]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.317]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.339]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.339]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.01it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:23,  1.01it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.06it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:21,  1.06it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.04it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.04it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.04it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.04it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.01it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.01it/s, train_loss=0.646]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.02it/s, train_loss=0.646]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.02it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.03it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.03it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.07it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.07it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.10it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.10it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.04it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.04it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.05it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.05it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.05it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.06it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.06it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:11,  1.01s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.01s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:10,  1.01s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.34] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.00it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.00it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.04it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.04it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.01s/it, train_loss=0.314]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.379]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.05s/it, train_loss=0.379]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.05s/it, train_loss=0.31] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.13s/it, train_loss=0.31]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.13s/it, train_loss=0.402]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.16s/it, train_loss=0.402]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.16s/it, train_loss=0.432]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.17s/it, train_loss=0.432]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.17s/it, train_loss=0.525]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.19s/it, train_loss=0.525]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.19s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.11s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.11s/it, train_loss=0.535]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.08it/s, train_loss=0.535]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 average loss: 0.3947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 8/100 [04:32<50:51, 33.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 8 current AUC: 0.9922 current accuracy: 0.7888 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.10s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.10s/it, train_loss=0.597]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.597]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.04it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.04it/s, train_loss=0.427]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.04s/it, train_loss=0.427]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.04s/it, train_loss=0.543]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.05it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.05it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.02it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.02it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.05it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.05it/s, train_loss=0.426]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.05it/s, train_loss=0.426]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.05it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.05it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.05it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.02it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.02it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.09s/it, train_loss=0.263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.09s/it, train_loss=0.523]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.06s/it, train_loss=0.523]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.06s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.11s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.11s/it, train_loss=0.35] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.08s/it, train_loss=0.35]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.484]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:17,  1.17s/it, train_loss=0.484]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.17s/it, train_loss=0.666]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.14s/it, train_loss=0.666]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.35] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.15s/it, train_loss=0.35]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.15s/it, train_loss=0.353]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.09s/it, train_loss=0.353]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.09s/it, train_loss=0.51] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.20s/it, train_loss=0.51]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.20s/it, train_loss=0.489]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:12,  1.24s/it, train_loss=0.489]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.24s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:11,  1.27s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.27s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.23s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.23s/it, train_loss=0.394]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.22s/it, train_loss=0.394]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.22s/it, train_loss=0.433]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:07,  1.24s/it, train_loss=0.433]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.24s/it, train_loss=0.479]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.17s/it, train_loss=0.479]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.17s/it, train_loss=0.477]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.14s/it, train_loss=0.477]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.14s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.17s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.17s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.21s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.21s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.24s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.24s/it, train_loss=0.519]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.03it/s, train_loss=0.519]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 average loss: 0.4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 9/100 [05:06<51:01, 33.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 9 current AUC: 0.9875 current accuracy: 0.6149 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.03it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.566]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.27] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.04s/it, train_loss=0.27]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.04s/it, train_loss=0.42]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.42]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.02s/it, train_loss=0.341]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.02it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.02it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.01s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.01s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.01s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.01s/it, train_loss=0.391]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.391]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.349]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.349]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.385]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.385]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.458]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.02it/s, train_loss=0.458]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.02it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.417]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.417]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.347]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.04s/it, train_loss=0.347]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.487]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.13s/it, train_loss=0.487]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.405]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:16,  1.16s/it, train_loss=0.405]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.16s/it, train_loss=0.42] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.14s/it, train_loss=0.42]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.502]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.502]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.422]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.422]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.14s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.14s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.07s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.03s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.02it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.02it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.04s/it, train_loss=0.292]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.04s/it, train_loss=0.28] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.01s/it, train_loss=0.28]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.01s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.01s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.344]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.01it/s, train_loss=0.344]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.01it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.03s/it, train_loss=0.312]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.33] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.02it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.02it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.28it/s, train_loss=0.33]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 average loss: 0.3817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 10/100 [05:39<49:48, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 10 current AUC: 0.9937 current accuracy: 0.9068 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.308]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.308]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.453]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.453]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.04it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.04it/s, train_loss=0.418]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.00s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.00s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.05s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.05s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.443]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.443]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=0.381]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.381]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.491]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.04it/s, train_loss=0.491]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.04it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.05s/it, train_loss=0.299]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.10s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.12s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.367]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.367]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.14s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.14s/it, train_loss=0.713]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.713]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.09s/it, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.09s/it, train_loss=0.49] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.49]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.296]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.10s/it, train_loss=0.296]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.08s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.08s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.08s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.08s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.07s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.07s/it, train_loss=0.308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.06s/it, train_loss=0.308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.261]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.14s/it, train_loss=0.261]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.14s/it, train_loss=0.283]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.18s/it, train_loss=0.283]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.15s/it, train_loss=0.396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.14s/it, train_loss=0.396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.14s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.12s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.12s/it, train_loss=0.368]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.12s/it, train_loss=0.368]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.12s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.09it/s, train_loss=0.217]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 average loss: 0.3499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 11/100 [06:12<49:31, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 11 current AUC: 0.9937 current accuracy: 0.8385 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.298]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.03s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.06s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.07s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.07s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.11s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.11s/it, train_loss=0.37] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.37]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.02s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.02s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.38] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.38]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.483]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.483]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.309]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.03it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.03it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.00s/it, train_loss=0.236]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.00s/it, train_loss=0.231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.382]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.08s/it, train_loss=0.382]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.08s/it, train_loss=0.518]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.13s/it, train_loss=0.518]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.13s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.09s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.12s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.12s/it, train_loss=0.33] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.13s/it, train_loss=0.33]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.08s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.08s/it, train_loss=0.32] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.08s/it, train_loss=0.32]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.08s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.06s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.06s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.09s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.392]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.12s/it, train_loss=0.392]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.327]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.327]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.06s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.05s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.05s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.01s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.01s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.03s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.03s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.02it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.01s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.01s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.01s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.01s/it, train_loss=0.365]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.18it/s, train_loss=0.365]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 average loss: 0.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 12/100 [06:45<48:42, 33.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 12 current AUC: 0.9930 current accuracy: 0.8634 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.23s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.23s/it, train_loss=0.305]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.24s/it, train_loss=0.305]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.24s/it, train_loss=0.206]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:35,  1.27s/it, train_loss=0.206]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:05<00:35,  1.27s/it, train_loss=0.258]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:33,  1.25s/it, train_loss=0.258]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:33,  1.25s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:33,  1.29s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:33,  1.29s/it, train_loss=0.185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.15s/it, train_loss=0.185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.15s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.15s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.15s/it, train_loss=0.373]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.373]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.10s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.04s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.01s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.02s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.04s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.04s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.03s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.03s/it, train_loss=0.236]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.236]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.175]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.175]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.08s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.08s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.18s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.18s/it, train_loss=0.415]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.15s/it, train_loss=0.415]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.15s/it, train_loss=0.457]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.18s/it, train_loss=0.457]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.211]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.14s/it, train_loss=0.211]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.14s/it, train_loss=0.354]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.13s/it, train_loss=0.354]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.10s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.3]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.3]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.182]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.16s/it, train_loss=0.182]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.16s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.07s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.07s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.07s/it, train_loss=0.352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.536]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.07s/it, train_loss=0.536]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.07s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.13s/it, train_loss=0.25] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.25]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.09it/s, train_loss=0.153]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 average loss: 0.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 13/100 [07:20<48:46, 33.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 13 current AUC: 0.9946 current accuracy: 0.8758 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.06s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:31,  1.06s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.18s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.18s/it, train_loss=0.243]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.21s/it, train_loss=0.243]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.21s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:33,  1.23s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:33,  1.23s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.14s/it, train_loss=0.215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.04s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.05s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:23,  1.05s/it, train_loss=0.301]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.08s/it, train_loss=0.301]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.08s/it, train_loss=0.237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.10s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.06s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.06s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.06s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.06s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.04s/it, train_loss=0.286]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.286]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.09s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.09s/it, train_loss=0.498]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=0.498]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.06s/it, train_loss=0.376]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.08s/it, train_loss=0.376]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.08s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.09s/it, train_loss=0.357]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.357]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.10s/it, train_loss=0.287]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.14s/it, train_loss=0.287]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.14s/it, train_loss=0.306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.12s/it, train_loss=0.306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.12s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.15s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.15s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.24] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.23s/it, train_loss=0.24]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.23s/it, train_loss=0.734]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.00s/it, train_loss=0.734]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 average loss: 0.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 14/100 [07:54<48:35, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 14 current AUC: 0.9942 current accuracy: 0.8261 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.03it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.05s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.05s/it, train_loss=0.298]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.10s/it, train_loss=0.298]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.10s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.13s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.13s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.413]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.11s/it, train_loss=0.413]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.11s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.23s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.23s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.227]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.15s/it, train_loss=0.227]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.15s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.04s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.00s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.00s/it, train_loss=0.331]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.331]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.15s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.365]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.365]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.292]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.08s/it, train_loss=0.292]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.08s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.06s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.11s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.11s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.07s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.09s/it, train_loss=0.44] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.44]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.08s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.08s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.476]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.07s/it, train_loss=0.476]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.08s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.08s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.04s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.04s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.06s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.01s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.01s/it, train_loss=0.399]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.399]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 average loss: 0.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 15/100 [08:28<47:55, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 15 current AUC: 0.9911 current accuracy: 0.8137 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.10it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.10it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.02it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:28,  1.02it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.07s/it, train_loss=0.249]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.07s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.07s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.07s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.14s/it, train_loss=0.805]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.16s/it, train_loss=0.805]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.16s/it, train_loss=0.449]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.449]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.09s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.03s/it, train_loss=0.21] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.03it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.03it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.00s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.00s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.05s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.03s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.03s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.02it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.02it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.04it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.04it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.04it/s, train_loss=0.55] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.02it/s, train_loss=0.55]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.02it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.05it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.05it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.03it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.03it/s, train_loss=0.37] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.37]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.03it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.05it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.05it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.02it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.02it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.07it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.07it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.08it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.08it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.07it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.07it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.08it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.08it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.01it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.01it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.01s/it, train_loss=0.356]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.03s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.03s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.17it/s, train_loss=0.202]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 average loss: 0.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|█▌        | 16/100 [08:59<46:15, 33.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 16 current AUC: 0.9947 current accuracy: 0.8509 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.21s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.21s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.05s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.05s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.00it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.03it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.03it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.03s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.18] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.09s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.09s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.183]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.00it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.00it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.01it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.01it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.06it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.06it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.06it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.06it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.06it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.06it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:12,  1.09it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.09it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.12it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:11,  1.12it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:10,  1.12it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:10,  1.12it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.08it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:08,  1.12it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:08,  1.12it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:07,  1.14it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:07,  1.14it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:06,  1.15it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:06,  1.15it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.11it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.11it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.08it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.08it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.07it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.07it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.01it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.01it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.07s/it, train_loss=0.231]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.07s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.15s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.15s/it, train_loss=0.253]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.23s/it, train_loss=0.253]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.23s/it, train_loss=0.539]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.02it/s, train_loss=0.539]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 average loss: 0.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 17/100 [09:30<44:54, 32.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 17 current AUC: 0.9909 current accuracy: 0.8323 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.08s/it, train_loss=0.237]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.08s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.03s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.03s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.02it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.02s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.06s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.06s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.169]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.15s/it, train_loss=0.169]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.15s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.15s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.14s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.10s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.10s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.07s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.07s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.07s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.07s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.304]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.01it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.01it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.00s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.00s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.05it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.00s/it, train_loss=0.183]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.00s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.01s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.04s/it, train_loss=0.369]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.29] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.15s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.15s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.15s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.12s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.12s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.14s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.14s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.09s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.09s/it, train_loss=0.341]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.341]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 average loss: 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  18%|█▊        | 18/100 [10:03<44:31, 32.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 18 current AUC: 0.9895 current accuracy: 0.7950 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.22s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.22s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.14s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.14s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.323]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.323]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.15s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.15s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.16s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.16s/it, train_loss=0.398]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.14s/it, train_loss=0.398]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.14s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.09s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.09s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.11s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.04s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.04s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.05s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.05s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.22] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.17s/it, train_loss=0.22]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.17s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.18s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.17s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:16,  1.23s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.23s/it, train_loss=0.527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.22s/it, train_loss=0.527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.22s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.19s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.19s/it, train_loss=0.21] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.16s/it, train_loss=0.21]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.16s/it, train_loss=0.45]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.45]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.15s/it, train_loss=0.27]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.10s/it, train_loss=0.27]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.10s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.10s/it, train_loss=0.0867]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.03s/it, train_loss=0.0867]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.03s/it, train_loss=0.232] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.06s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.12s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.15s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.15s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.10s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.10s/it, train_loss=0.135]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.17it/s, train_loss=0.135]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 average loss: 0.2520\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  19%|█▉        | 19/100 [10:40<45:27, 33.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 19 current AUC: 0.9980 current accuracy: 0.8447 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.27s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.27s/it, train_loss=0.212]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.20s/it, train_loss=0.212]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.20s/it, train_loss=0.445]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.18s/it, train_loss=0.445]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.18s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.18s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.18s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:28,  1.09s/it, train_loss=0.25] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.25]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.16s/it, train_loss=0.116]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:29,  1.29s/it, train_loss=0.116]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:29,  1.29s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.26s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:12<00:27,  1.26s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:26,  1.26s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:13<00:26,  1.26s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:24,  1.23s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:24,  1.23s/it, train_loss=0.14] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.16s/it, train_loss=0.14]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.16s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.14s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.21s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.21s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.18s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:20<00:16,  1.09s/it, train_loss=0.275]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.14s/it, train_loss=0.275]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:21<00:15,  1.14s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.24s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:16,  1.24s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.18s/it, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.23s/it, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:25<00:13,  1.23s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.19s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:26<00:11,  1.19s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.14s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:27<00:10,  1.14s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.16s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:28<00:09,  1.16s/it, train_loss=0.29] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.14s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:29<00:07,  1.14s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.16s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:30<00:06,  1.16s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.19s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:31<00:05,  1.19s/it, train_loss=0.316]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.15s/it, train_loss=0.316]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.15s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.11s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.10s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.10s/it, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.04s/it, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:35<00:01,  1.04s/it, train_loss=0.507] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:35<00:00,  1.21it/s, train_loss=0.507]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 average loss: 0.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 20/100 [11:15<45:46, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 20 current AUC: 0.9971 current accuracy: 0.8634 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.212]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.05s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.05s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.04s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.04s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.04s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.321]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.04s/it, train_loss=0.321]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.04s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.01it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.01it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.01it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.01it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.119]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.519]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.05it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.05it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.05it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.05it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.05it/s, train_loss=0.235] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.04s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.04s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.01it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.01it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.197]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.08s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.08s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:09,  1.14s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.14s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:08,  1.16s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.16s/it, train_loss=0.429]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.429]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.11s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.11s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.03s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.03s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.07s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.02s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.258]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.16it/s, train_loss=0.258]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 average loss: 0.2309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  21%|██        | 21/100 [11:48<44:29, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 21 current AUC: 0.9973 current accuracy: 0.8571 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.21s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.21s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.21s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.21s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.19s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.03s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.11] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.11s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.16s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.16s/it, train_loss=0.081]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.15s/it, train_loss=0.081]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.15s/it, train_loss=0.304]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.15s/it, train_loss=0.304]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.15s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.09s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.04s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.03s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.03s/it, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.01it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:13,  1.01it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.01it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:12,  1.01it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:11,  1.01it/s, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:10,  1.04it/s, train_loss=0.247] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:09,  1.03it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.05it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:08,  1.05it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.05it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.05it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.08s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.07s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.02s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.03it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.03it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.02it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.04it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.04it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.05it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.05it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.26it/s, train_loss=0.181]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 average loss: 0.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 22/100 [12:20<43:23, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 22 current AUC: 0.9840 current accuracy: 0.7764 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.14it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.14it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:29,  1.00s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.00s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.03s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.03s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.10s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.10s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:33,  1.30s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:33,  1.30s/it, train_loss=0.167]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:31,  1.26s/it, train_loss=0.167]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:31,  1.26s/it, train_loss=0.314]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.24s/it, train_loss=0.314]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.24s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.26s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.26s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:25,  1.21s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:25,  1.21s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.14s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.14s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.04s/it, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:16,  1.02it/s, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.07it/s, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.07it/s, train_loss=0.221] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:13,  1.09it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:13,  1.09it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:12,  1.10it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:12,  1.10it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:11,  1.12it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:11,  1.12it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:10,  1.13it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:10,  1.13it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:09,  1.12it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:09,  1.12it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:08,  1.14it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:08,  1.14it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:07,  1.13it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:07,  1.13it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:06,  1.16it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:06,  1.16it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:05,  1.17it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:05,  1.17it/s, train_loss=0.0951]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.08it/s, train_loss=0.0951]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.08it/s, train_loss=0.185] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.00it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.00it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.00it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.00s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.00s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.03it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.03it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.06it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.06it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.35it/s, train_loss=0.175]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 average loss: 0.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 23/100 [12:52<42:00, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 23 current AUC: 0.9937 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.01s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.01s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.04it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.04it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.01it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.01it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.03s/it, train_loss=0.117]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.02it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.02it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.00s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.00s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.03it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.03it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.05it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.05it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.06it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.06it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.09it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.09it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.11it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.11it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:13,  1.09it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:13,  1.09it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.07it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.07it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.03it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.07it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.07it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.09it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.09it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:10,  1.07s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.07s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.01s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.01s/it, train_loss=0.246]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.03it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.03it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.03it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.04it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.04it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.04it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.04it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.04it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.04it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.06it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.06it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.03it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.03it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.05s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.05s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.20it/s, train_loss=0.315]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 average loss: 0.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 24/100 [13:22<40:41, 32.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 24 current AUC: 0.9955 current accuracy: 0.9193 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0775]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.11s/it, train_loss=0.0775]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.11s/it, train_loss=0.204] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.01it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:28,  1.01it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.02s/it, train_loss=0.219]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.02s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.08s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.08s/it, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.1]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.14s/it, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.12s/it, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.12s/it, train_loss=0.147] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.19s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.19s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.12] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.17s/it, train_loss=0.12]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.17s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.06s/it, train_loss=0.179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.06s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.02s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.02s/it, train_loss=0.107] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.04it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.04it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.06it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.06it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.00s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.00s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.08it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.08it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.10it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.10it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.06it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.06it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.08it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.08it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.06it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.06it/s, train_loss=0.134] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.03it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.01it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.01it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.05it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.05it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.09it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.09it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.08it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.08it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.08it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.08it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.29it/s, train_loss=0.217]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 average loss: 0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 25/100 [13:53<39:46, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 25 current AUC: 0.9955 current accuracy: 0.8944 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.00it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.00it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.01s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.01s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.08it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.08it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.02it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:31,  1.20s/it, train_loss=0.21]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:31,  1.20s/it, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:30,  1.20s/it, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.20s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.17s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.17s/it, train_loss=0.179] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.179]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.03it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.03it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.05it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.05it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.03it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.03it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.03it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.04it/s, train_loss=0.0896]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.03s/it, train_loss=0.0896]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.03s/it, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.02it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.02it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.04it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.04it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.04it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.04it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.03it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.06it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.06it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.05it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.05it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.01it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.01it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.01s/it, train_loss=0.159]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.03s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.03s/it, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.02s/it, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.01it/s, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.01it/s, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.05s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.05s/it, train_loss=0.148] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.10s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.10s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.12s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.12s/it, train_loss=0.557]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.09it/s, train_loss=0.557]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 average loss: 0.1580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|██▌       | 26/100 [14:25<39:18, 31.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 26 current AUC: 0.9907 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.02s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.02s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.04s/it, train_loss=0.089]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.089]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.00s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.00s/it, train_loss=0.134] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.226]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.04s/it, train_loss=0.226]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.259]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.00s/it, train_loss=0.259]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.00s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.02it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.00it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.03it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.03it/s, train_loss=0.253] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.06it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.06it/s, train_loss=0.0924]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.10it/s, train_loss=0.0924]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.10it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.07it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.07it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:12,  1.10it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.10it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.10it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:11,  1.10it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:10,  1.12it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:10,  1.12it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:09,  1.13it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:09,  1.13it/s, train_loss=0.419] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.09it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.09it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.09it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.09it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.08it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.08it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.02it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.02it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.07it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.07it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.07it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.07it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.09it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.09it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:02,  1.11it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.11it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.08it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.08it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.13it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.13it/s, train_loss=0.38] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.44it/s, train_loss=0.38]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 average loss: 0.1815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 27/100 [14:55<37:54, 31.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 27 current AUC: 0.9947 current accuracy: 0.8385 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.20it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.20it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.19it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:17,  1.19it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:15,  1.14it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:15,  1.14it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:15,  1.09it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.09it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:16,  1.02s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.02s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:14<00:15,  1.06s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:15,  1.06s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:15<00:14,  1.01s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.01s/it, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:12,  1.01it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:12,  1.01it/s, train_loss=0.11] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:11,  1.06it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.06it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:10,  1.01it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:10,  1.01it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:18<00:09,  1.03it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:19<00:09,  1.03it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:19<00:08,  1.01it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:20<00:08,  1.01it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:20<00:07,  1.01it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.01it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:21<00:06,  1.05it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.05it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:22<00:05,  1.07it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.07it/s, train_loss=0.0971]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:23<00:04,  1.04it/s, train_loss=0.0971]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.04it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:24<00:03,  1.07it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:03,  1.07it/s, train_loss=0.122] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:25<00:03,  1.00s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:03,  1.00s/it, train_loss=0.11] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:26<00:01,  1.04it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.04it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:01,  1.08s/it, train_loss=0.135]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:01,  1.08s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.18it/s, train_loss=0.163]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 average loss: 0.1525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|██▊       | 28/100 [15:24<36:36, 30.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 28 current AUC: 0.9963 current accuracy: 0.8634 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.10it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.10it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.11it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.09it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:24,  1.09it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.10s/it, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.321] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.321]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.04s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.04s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.01s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:18,  1.06it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:18,  1.06it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:17,  1.09it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.09it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.12it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.12it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.10it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.10it/s, train_loss=0.144] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.10it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.10it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:16,  1.08s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.08s/it, train_loss=0.12] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.06s/it, train_loss=0.12]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.06s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.04s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.04s/it, train_loss=0.206] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.02it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.02it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.07it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.07it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.09it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.09it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.11it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.11it/s, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:06,  1.15it/s, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:06,  1.15it/s, train_loss=0.196] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.01it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.01it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:06,  1.01s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.01s/it, train_loss=0.16] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.01s/it, train_loss=0.16]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.01s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.00it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.02it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.02it/s, train_loss=0.203] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.02it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.02it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.02it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.02it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.29it/s, train_loss=0.185]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 average loss: 0.1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  29%|██▉       | 29/100 [15:54<36:01, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 29 current AUC: 0.9881 current accuracy: 0.8696 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0499]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.0499]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.15it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.15it/s, train_loss=0.177] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.07it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.07it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.10it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.10it/s, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.03it/s, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.03it/s, train_loss=0.227] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:25,  1.01s/it, train_loss=0.227]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.01s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.12s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.19] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.02it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:18,  1.08it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:18,  1.08it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:17,  1.09it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.09it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.07it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.07it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.05it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.04it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.0653]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.06it/s, train_loss=0.0653]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.06it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:12,  1.09it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.09it/s, train_loss=0.0969]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.11it/s, train_loss=0.0969]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.11it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:17<00:10,  1.15it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:10,  1.15it/s, train_loss=0.143] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:09,  1.14it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:09,  1.14it/s, train_loss=0.0839]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.03it/s, train_loss=0.0839]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.03it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.03it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.04it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.04it/s, train_loss=0.184] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.03s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.03s/it, train_loss=0.13] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:06,  1.05s/it, train_loss=0.13]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.05s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.06s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.06s/it, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.00it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.01it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.01s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.205]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.04s/it, train_loss=0.205]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.04s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.18it/s, train_loss=0.0361]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 average loss: 0.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 30/100 [16:25<35:30, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 30 current AUC: 0.9926 current accuracy: 0.9006 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0644]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.06s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:31,  1.06s/it, train_loss=0.0723]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.0723]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.0753]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.14s/it, train_loss=0.0753]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.14s/it, train_loss=0.121] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.11s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.11s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.02s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.02s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.12s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.02it/s, train_loss=0.0932]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.05it/s, train_loss=0.0932]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.05it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.00s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.00s/it, train_loss=0.129] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.0674]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.0674]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.03it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.01s/it, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.01s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.05s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.05s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.0898]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.04s/it, train_loss=0.0898]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.04s/it, train_loss=0.132] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.03s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.03s/it, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.00s/it, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.00s/it, train_loss=0.132] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.07s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.11s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.0579]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.13s/it, train_loss=0.0579]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.13s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.054]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.054]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.06s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.04s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.04s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.05s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.05s/it, train_loss=0.137] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.05s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.05s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.0815]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.17it/s, train_loss=0.0815]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 average loss: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  31%|███       | 31/100 [16:57<35:43, 31.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 31 current AUC: 0.9894 current accuracy: 0.8696 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.14s/it, train_loss=0.135]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.14s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.06s/it, train_loss=0.263]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.07s/it, train_loss=0.263]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.07s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.00s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.00s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.03s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.03it/s, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.03it/s, train_loss=0.063] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.06it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.06it/s, train_loss=0.065]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.07it/s, train_loss=0.065]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.07it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:19,  1.06it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:19,  1.06it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:18,  1.08it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:18,  1.08it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.04it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.04it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:17,  1.01it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.068] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.03it/s, train_loss=0.068]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.01it/s, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.01it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.01it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.03it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.03it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.02it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.02it/s, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.03it/s, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.03it/s, train_loss=0.0589]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.0589]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.08it/s, train_loss=0.066] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.04it/s, train_loss=0.066]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.04it/s, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.00s/it, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.00s/it, train_loss=0.122] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:08,  1.02s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.02s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.02s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.02s/it, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.00it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.00it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.00s/it, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.00s/it, train_loss=0.143] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.04it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.04it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.08it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.08it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.09it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.09it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.11it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.11it/s, train_loss=0.0458]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.32it/s, train_loss=0.0458]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 average loss: 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|███▏      | 32/100 [17:27<34:56, 30.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 32 current AUC: 0.9943 current accuracy: 0.9379 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.07it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:27,  1.07it/s, train_loss=0.0942]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:36,  1.26s/it, train_loss=0.0942]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:36,  1.26s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.20s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.20s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.11s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.11s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.08s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.08s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.128] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.04s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.04s/it, train_loss=0.0818]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.09s/it, train_loss=0.0818]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.09s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.0938]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.0938]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.00it/s, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.168] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.01s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.01s/it, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.04it/s, train_loss=0.0989]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.06it/s, train_loss=0.0989]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.06it/s, train_loss=0.045] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.09it/s, train_loss=0.045]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:12,  1.09it/s, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.05it/s, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.05it/s, train_loss=0.139] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.01it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.02it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.02it/s, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.04it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.04it/s, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.03it/s, train_loss=0.074]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.06it/s, train_loss=0.074]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.06it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.08it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.08it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.09it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.09it/s, train_loss=0.176] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.07it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.07it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.12it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.12it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.14it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.14it/s, train_loss=0.0728]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.30it/s, train_loss=0.0728]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 average loss: 0.0790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 33/100 [17:58<34:25, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 33 current AUC: 0.9899 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.194] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.13it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.13it/s, train_loss=0.0927]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s, train_loss=0.0927]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.11it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.07it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.07it/s, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.10it/s, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.10it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:21,  1.15it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:21,  1.15it/s, train_loss=0.0279]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:21,  1.13it/s, train_loss=0.0279]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:21,  1.13it/s, train_loss=0.0624]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:20,  1.13it/s, train_loss=0.0624]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:20,  1.13it/s, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:19,  1.12it/s, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:19,  1.12it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:19,  1.05it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:19,  1.05it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.02s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.02s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:20,  1.09s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.09s/it, train_loss=0.108] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:19,  1.07s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.07s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:17,  1.00s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.00s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.02s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.02s/it, train_loss=0.0505]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:15,  1.04s/it, train_loss=0.0505]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.04s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.03s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.135] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.03it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.05it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.05it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.04it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.07it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.07it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.06it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.06it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.08it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.08it/s, train_loss=0.203] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.02it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.02it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.02it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.02it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.07it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.07it/s, train_loss=0.163] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:03,  1.08it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.08it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.01s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.01s/it, train_loss=0.27] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.03it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.03it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.02s/it, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.02s/it, train_loss=0.0655]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.26it/s, train_loss=0.0655]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 average loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|███▍      | 34/100 [18:28<33:38, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 34 current AUC: 0.9978 current accuracy: 0.9441 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0887]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.0887]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.0822]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.0822]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.159] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.01s/it, train_loss=0.159]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.05it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.05it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.07it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.07it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:22,  1.05it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:22,  1.05it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.06it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.06it/s, train_loss=0.046] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:21,  1.04it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.04it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.01it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.01it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.05it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.05it/s, train_loss=0.13]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.04it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.04it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:17,  1.06it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.06it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.07it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.07it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.03it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.03it/s, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.03it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.03it/s, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.00it/s, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.00it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.03it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.05it/s, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.05it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:19<00:09,  1.10it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.10it/s, train_loss=0.0796]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:20<00:08,  1.09it/s, train_loss=0.0796]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.09it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.07it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.07it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.11it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.11it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.12it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.12it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.14it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.14it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:03,  1.13it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.13it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:02,  1.16it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.16it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.15it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.15it/s, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.10it/s, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.10it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.39it/s, train_loss=0.116]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 average loss: 0.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 35/100 [18:57<32:36, 30.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 35 current AUC: 0.9906 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.15it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.15it/s, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:24,  1.13it/s, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:24,  1.13it/s, train_loss=0.0608] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.11it/s, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.11it/s, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.07it/s, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.07it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.06it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.06it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:22,  1.05it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:22,  1.05it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.05it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.05it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.08it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.08it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:18,  1.12it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:18,  1.12it/s, train_loss=0.15]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:17,  1.14it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:17,  1.14it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:16,  1.13it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:16,  1.13it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:15,  1.14it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:15,  1.14it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.15it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:14,  1.15it/s, train_loss=0.073]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:14,  1.13it/s, train_loss=0.073]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.13it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:14<00:14,  1.06it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.06it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:15<00:13,  1.01it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.01it/s, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:13,  1.01s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.01s/it, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:17<00:12,  1.00s/it, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:12,  1.00s/it, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:10,  1.03it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.03it/s, train_loss=0.171] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:19<00:09,  1.05it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.05it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:20<00:08,  1.04it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.04it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.01it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.01it/s, train_loss=0.179] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.02it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.02it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.08it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.08it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.01it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.01it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:04,  1.01s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:04,  1.01s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:03,  1.00s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.00s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.04it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.04it/s, train_loss=0.0421]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.06it/s, train_loss=0.0421]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.06it/s, train_loss=0.133] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.34it/s, train_loss=0.133]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 average loss: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 36/100 [19:27<31:54, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 36 current AUC: 0.9728 current accuracy: 0.7640 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.07it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.07it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.02it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.02it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.02it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.152] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.00it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.00it/s, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.052] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.052]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.02it/s, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.02it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:21,  1.03it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.03it/s, train_loss=0.128] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.05it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.05it/s, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.02s/it, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.103] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.10s/it, train_loss=0.103]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.10s/it, train_loss=0.0612]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.13s/it, train_loss=0.0612]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.13s/it, train_loss=0.196] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.09s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.12s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.04s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.02s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.02s/it, train_loss=0.115] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.01s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.01s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.04it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.04it/s, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.01it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.01it/s, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.02it/s, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.02it/s, train_loss=0.401] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.00it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.00it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.02it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.02it/s, train_loss=0.099] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.02s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.04it/s, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.04it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.05it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.05it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.06it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.06it/s, train_loss=0.187] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.03it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.03it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.26it/s, train_loss=0.179]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 average loss: 0.0901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 37/100 [19:58<31:47, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 37 current AUC: 0.9912 current accuracy: 0.8634 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0908]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.07it/s, train_loss=0.0908]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.07it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.00it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.06it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.06it/s, train_loss=0.024] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.02s/it, train_loss=0.024]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.04it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.04it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:22,  1.07it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:22,  1.07it/s, train_loss=0.0554]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.08it/s, train_loss=0.0554]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.08it/s, train_loss=0.025] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:19,  1.11it/s, train_loss=0.025]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:19,  1.11it/s, train_loss=0.0803]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:18,  1.12it/s, train_loss=0.0803]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:18,  1.12it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.03s/it, train_loss=0.0294]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.00it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.075] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:18,  1.03s/it, train_loss=0.075]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.03s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.02it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.02it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.02it/s, train_loss=0.0826]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:15,  1.05s/it, train_loss=0.0826]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.05s/it, train_loss=0.121] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.01s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.01s/it, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.04it/s, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.04it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.02it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.02it/s, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.03it/s, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.01it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.01it/s, train_loss=0.019] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.03s/it, train_loss=0.019]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.03s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:08,  1.05s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.05s/it, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.07s/it, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.07s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:06,  1.03s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.03s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.07s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.07s/it, train_loss=0.0729]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.16s/it, train_loss=0.0729]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.16s/it, train_loss=0.161] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.24s/it, train_loss=0.161]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.24s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.28s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.28s/it, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.18s/it, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.18s/it, train_loss=0.218] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.05it/s, train_loss=0.218]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 average loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|███▊      | 38/100 [20:30<31:53, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 38 current AUC: 0.9904 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.07s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.07s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.02s/it, train_loss=0.0711]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.0711]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.06it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.06it/s, train_loss=0.072] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.05it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.05it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.07it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.07it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.01it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.01it/s, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.00it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.00it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.01s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.00it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.00it/s, train_loss=0.166] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.02s/it, train_loss=0.166]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.02s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:19,  1.01s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.01s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:18,  1.01s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.0912]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.0912]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.262] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.01it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.01it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.02it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.02it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.01it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.01it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.01s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.01s/it, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.04s/it, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.04s/it, train_loss=0.302] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.08s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0702]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.0702]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.027] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.05s/it, train_loss=0.027]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.01s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.134] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.03s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.03s/it, train_loss=0.0713]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.14s/it, train_loss=0.0713]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.14s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.12s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.12s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.16s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.16s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.16s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.16s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.17s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.17s/it, train_loss=0.159] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.21s/it, train_loss=0.159]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.21s/it, train_loss=0.0364]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.08it/s, train_loss=0.0364]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 average loss: 0.0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  39%|███▉      | 39/100 [21:03<31:54, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 39 current AUC: 0.9850 current accuracy: 0.6832 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.01s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.01s/it, train_loss=0.0834]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.06it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.06it/s, train_loss=0.161] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.161]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.04s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.04s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.016] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.016]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.13s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.0506]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0506]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.07s/it, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.07s/it, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.03s/it, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.03s/it, train_loss=0.175] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.06s/it, train_loss=0.175]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.06s/it, train_loss=0.0605]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.03s/it, train_loss=0.0605]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.03s/it, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.01s/it, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.01s/it, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.03it/s, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.03it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.06it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.06it/s, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.06it/s, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.06it/s, train_loss=0.217] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.06it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.06it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.05it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.05it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.05s/it, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.04s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.10s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.0819]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.08s/it, train_loss=0.0819]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.08s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.14s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.14s/it, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.09s/it, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.09s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.10s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.10s/it, train_loss=0.0582]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.14s/it, train_loss=0.0582]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.14s/it, train_loss=0.26]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.15s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.15s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.03it/s, train_loss=0.207]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 average loss: 0.0822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 40/100 [21:36<31:53, 31.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 40 current AUC: 0.9884 current accuracy: 0.8323 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.01it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.01it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.02s/it, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.07s/it, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.07s/it, train_loss=0.202] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.0907]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.01s/it, train_loss=0.0907]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.01s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.04it/s, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:23,  1.04it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.0669]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.01s/it, train_loss=0.0669]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.01s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.173] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.02it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.02it/s, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.13s/it, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.13s/it, train_loss=0.05]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.14s/it, train_loss=0.05]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0816]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.09s/it, train_loss=0.0816]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.141] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.05s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.05s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.076]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.10s/it, train_loss=0.076]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.10s/it, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.09s/it, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.09s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.15s/it, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.15s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.14s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.14s/it, train_loss=0.0323]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:09,  1.24s/it, train_loss=0.0323]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.24s/it, train_loss=0.101] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.19s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.19s/it, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:07,  1.18s/it, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.18s/it, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.14s/it, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.14s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.09s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.217] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.08s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.08s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.03s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.01it/s, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.01it/s, train_loss=0.0338]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.28it/s, train_loss=0.0338]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 average loss: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  41%|████      | 41/100 [22:09<31:45, 32.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 41 current AUC: 0.9864 current accuracy: 0.8571 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.26s/it, train_loss=0.0236]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.26s/it, train_loss=0.0635]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.23s/it, train_loss=0.0635]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.23s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.10s/it, train_loss=0.112]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.10s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.10s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.10s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.163] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.04s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.16s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.16s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.099] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.07s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.07s/it, train_loss=0.072]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.072]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.12s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.19s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.19s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.21s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.21s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.19s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.19s/it, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.22s/it, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.22s/it, train_loss=0.142] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.24s/it, train_loss=0.142]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.24s/it, train_loss=0.0843]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.22s/it, train_loss=0.0843]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.22s/it, train_loss=0.051] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.051]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.19s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.12s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.13s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.10s/it, train_loss=0.132] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.22s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.22s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.13s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.13s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.10s/it, train_loss=0.122] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.06s/it, train_loss=0.0499]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.0499]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.0775]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.0775]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.06s/it, train_loss=0.0406]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:01,  1.01it/s, train_loss=0.0406]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:01,  1.01it/s, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.05s/it, train_loss=0.06]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.05s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.22it/s, train_loss=0.126]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 average loss: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 42/100 [22:44<31:54, 33.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 42 current AUC: 0.9935 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.17s/it, train_loss=0.0281]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.17s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.091] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.12s/it, train_loss=0.091]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.12s/it, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.03s/it, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.03s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.04s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.04s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.08s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.08s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.08s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.08s/it, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.05s/it, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.05s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.12s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.15s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.15s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.17s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.17s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.17s/it, train_loss=0.108] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.16s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.16s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.18s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.23s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.23s/it, train_loss=0.0952]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:16,  1.23s/it, train_loss=0.0952]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.23s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.20s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.20s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.24s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.24s/it, train_loss=0.0532] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.23s/it, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:12,  1.23s/it, train_loss=0.00915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.20s/it, train_loss=0.00915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.20s/it, train_loss=0.223]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.19s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.19s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.19s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:29<00:08,  1.19s/it, train_loss=0.0665]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:07,  1.21s/it, train_loss=0.0665]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:30<00:07,  1.21s/it, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:06,  1.23s/it, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:31<00:06,  1.23s/it, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.15s/it, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.15s/it, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.14s/it, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.14s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.19s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.19s/it, train_loss=0.209] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.18s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:35<00:01,  1.18s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:35<00:00,  1.06it/s, train_loss=0.0238]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 average loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 43/100 [23:20<32:13, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 43 current AUC: 0.9888 current accuracy: 0.8447 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.02s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.02s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.11s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.11s/it, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.139] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.03s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.03s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.171] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.04s/it, train_loss=0.0646]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.0646]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=0.105] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.14s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.0733]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0733]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.02]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.0592]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.0592]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.10s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.08s/it, train_loss=0.05]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.05]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.15s/it, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.14s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.10s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.10s/it, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.09s/it, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.09s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.13s/it, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.172] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.172]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.115] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.06s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.16s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.16s/it, train_loss=0.119]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.16s/it, train_loss=0.119]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.16s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.23s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.23s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.19s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.19s/it, train_loss=0.125] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.13s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.06s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.06s/it, train_loss=0.0526]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.19it/s, train_loss=0.0526]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 average loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|████▍     | 44/100 [23:54<31:40, 33.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 44 current AUC: 0.9973 current accuracy: 0.9130 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0761]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.28s/it, train_loss=0.0761]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.28s/it, train_loss=0.0943]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.23s/it, train_loss=0.0943]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.23s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.18s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.18s/it, train_loss=0.162] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.21s/it, train_loss=0.162]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.21s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.19s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.12s/it, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.202] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.01s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.03s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.01s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.04s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.04s/it, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.00it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:17,  1.00it/s, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.08s/it, train_loss=0.154] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.06s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.06s/it, train_loss=0.102] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.09s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.09s/it, train_loss=0.31]  \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.31]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.12s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.11s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.11s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.04s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.0496]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.0496]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.01s/it, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.02s/it, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.02s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.09s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.09s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.08s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.08s/it, train_loss=0.109] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.10s/it, train_loss=0.109]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.10s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.10s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.10s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.11it/s, train_loss=0.0155]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 average loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 45/100 [24:27<31:03, 33.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 45 current AUC: 0.9897 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.09it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.09it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.00it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.196] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.00it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.00it/s, train_loss=0.05] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.01it/s, train_loss=0.05]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.03it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.03it/s, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.08s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.08s/it, train_loss=0.157] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.06s/it, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.06s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.00it/s, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.00it/s, train_loss=0.022] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.04it/s, train_loss=0.022]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.04it/s, train_loss=0.061]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.00it/s, train_loss=0.061]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.00it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.03s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.03s/it, train_loss=0.0353]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.10s/it, train_loss=0.0353]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.10s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.02s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.02s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.05it/s, train_loss=0.191] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.00s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.00s/it, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.05s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.05s/it, train_loss=0.00912]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.07s/it, train_loss=0.00912]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.0455] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.02s/it, train_loss=0.0455]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.02s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.06s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.152] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.06s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.06s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.03s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.03s/it, train_loss=0.0581]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.0581]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.01s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.01it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.01it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.21it/s, train_loss=0.0206]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 average loss: 0.0648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|████▌     | 46/100 [24:59<29:51, 33.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 46 current AUC: 0.9926 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.11s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.11s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.13s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.13s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.06s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.06s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.205] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.15s/it, train_loss=0.205]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.15s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.19s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.19s/it, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.16s/it, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.16s/it, train_loss=0.0232]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.20s/it, train_loss=0.0232]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.20s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.21s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.21s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.06s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.06s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.11s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.10s/it, train_loss=0.00785]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.00785]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.04s/it, train_loss=0.0467] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.01it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.01it/s, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.05it/s, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:14,  1.05it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.07it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:13,  1.07it/s, train_loss=0.145] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.04it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:12,  1.04it/s, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.05s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.05s/it, train_loss=0.064] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.16s/it, train_loss=0.064]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.16s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.06s/it, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.05s/it, train_loss=0.057]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.05s/it, train_loss=0.057]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.01s/it, train_loss=0.124]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.07s/it, train_loss=0.188] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.12s/it, train_loss=0.188]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.12s/it, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.16s/it, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.12]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.13it/s, train_loss=0.288]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 average loss: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 47/100 [25:32<29:24, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 47 current AUC: 0.9905 current accuracy: 0.8944 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.08it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.08it/s, train_loss=0.321] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.02it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.01s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.02it/s, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.02it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.03it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.03it/s, train_loss=0.044] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.02it/s, train_loss=0.044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.02it/s, train_loss=0.0987]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.07it/s, train_loss=0.0987]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.07it/s, train_loss=0.023] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.10it/s, train_loss=0.023]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.10it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:19,  1.09it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:19,  1.09it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.00it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.00it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.01it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.01it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:17,  1.02it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.02it/s, train_loss=0.126] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.01it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.01it/s, train_loss=0.042]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.05s/it, train_loss=0.042]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.05s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.13s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.14s/it, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.0684]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:15,  1.16s/it, train_loss=0.0684]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.16s/it, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.16s/it, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.16s/it, train_loss=0.00733]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:13,  1.19s/it, train_loss=0.00733]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.19s/it, train_loss=0.0192] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:11,  1.12s/it, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.10s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.10s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.08s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.08s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.13s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.13s/it, train_loss=0.102] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.06s/it, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.01s/it, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.147] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.03s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.03s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.03s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.193] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.08s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.08s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.14it/s, train_loss=0.186]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 average loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|████▊     | 48/100 [26:05<28:37, 33.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 48 current AUC: 0.9960 current accuracy: 0.9255 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0287]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.10s/it, train_loss=0.0287]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.10s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.17s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.17s/it, train_loss=0.107] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.0295]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.0295]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.13s/it, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.18s/it, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.18s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.22s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.22s/it, train_loss=0.104] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.19s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.19s/it, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:26,  1.25s/it, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:26,  1.25s/it, train_loss=0.2]   \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.18s/it, train_loss=0.2]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.18s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.13s/it, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.06s/it, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.06s/it, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:17,  1.05s/it, train_loss=0.154] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.15s/it, train_loss=0.0683]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.0683]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.15s/it, train_loss=0.103] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.18s/it, train_loss=0.103]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.18s/it, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:15,  1.17s/it, train_loss=0.149] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.21s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.21s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.18s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.18s/it, train_loss=0.069]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.17s/it, train_loss=0.069]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.17s/it, train_loss=0.039]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.039]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.12s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.14s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.14s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.14s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.14s/it, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.10s/it, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.07s/it, train_loss=0.028]  \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.04s/it, train_loss=0.028]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.04s/it, train_loss=0.0942]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.0942]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.03s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.02s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.02s/it, train_loss=0.153] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.21it/s, train_loss=0.153]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 average loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  49%|████▉     | 49/100 [26:40<28:31, 33.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 49 current AUC: 0.9959 current accuracy: 0.9255 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00513]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.00513]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.0738] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.13s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.13s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.05s/it, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.05s/it, train_loss=0.078] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.09s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.09s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.203] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.203]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.03s/it, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.03s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.02s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.01s/it, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.01s/it, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.06s/it, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.157] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.08s/it, train_loss=0.157]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.08s/it, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.08s/it, train_loss=0.0223] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.07s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.13s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.13s/it, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.07s/it, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.07s/it, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.10s/it, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.129] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.07s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.08s/it, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.08s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.08s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.08s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.125] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.09it/s, train_loss=0.125]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 average loss: 0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 50/100 [27:13<27:57, 33.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 50 current AUC: 0.9957 current accuracy: 0.9565 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00706]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.18s/it, train_loss=0.00706]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.18s/it, train_loss=0.062]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:37,  1.30s/it, train_loss=0.062]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:37,  1.30s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.20s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.20s/it, train_loss=0.034]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.20s/it, train_loss=0.034]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.20s/it, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.18s/it, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.18s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.21s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:30,  1.21s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.13s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.17s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.17s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.16s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.16s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.16s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.16s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:22,  1.10s/it, train_loss=0.00493]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.00493]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.17s/it, train_loss=0.0721] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.15s/it, train_loss=0.0721]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.15s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.13s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.13s/it, train_loss=0.151] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.13s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.13s/it, train_loss=0.092]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.10s/it, train_loss=0.092]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.10s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.13s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.20s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:15,  1.20s/it, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.21s/it, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.21s/it, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.14s/it, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.14s/it, train_loss=0.177] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.09s/it, train_loss=0.177]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.09s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.13s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.13s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.15s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.15s/it, train_loss=0.015] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.06s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.06s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.06s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.06s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.06s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.06s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.04s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.04s/it, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.00s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.00s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.22it/s, train_loss=0.017]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 average loss: 0.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  51%|█████     | 51/100 [27:48<27:41, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 51 current AUC: 0.9919 current accuracy: 0.9006 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00721]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.09it/s, train_loss=0.00721]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.09it/s, train_loss=0.153]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.17it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.17it/s, train_loss=0.077]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.12it/s, train_loss=0.077]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.12it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.11it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.11it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.06it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.06it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.04it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.04it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:23,  1.03s/it, train_loss=0.117]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.02s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.108] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:22,  1.07s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.07s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.14s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.14s/it, train_loss=0.0745]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.12s/it, train_loss=0.0745]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.05s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.05s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:17,  1.00s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.00s/it, train_loss=0.0664] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.00s/it, train_loss=0.0664]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.00s/it, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.02it/s, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.02it/s, train_loss=0.0225] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.03it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.03it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.07it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.07it/s, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.04it/s, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.04it/s, train_loss=0.0645]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:11,  1.01s/it, train_loss=0.0645]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.05s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.05s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.02s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.02s/it, train_loss=0.101] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.06s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.06s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.12s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.12s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.11s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.11s/it, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.10s/it, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.10s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.08s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.08s/it, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.14s/it, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.14s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.17s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.17s/it, train_loss=0.0963]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.19s/it, train_loss=0.0963]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.19s/it, train_loss=0.0214]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.06it/s, train_loss=0.0214]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 average loss: 0.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|█████▏    | 52/100 [28:20<26:46, 33.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 52 current AUC: 0.9947 current accuracy: 0.9441 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0201]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.01it/s, train_loss=0.0201]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.01it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.08s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.08s/it, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.15s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.15s/it, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.16s/it, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.16s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.18s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.18s/it, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.24s/it, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.24s/it, train_loss=0.00378]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.00378]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.0145] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.18s/it, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.18s/it, train_loss=0.288] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.20s/it, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.20s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.07s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:18,  1.09s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.10s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:17,  1.10s/it, train_loss=0.0822]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.07s/it, train_loss=0.0822]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.07s/it, train_loss=0.0037]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.0037]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.09s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.11s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.01s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.01s/it, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:10,  1.00it/s, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:10,  1.00it/s, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.01s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.01s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.02s/it, train_loss=0.12] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.00it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:07,  1.00it/s, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.11s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.05s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.05s/it, train_loss=0.074] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.074]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.01s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.03s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.00701]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.01s/it, train_loss=0.00701]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.01s/it, train_loss=0.038]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.038]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.30it/s, train_loss=0.0297]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 average loss: 0.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 53/100 [28:54<26:10, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 53 current AUC: 0.9979 current accuracy: 0.9130 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.00886]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.05it/s, train_loss=0.00886]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.05it/s, train_loss=0.146]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.01s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.03s/it, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.125] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:24,  1.01s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.01s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.02s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.02s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.04s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.07s/it, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.07s/it, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.00it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.07s/it, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.0622]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.05s/it, train_loss=0.0622]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.03s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.03s/it, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.00it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.00it/s, train_loss=0.076]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.02it/s, train_loss=0.076]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.02it/s, train_loss=0.00481]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.05s/it, train_loss=0.00481]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.05s/it, train_loss=0.0881] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.02s/it, train_loss=0.0881]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.02s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.05it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.05it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.03it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.03it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.03it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.03it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.05it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.05it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.05it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.05it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.05it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.05it/s, train_loss=0.234] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.07it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.07it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.03it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.03it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.01s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.165] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.01it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.01it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.25it/s, train_loss=0.132]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 average loss: 0.0631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|█████▍    | 54/100 [29:25<25:02, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 54 current AUC: 0.9968 current accuracy: 0.8944 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0214]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.0214]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.03it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:29,  1.00s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.00s/it, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.01it/s, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.01it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.01it/s, train_loss=0.044] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.00it/s, train_loss=0.044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.00it/s, train_loss=0.0611]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.04it/s, train_loss=0.0611]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.04it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:21,  1.02it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.02it/s, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.06s/it, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.06s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:23,  1.15s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.15s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.11s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.14s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.14s/it, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.09s/it, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.07s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.07s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.07s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.07s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.09s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.14s/it, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.00458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.00458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0238] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.09s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.09s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.08s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.08s/it, train_loss=0.0329] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.07s/it, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.07s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.00s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.00s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.04s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.04s/it, train_loss=0.0076]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.00s/it, train_loss=0.0076]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.00s/it, train_loss=0.104] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.02s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.02s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.23it/s, train_loss=0.0123]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 average loss: 0.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 55/100 [29:57<24:32, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 55 current AUC: 0.9950 current accuracy: 0.9379 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.14it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.14it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.05it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.05it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.01it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.01it/s, train_loss=0.00676]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.07it/s, train_loss=0.00676]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.07it/s, train_loss=0.0556] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.03s/it, train_loss=0.0556]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.06s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.06s/it, train_loss=0.0157] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.0903]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.09s/it, train_loss=0.0903]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.0647]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.09s/it, train_loss=0.0647]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.00751]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.14s/it, train_loss=0.00751]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.0166] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.18s/it, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.18s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:19,  1.22s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.22s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:18,  1.20s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.20s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.15s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.15s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.12s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.12s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.17s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.17s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.12s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.14s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.14s/it, train_loss=0.066] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.10s/it, train_loss=0.066]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0872]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.12s/it, train_loss=0.0872]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.12s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.11s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.06s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.07s/it, train_loss=0.00412]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.10s/it, train_loss=0.00412]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.10s/it, train_loss=0.168]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.08s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.08s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.048] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.08it/s, train_loss=0.048]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 average loss: 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|█████▌    | 56/100 [30:32<24:19, 33.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 56 current AUC: 0.9947 current accuracy: 0.9255 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00616]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.00616]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.05it/s, train_loss=0.0816] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.0816]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.12s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.12s/it, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0177] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.19s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.16s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.16s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.00905]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.00905]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.10s/it, train_loss=0.046]  \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.046]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.14s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.11s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.14s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.14s/it, train_loss=0.0148] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.10s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.04s/it, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.10s/it, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.10s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.02s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.02s/it, train_loss=0.14]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.03s/it, train_loss=0.14]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.03s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.13s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.05s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.05s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.00s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.00s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.00466]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.00466]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0838] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.0838]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.09s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.09s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.04s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.04s/it, train_loss=0.0197] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.03it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.03it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.00508]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.00508]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.0146] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.16it/s, train_loss=0.0146]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 average loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  57%|█████▋    | 57/100 [31:05<23:53, 33.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 57 current AUC: 0.9902 current accuracy: 0.9130 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00845]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.18s/it, train_loss=0.00845]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.18s/it, train_loss=0.0225] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.059] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.059]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.13s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.13s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.00484]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.14s/it, train_loss=0.00484]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.14s/it, train_loss=0.0402] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.21s/it, train_loss=0.0402]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.21s/it, train_loss=0.148] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.11s/it, train_loss=0.0774]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.07s/it, train_loss=0.0774]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.07s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.11s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.049] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.049]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.07s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.14s/it, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.20s/it, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.20s/it, train_loss=0.065] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.20s/it, train_loss=0.065]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.20s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.19s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.19s/it, train_loss=0.0545]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.16s/it, train_loss=0.0545]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.16s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.18s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.0829]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=0.0829]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.13s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.09s/it, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.10s/it, train_loss=0.0715] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.10s/it, train_loss=0.0715]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.275] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.08s/it, train_loss=0.275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.08s/it, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.08s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.17s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.17s/it, train_loss=0.00759]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.19s/it, train_loss=0.00759]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.19s/it, train_loss=0.148]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.17s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.17s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.20s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.20s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.0215]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.03it/s, train_loss=0.0215]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 average loss: 0.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  58%|█████▊    | 58/100 [31:41<23:47, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 58 current AUC: 0.9911 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.03it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.02it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.02it/s, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.02it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.04s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.04s/it, train_loss=0.032] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.09s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.12s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.12s/it, train_loss=0.0861]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.13s/it, train_loss=0.0861]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.00734]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:27,  1.18s/it, train_loss=0.00734]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.18s/it, train_loss=0.0126] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.08s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.08s/it, train_loss=0.013] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.013]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.07it/s, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:17,  1.07it/s, train_loss=0.238] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.06] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.04s/it, train_loss=0.06]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:18,  1.13s/it, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.13s/it, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.06s/it, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.0663]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.09s/it, train_loss=0.0663]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.12s/it, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.12s/it, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.08s/it, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.08s/it, train_loss=0.0401] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.00s/it, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.00s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.03s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.03s/it, train_loss=0.0402]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.03s/it, train_loss=0.0402]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.03s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.01s/it, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.04s/it, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.04s/it, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.16]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.00928]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.10s/it, train_loss=0.00928]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.10s/it, train_loss=0.0608] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.07s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.07s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.006] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.06s/it, train_loss=0.006]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.04s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.04s/it, train_loss=0.015] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.22it/s, train_loss=0.015]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59 average loss: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  59%|█████▉    | 59/100 [32:14<22:56, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 59 current AUC: 0.9934 current accuracy: 0.8509 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.05it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.07it/s, train_loss=0.0174] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.05s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.05s/it, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.19s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.19s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.03it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.03it/s, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.06s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.06s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.09s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.09s/it, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.14s/it, train_loss=0.06]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.14s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.14s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.14s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.16s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.16s/it, train_loss=0.122] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.11s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.11s/it, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.07s/it, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.07s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.04s/it, train_loss=0.014]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.01it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.01it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.01it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.02it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.02it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.00869]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.00s/it, train_loss=0.00869]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.00s/it, train_loss=0.0201] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.01s/it, train_loss=0.0201]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.01s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.02s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.02s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.02it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.02it/s, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.09it/s, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.09it/s, train_loss=0.0169] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.00it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.02it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.13]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.18s/it, train_loss=0.13]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.18s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.09s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.09s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.15it/s, train_loss=0.0644]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 average loss: 0.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 60/100 [32:46<22:08, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 60 current AUC: 0.9929 current accuracy: 0.9193 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.08it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.08it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.13it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.13it/s, train_loss=0.083] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.01it/s, train_loss=0.083]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.01it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.05s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.05s/it, train_loss=0.00469]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.00469]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.0908] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.08s/it, train_loss=0.0908]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.08s/it, train_loss=0.0768]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.0768]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.03s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.0596] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.09s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.057] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.057]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.05s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.0899]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.00s/it, train_loss=0.0899]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.00s/it, train_loss=0.00812]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.00812]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.0732] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.07s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.07s/it, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.12s/it, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.12s/it, train_loss=0.00549]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.16s/it, train_loss=0.00549]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.16s/it, train_loss=0.0196] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.19s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:14,  1.19s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.19s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.11s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.11s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.05s/it, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.03s/it, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.0788]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.02it/s, train_loss=0.0788]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.02it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.03it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.03it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.03it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.00512]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.03it/s, train_loss=0.00512]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.03it/s, train_loss=0.0264] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.02s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.07s/it, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.07s/it, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.12s/it, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.12s/it, train_loss=0.012] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.13it/s, train_loss=0.012]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61 average loss: 0.0454\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  61%|██████    | 61/100 [33:20<21:47, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 61 current AUC: 0.9984 current accuracy: 0.9565 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.12s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.12s/it, train_loss=0.0853]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.07it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.07s/it, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.07s/it, train_loss=0.0286] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:31,  1.23s/it, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:31,  1.23s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.23s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:30,  1.23s/it, train_loss=0.149] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.22s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.22s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.26s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.26s/it, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:25,  1.20s/it, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:25,  1.20s/it, train_loss=0.0809]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.08s/it, train_loss=0.0809]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.08s/it, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.01s/it, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.01s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.07s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.07s/it, train_loss=0.015] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.02s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.02s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.08s/it, train_loss=0.116] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.116]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.08s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.11s/it, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.0414] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.15s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.19s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.19s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.16s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.16s/it, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.07s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.12s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.12s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.19s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.19s/it, train_loss=0.104] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.19s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.19s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.18s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.18s/it, train_loss=0.0911]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.0911]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.223] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.02it/s, train_loss=0.223]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62 average loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 62/100 [33:56<21:36, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 62 current AUC: 0.9932 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.13s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.13s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.05it/s, train_loss=0.0218]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.05it/s, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.00s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.00s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.10s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:30,  1.22s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.22s/it, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.18s/it, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.18s/it, train_loss=0.0476]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:28,  1.24s/it, train_loss=0.0476]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:28,  1.24s/it, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.16s/it, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.16s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.08s/it, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.08s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.101] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.11s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.11s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.17s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.17s/it, train_loss=0.036] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.16s/it, train_loss=0.036]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.16s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.17s/it, train_loss=0.15]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.15]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.14s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.16s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.16s/it, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.21s/it, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.21s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.19s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.19s/it, train_loss=0.0308] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.28s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:11,  1.28s/it, train_loss=0.058] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:10,  1.28s/it, train_loss=0.058]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:10,  1.28s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.26s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:29<00:08,  1.26s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:07,  1.19s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:30<00:07,  1.19s/it, train_loss=0.225] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.15s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:31<00:05,  1.15s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.16s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.16s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.16s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.16s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.17s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.17s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:35<00:01,  1.15s/it, train_loss=0.00876]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:35<00:00,  1.00s/it, train_loss=0.00876]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63 average loss: 0.0522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  63%|██████▎   | 63/100 [34:32<21:25, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 63 current AUC: 0.9955 current accuracy: 0.9317 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.25s/it, train_loss=0.063]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.25s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.05s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.05s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:34,  1.22s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:34,  1.22s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.20s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.20s/it, train_loss=0.0383]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.17s/it, train_loss=0.0383]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.17s/it, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.21]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.02s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.02s/it, train_loss=0.022] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.022]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.01s/it, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.03it/s, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:20,  1.03it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.10s/it, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.1]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.08s/it, train_loss=0.109] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.109]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0459]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.06s/it, train_loss=0.0459]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.00s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.00s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.03s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.03s/it, train_loss=0.139] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.0742] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.02s/it, train_loss=0.0742]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.01s/it, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.00it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.00it/s, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.01s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.01s/it, train_loss=0.00536]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.02it/s, train_loss=0.00536]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.02it/s, train_loss=0.0671] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.00s/it, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.00s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.02s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.05s/it, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.05s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.08s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.187] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.187]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 average loss: 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  64%|██████▍   | 64/100 [35:05<20:32, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 64 current AUC: 0.9958 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.10s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.10s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.12s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.12s/it, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.08s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.08s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.14s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.14s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.15s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.15s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.14s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.14s/it, train_loss=0.0706]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0706]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.11s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.15s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.15s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.15s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.15s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.00891]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.10s/it, train_loss=0.00891]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.10s/it, train_loss=0.0231] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.12s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.12s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.13s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.13s/it, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.12s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.07s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.04s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.02s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.02s/it, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.02it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:06,  1.02it/s, train_loss=0.063] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.04it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.04it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.00s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.00s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.01it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.01it/s, train_loss=0.0062]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.04it/s, train_loss=0.0062]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.04it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.03s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.00436]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.00436]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 average loss: 0.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 65/100 [35:39<19:53, 34.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 65 current AUC: 0.9928 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.13s/it, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.13s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.12s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.12s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.02it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.02it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.03it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.03it/s, train_loss=0.013] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.013]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.02s/it, train_loss=0.073]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.073]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.13s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.13s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.14s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.0134] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.13s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.063] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.06s/it, train_loss=0.063]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.06s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.037] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.00s/it, train_loss=0.037]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.00s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.00it/s, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.00it/s, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.02s/it, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.02s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.02it/s, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.02it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.01s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.01s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.05s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.05s/it, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.21s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.21s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.11s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.11s/it, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.10s/it, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.10s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.04s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.0814]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.08s/it, train_loss=0.0814]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.08s/it, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.18s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.146] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.13s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.13s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.07s/it, train_loss=0.00485]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.00485]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.107]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.01s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.01s/it, train_loss=0.00786]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.23it/s, train_loss=0.00786]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66 average loss: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  66%|██████▌   | 66/100 [36:12<19:08, 33.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 66 current AUC: 0.9964 current accuracy: 0.9317 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.15s/it, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.15s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.0736]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.08s/it, train_loss=0.0736]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.08s/it, train_loss=0.0689]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0689]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.01s/it, train_loss=0.00802]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.04s/it, train_loss=0.00802]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.04s/it, train_loss=0.0767] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0136] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.06s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.06s/it, train_loss=0.269] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.05s/it, train_loss=0.269]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.10s/it, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.10s/it, train_loss=0.00498]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.10s/it, train_loss=0.00498]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.10s/it, train_loss=0.015]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.00989]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.07s/it, train_loss=0.00989]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.07s/it, train_loss=0.0473] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.12s/it, train_loss=0.024] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.11s/it, train_loss=0.024]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.11s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.13s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.077] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.19s/it, train_loss=0.077]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.16s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.16s/it, train_loss=0.0158] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.21s/it, train_loss=0.0158]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.21s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.18s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.18s/it, train_loss=0.0043]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.15s/it, train_loss=0.0043]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.00639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.24s/it, train_loss=0.00639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.24s/it, train_loss=0.069]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.22s/it, train_loss=0.069]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.22s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.14s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.14s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.06s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.13s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.13s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.14s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.14s/it, train_loss=0.0574]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.17s/it, train_loss=0.0574]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.17s/it, train_loss=0.811] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.09it/s, train_loss=0.811]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 average loss: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 67/100 [36:47<18:44, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 67 current AUC: 0.9960 current accuracy: 0.9255 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.05s/it, train_loss=0.0978]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:31,  1.05s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.14s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.14s/it, train_loss=0.00542]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.20s/it, train_loss=0.00542]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.20s/it, train_loss=0.00306]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.00306]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.157]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.157]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.23]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.16s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.04s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.03]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.03]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.13s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.13s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.052] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.16s/it, train_loss=0.052]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.16s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.11s/it, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.11s/it, train_loss=0.155] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.15] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.15]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.10s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.10s/it, train_loss=0.137] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.03s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.03s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.00s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.00s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.02s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.0937]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.00s/it, train_loss=0.0937]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.00s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.00it/s, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.00it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.06it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.06it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.09it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.09it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.08it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.08it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.06it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.06it/s, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.04it/s, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.04it/s, train_loss=0.0851]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.06s/it, train_loss=0.0851]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.148] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.148]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 average loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  68%|██████▊   | 68/100 [37:20<18:00, 33.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 68 current AUC: 0.9966 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0852]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.19s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.19s/it, train_loss=0.128] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:36,  1.25s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:36,  1.25s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.17s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.17s/it, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:32,  1.29s/it, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:32,  1.29s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.13s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.09s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.09s/it, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.06s/it, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.06s/it, train_loss=0.169] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.169]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.01s/it, train_loss=0.049]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.01s/it, train_loss=0.049]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.01s/it, train_loss=0.0571]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.0571]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.07s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.01it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.01it/s, train_loss=0.0468]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.09s/it, train_loss=0.0468]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.00855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.00855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.14s/it, train_loss=0.0186] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.10s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.10s/it, train_loss=0.143] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.09s/it, train_loss=0.143]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.09s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.09s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.07s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.10s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.11s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.09s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.09s/it, train_loss=0.011] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.011]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.02s/it, train_loss=0.00862]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.08s/it, train_loss=0.00862]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.08s/it, train_loss=0.132]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.04s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.04s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.0174]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 average loss: 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  69%|██████▉   | 69/100 [37:53<17:25, 33.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 69 current AUC: 0.9944 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00778]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.03s/it, train_loss=0.0361] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.12s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.12s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.11s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.11s/it, train_loss=0.0104] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.12s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.12s/it, train_loss=0.00983]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.00983]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.0419] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.08s/it, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.08s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.12s/it, train_loss=0.0527] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.17s/it, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.17s/it, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.031] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.13s/it, train_loss=0.031]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.13s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.05s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.05s/it, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.06s/it, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.03s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.02it/s, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:12,  1.02it/s, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.04it/s, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:11,  1.04it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:10,  1.04it/s, train_loss=0.0499] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.05it/s, train_loss=0.0499]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:09,  1.05it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.01s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.01s/it, train_loss=0.0637]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:09,  1.14s/it, train_loss=0.0637]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.14s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.112]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.037]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.037]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.12s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.0342] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.12s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.14s/it, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.14s/it, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.19s/it, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.19s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0654]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.0654]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70 average loss: 0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 70/100 [38:27<16:50, 33.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 70 current AUC: 0.9938 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.07it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.07it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.02s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.02s/it, train_loss=0.00944]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.08s/it, train_loss=0.00944]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.08s/it, train_loss=0.0132] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.0941]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.0941]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.04s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.04s/it, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.03it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.03it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.02it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.02it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.09s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.00614]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.05it/s, train_loss=0.00614]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.05it/s, train_loss=0.0206] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.02s/it, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.02s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.08s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.08s/it, train_loss=0.00745]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.10s/it, train_loss=0.00745]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.10s/it, train_loss=0.0141] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:12,  1.13s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.13s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.04s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.04s/it, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.07s/it, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.07s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.06s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.0348]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.07s/it, train_loss=0.0348]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:06,  1.20s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:06,  1.20s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.20s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.20s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.15s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.15s/it, train_loss=0.00583]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.15s/it, train_loss=0.00583]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.0426] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.07it/s, train_loss=0.0151]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71 average loss: 0.0347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  71%|███████   | 71/100 [39:00<16:14, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 71 current AUC: 0.9972 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00556]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.00556]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.05it/s, train_loss=0.0427] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.04s/it, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0437]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.0437]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.02s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.02s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.14s/it, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.14s/it, train_loss=0.0079]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.13s/it, train_loss=0.0079]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.13s/it, train_loss=0.155] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.16s/it, train_loss=0.155]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.16s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.00662]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.18s/it, train_loss=0.00662]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.18s/it, train_loss=0.00652]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.18s/it, train_loss=0.00652]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.18s/it, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.21s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.21s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.19s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.19s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.21s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.21s/it, train_loss=0.0165] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.23s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.23s/it, train_loss=0.00323]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.25s/it, train_loss=0.00323]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.25s/it, train_loss=0.114]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:16,  1.26s/it, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.26s/it, train_loss=0.00472]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.00472]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.0382] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.15s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.14s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.14s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.15s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.16s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.16s/it, train_loss=0.246] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.07s/it, train_loss=0.246]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.07s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.09s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.09s/it, train_loss=0.0395] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.13s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.13s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.18s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.18s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.19s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.19s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.16s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.17s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.17s/it, train_loss=0.00441]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.07it/s, train_loss=0.00441]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72 average loss: 0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  72%|███████▏  | 72/100 [39:36<15:56, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 72 current AUC: 0.9923 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00498]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.09s/it, train_loss=0.00498]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.09s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.13s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.13s/it, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.013]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.10s/it, train_loss=0.013]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.10s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.10s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.00464]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.12s/it, train_loss=0.00464]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0139] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:23,  1.09s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.00595]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.00595]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.10s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.13s/it, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.00819]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.00819]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.0129] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.18s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.22s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.22s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.09s/it, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.09s/it, train_loss=0.185] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=0.185]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.06s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.06s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.06s/it, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.09s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.02s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.05s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.13s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.13s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.09s/it, train_loss=0.00505]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.11s/it, train_loss=0.00505]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0411] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.03s/it, train_loss=0.0411]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.03s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.11s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.12s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.12s/it, train_loss=0.0398] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.0364]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.12it/s, train_loss=0.0364]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 average loss: 0.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  73%|███████▎  | 73/100 [40:10<15:24, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 73 current AUC: 0.9955 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0743]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.15s/it, train_loss=0.0743]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.15s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.22s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.22s/it, train_loss=0.00322]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.12s/it, train_loss=0.00322]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.12s/it, train_loss=0.00296]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.19s/it, train_loss=0.00296]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.19s/it, train_loss=0.0062] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.18s/it, train_loss=0.0062]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.18s/it, train_loss=0.029] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.18s/it, train_loss=0.029]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.18s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.02s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.02s/it, train_loss=0.00207]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.00207]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.01s/it, train_loss=0.1]    \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.06s/it, train_loss=0.1]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.06s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.05s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.05s/it, train_loss=0.088] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.088]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.02s/it, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.04s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.05s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.05s/it, train_loss=0.00702]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.09s/it, train_loss=0.00702]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.0179] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.08s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.05s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.05s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.00s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.00s/it, train_loss=0.0122] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.01s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.00s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.00s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.12s/it, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.11s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.11s/it, train_loss=0.121] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.13s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.13s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.12s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.09s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.09s/it, train_loss=0.00711]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.09s/it, train_loss=0.00711]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.09s/it, train_loss=0.0299] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.00788]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.00788]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74 average loss: 0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  74%|███████▍  | 74/100 [40:44<14:44, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 74 current AUC: 0.9948 current accuracy: 0.9565 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.07it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.07it/s, train_loss=0.0953]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.0953]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.01s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.05s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.05s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.0529] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.16s/it, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:27,  1.18s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.18s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.20s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.20s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.19s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.19s/it, train_loss=0.00239]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.17s/it, train_loss=0.00239]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.17s/it, train_loss=0.0168] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.14s/it, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.14s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.13s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.13s/it, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.14s/it, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.14s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.00977]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.16s/it, train_loss=0.00977]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.16s/it, train_loss=0.0361] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.00325]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.00325]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.00573]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.08s/it, train_loss=0.00573]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.00366]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.06s/it, train_loss=0.00366]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.06s/it, train_loss=0.0141] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.03s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.00it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:06,  1.00it/s, train_loss=0.0072]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.01it/s, train_loss=0.0072]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.01it/s, train_loss=0.00804]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.00804]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:04,  1.03it/s, train_loss=0.0325] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.08it/s, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.08it/s, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.01s/it, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.01s/it, train_loss=0.0105] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.06s/it, train_loss=0.0568]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.0568]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.0084]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.14it/s, train_loss=0.0084]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75 average loss: 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  75%|███████▌  | 75/100 [41:17<14:07, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 75 current AUC: 0.9959 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0574]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.0574]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.05it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.11s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.11s/it, train_loss=0.00339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.00339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.0543] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:33,  1.23s/it, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:33,  1.23s/it, train_loss=0.00865]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:31,  1.21s/it, train_loss=0.00865]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:31,  1.21s/it, train_loss=0.00302]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.00302]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.00996]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.19s/it, train_loss=0.00996]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.19s/it, train_loss=0.00119]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.00119]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.00994]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.00994]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.13s/it, train_loss=0.011]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.08s/it, train_loss=0.011]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.08s/it, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.02s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.01it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:18,  1.01it/s, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.03s/it, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.03s/it, train_loss=0.0192] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.03s/it, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.03s/it, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.00it/s, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.00it/s, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.05s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.05s/it, train_loss=0.0147] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.06s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.06s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.03s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.03s/it, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.05s/it, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.05s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.00529]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.00s/it, train_loss=0.00529]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.00s/it, train_loss=0.021]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.05s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.00939]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.01it/s, train_loss=0.00939]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.01it/s, train_loss=0.0497] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.0497]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.027] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.03s/it, train_loss=0.027]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.03s/it, train_loss=0.00352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.01it/s, train_loss=0.00352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.01it/s, train_loss=0.0117] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.04s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.04s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.02s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.00204]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.01it/s, train_loss=0.00204]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.01it/s, train_loss=0.0265] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.24it/s, train_loss=0.0265]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76 average loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  76%|███████▌  | 76/100 [41:50<13:26, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 76 current AUC: 0.9969 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.02it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.02it/s, train_loss=0.00558]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.07s/it, train_loss=0.00558]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.07s/it, train_loss=0.00522]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.04s/it, train_loss=0.00522]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.04s/it, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.04it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.04it/s, train_loss=0.0032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.04it/s, train_loss=0.0032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.04it/s, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.02it/s, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.02it/s, train_loss=0.0287] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.04s/it, train_loss=0.0287]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.04s/it, train_loss=0.183] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.05s/it, train_loss=0.183]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.05s/it, train_loss=0.00422]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.04s/it, train_loss=0.00422]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.0477] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.04it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.04it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:17,  1.07it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.07it/s, train_loss=0.00714]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.09it/s, train_loss=0.00714]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.09it/s, train_loss=0.107]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.08it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.08it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.03it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.03it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.01it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.00773]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.02s/it, train_loss=0.00773]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.02s/it, train_loss=0.0123] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.05s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.05s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.13s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:12,  1.11s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.11s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:11,  1.15s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.15s/it, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:10,  1.19s/it, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.19s/it, train_loss=0.0357] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:09,  1.19s/it, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.19s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:08,  1.14s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.14s/it, train_loss=0.00335]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.12s/it, train_loss=0.00335]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.12s/it, train_loss=0.0101] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.11s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.11s/it, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.09s/it, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.09s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.16s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.16s/it, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.17s/it, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.17s/it, train_loss=0.00452]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.16s/it, train_loss=0.00452]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.00341]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.00341]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77 average loss: 0.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  77%|███████▋  | 77/100 [42:23<12:47, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 77 current AUC: 0.9976 current accuracy: 0.9503 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00391]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.00it/s, train_loss=0.00391]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.00it/s, train_loss=0.00519]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.00519]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.0041] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.0041]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.00538]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.00538]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.0015] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.0015]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.03s/it, train_loss=0.00172]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.05s/it, train_loss=0.00172]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.00251]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.01it/s, train_loss=0.00251]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.01it/s, train_loss=0.066]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.04it/s, train_loss=0.066]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.04it/s, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.05it/s, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.05it/s, train_loss=0.00836]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.08it/s, train_loss=0.00836]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:16,  1.08it/s, train_loss=0.014]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.00s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.00s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.07s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.07s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.04s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.00394]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.10s/it, train_loss=0.00394]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.0501] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.18s/it, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.18s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.15s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.18s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.18s/it, train_loss=0.00223]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.16s/it, train_loss=0.00223]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.16s/it, train_loss=0.00719]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:11,  1.23s/it, train_loss=0.00719]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.23s/it, train_loss=0.00449]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.20s/it, train_loss=0.00449]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.20s/it, train_loss=0.015]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.19s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.19s/it, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:07,  1.20s/it, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.20s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.16s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.16s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.15s/it, train_loss=0.101]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.12s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.12s/it, train_loss=0.0049]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.13s/it, train_loss=0.0049]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.09s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.09s/it, train_loss=0.00904]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.16it/s, train_loss=0.00904]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78 average loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  78%|███████▊  | 78/100 [42:57<12:18, 33.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 78 current AUC: 0.9954 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00194]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.14s/it, train_loss=0.00194]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.14s/it, train_loss=0.0331] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.21s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.21s/it, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.00763]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.22s/it, train_loss=0.00763]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:32,  1.22s/it, train_loss=0.225]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:33,  1.30s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:33,  1.30s/it, train_loss=0.0201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.0201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:25,  1.06s/it, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.05s/it, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.05s/it, train_loss=0.00659]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.00659]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.04s/it, train_loss=0.00194]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.00194]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.00376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.18s/it, train_loss=0.00376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.18s/it, train_loss=0.0109] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.18s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.18s/it, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.15s/it, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.15s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.13s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.13s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.12s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:17,  1.12s/it, train_loss=0.00392]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.00392]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:15,  1.06s/it, train_loss=0.0167] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.00247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.15s/it, train_loss=0.00247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:14,  1.15s/it, train_loss=0.0265] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.25s/it, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.25s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.24s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.24s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.24s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:12,  1.24s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.25s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:11,  1.25s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.13s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.13s/it, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.05s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.05s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.00it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:05,  1.00it/s, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.08s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.08s/it, train_loss=0.0477] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.14s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.14s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.27s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.27s/it, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.20s/it, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.20s/it, train_loss=0.00958]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.00958]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.00673]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.10it/s, train_loss=0.00673]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79 average loss: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  79%|███████▉  | 79/100 [43:32<11:57, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 79 current AUC: 0.9961 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.004]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.004]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.05it/s, train_loss=0.0029]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0029]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.00it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.00it/s, train_loss=0.00305]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.00305]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.0142] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.18s/it, train_loss=0.02]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.18s/it, train_loss=0.00876]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.00876]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.15s/it, train_loss=0.00206]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.00206]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.04s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.04s/it, train_loss=0.17]   \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.16s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.16s/it, train_loss=0.00713]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.00713]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.0211] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.14s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.00995]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.16s/it, train_loss=0.00995]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.16s/it, train_loss=0.00453]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:19,  1.19s/it, train_loss=0.00453]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.19s/it, train_loss=0.0157] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.16s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.16s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.0481] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.00932]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.19s/it, train_loss=0.00932]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.19s/it, train_loss=0.0269] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.19s/it, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.19s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.20s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.20s/it, train_loss=0.0297] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.19s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.19s/it, train_loss=0.00643]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.22s/it, train_loss=0.00643]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.22s/it, train_loss=0.0391] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.15s/it, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.15s/it, train_loss=0.00495]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.10s/it, train_loss=0.00495]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.10s/it, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.12s/it, train_loss=0.149]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.10s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.10s/it, train_loss=0.00828]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.07s/it, train_loss=0.00828]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.07s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.03s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.03s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.19it/s, train_loss=0.00778]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 average loss: 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 80/100 [44:07<11:27, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 80 current AUC: 0.9960 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00855]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.15s/it, train_loss=0.00855]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.15s/it, train_loss=0.0253] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.18s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.18s/it, train_loss=0.000811]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.15s/it, train_loss=0.000811]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.15s/it, train_loss=0.0749]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.15s/it, train_loss=0.0749]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.15s/it, train_loss=0.0035]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.15s/it, train_loss=0.0035]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.15s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.14s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.14s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.09s/it, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.09s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.09s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.09s/it, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.09s/it, train_loss=0.00269]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.00269]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.05s/it, train_loss=0.00638]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.00638]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.00231]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.11s/it, train_loss=0.00231]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.11s/it, train_loss=0.0479] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.09s/it, train_loss=0.00764]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.00764]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0091] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.07s/it, train_loss=0.0091]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.07s/it, train_loss=0.00696]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.00696]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.10s/it, train_loss=0.0252] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.071] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.20s/it, train_loss=0.071]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.20s/it, train_loss=0.00587]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.17s/it, train_loss=0.00587]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.17s/it, train_loss=0.00319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.17s/it, train_loss=0.00319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.17s/it, train_loss=0.0115] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.15s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.00694]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.00694]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0187] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.10s/it, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.13s/it, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.13s/it, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.18s/it, train_loss=0.00835]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.19s/it, train_loss=0.00835]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.19s/it, train_loss=0.125]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.16s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.16s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.09s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.09s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.00703]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.25it/s, train_loss=0.00703]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81 average loss: 0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  81%|████████  | 81/100 [44:42<10:53, 34.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 81 current AUC: 0.9950 current accuracy: 0.9006 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00398]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.04it/s, train_loss=0.00398]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.04it/s, train_loss=0.064]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.064]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.09it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.09it/s, train_loss=0.00655]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:23,  1.14it/s, train_loss=0.00655]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:23,  1.14it/s, train_loss=0.00334]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.00it/s, train_loss=0.00334]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.00it/s, train_loss=0.0133] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:25,  1.00s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.00s/it, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.03it/s, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.03it/s, train_loss=0.00265]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:23,  1.01s/it, train_loss=0.00265]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.01s/it, train_loss=0.00168]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.02s/it, train_loss=0.00168]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.0112] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:21,  1.02s/it, train_loss=0.0112]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.02s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.00s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.00s/it, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.00it/s, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.00878]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:18,  1.01s/it, train_loss=0.00878]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.0655] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.03it/s, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.02s/it, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.02s/it, train_loss=0.155] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.09s/it, train_loss=0.155]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.09s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.12s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.13s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.13s/it, train_loss=0.00503]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.09s/it, train_loss=0.00503]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.09s/it, train_loss=0.0525] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:12,  1.12s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.12s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:11,  1.12s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.0302] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:10,  1.13s/it, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.13s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.08s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.0106] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.07s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.03s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.03s/it, train_loss=0.0206] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.05s/it, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.05s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.05s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.05s/it, train_loss=0.00398]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.03s/it, train_loss=0.00398]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.03s/it, train_loss=0.0305] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.05s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.05s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.11s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.11s/it, train_loss=0.0434]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.12it/s, train_loss=0.0434]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 average loss: 0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 82/100 [45:14<10:08, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 82 current AUC: 0.9884 current accuracy: 0.8882 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:39,  1.33s/it, train_loss=0.00279]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:39,  1.33s/it, train_loss=0.0203] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.15s/it, train_loss=0.0203]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.15s/it, train_loss=0.00968]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.00968]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.0652] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.01s/it, train_loss=0.0054]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.04s/it, train_loss=0.0054]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.04s/it, train_loss=0.00669]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.00669]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.053]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.14s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.14s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.00447]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.00447]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.00957]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.11s/it, train_loss=0.00957]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.11s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.17s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.17s/it, train_loss=0.13]   \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.13]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.13s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.16s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.16s/it, train_loss=0.00615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.00615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.19s/it, train_loss=0.00147]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.00147]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.0311] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.17s/it, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.17s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:12,  1.20s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.20s/it, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.14s/it, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.14s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.12s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.12s/it, train_loss=0.0177] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.13s/it, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.13s/it, train_loss=0.0682]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0682]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.08s/it, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.08s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.11s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.0159] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.17s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.17s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.10it/s, train_loss=0.00637]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83 average loss: 0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  83%|████████▎ | 83/100 [45:49<09:39, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 83 current AUC: 0.9938 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.30s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.30s/it, train_loss=0.00365]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.20s/it, train_loss=0.00365]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.20s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.13s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.13s/it, train_loss=0.0106] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.16s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.16s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.16s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.16s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.00377]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.00377]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.00203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.00203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:23,  1.09s/it, train_loss=0.0389] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.09s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.05s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.09s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.09s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.05s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.05s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.05s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.05s/it, train_loss=0.00182]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.10s/it, train_loss=0.00182]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.10s/it, train_loss=0.063]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.17s/it, train_loss=0.063]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.0028]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.09s/it, train_loss=0.0028]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.09s/it, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.00341]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.00341]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.0104] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.03s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.03s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.09s/it, train_loss=0.025] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.025]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.12s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.309] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.13s/it, train_loss=0.309]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.13s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.11s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.0081]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.0081]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.15s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0865]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.06it/s, train_loss=0.0865]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84 average loss: 0.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  84%|████████▍ | 84/100 [46:23<09:06, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 84 current AUC: 0.9910 current accuracy: 0.8882 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00714]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.22s/it, train_loss=0.00714]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.22s/it, train_loss=0.00746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:36,  1.27s/it, train_loss=0.00746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:36,  1.27s/it, train_loss=0.0452] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.14s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.14s/it, train_loss=0.00648]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.00648]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0126] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.19s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.15s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.15s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.10s/it, train_loss=0.0355]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0355]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.00755]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.00755]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0144] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.01s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.0082]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.08s/it, train_loss=0.0082]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.08s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.11s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.11s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.17s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.17s/it, train_loss=0.033] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.033]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.17s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.14s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.18s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.00721]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.22s/it, train_loss=0.00721]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.22s/it, train_loss=0.031]  \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.24s/it, train_loss=0.031]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:12,  1.24s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.18s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.18s/it, train_loss=0.152] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.14s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.14s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.09s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.03s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.03s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.01it/s, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:04,  1.01it/s, train_loss=0.00362]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.02it/s, train_loss=0.00362]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.02it/s, train_loss=0.0103] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.01it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:02,  1.01it/s, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.02s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.03s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.03s/it, train_loss=0.00484]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.00484]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85 average loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  85%|████████▌ | 85/100 [46:58<08:32, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 85 current AUC: 0.9922 current accuracy: 0.8696 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00223]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.00s/it, train_loss=0.00223]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.00s/it, train_loss=0.0342] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.11s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.11s/it, train_loss=0.00933]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.04s/it, train_loss=0.00933]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.04s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.01s/it, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.125]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.01s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.01s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.02it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:23,  1.02it/s, train_loss=0.0036]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0036]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.16s/it, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.16s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.12s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.13s/it, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.16s/it, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.16s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.13s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.13s/it, train_loss=0.00704]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.14s/it, train_loss=0.00704]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.14s/it, train_loss=0.0351] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.17s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.17s/it, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.20s/it, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.20s/it, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.20s/it, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.20s/it, train_loss=0.00236]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:15,  1.26s/it, train_loss=0.00236]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:15,  1.26s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.18s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.18s/it, train_loss=0.0988] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.19s/it, train_loss=0.0988]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.19s/it, train_loss=0.00791]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.14s/it, train_loss=0.00791]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.14s/it, train_loss=0.0436] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.13s/it, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.13s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.11s/it, train_loss=0.112]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.11s/it, train_loss=0.023]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.023]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.07s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.07s/it, train_loss=0.0226] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.029] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.15s/it, train_loss=0.029]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.15s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.12s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.12s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.15s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.0576]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.09it/s, train_loss=0.0576]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86 average loss: 0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|████████▌ | 86/100 [47:32<08:01, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 86 current AUC: 0.9944 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.05s/it, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.05s/it, train_loss=0.00827]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.01it/s, train_loss=0.00827]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:28,  1.01it/s, train_loss=0.0113] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.06s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.06s/it, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.00767]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.04it/s, train_loss=0.00767]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.04it/s, train_loss=0.00735]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.00735]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.0278] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.01it/s, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:23,  1.01it/s, train_loss=0.00792]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.00s/it, train_loss=0.00792]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.00s/it, train_loss=0.00328]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.06s/it, train_loss=0.00328]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.06s/it, train_loss=0.021]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.05s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.05s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.12s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:22,  1.17s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.184] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.19s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.19s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.22s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.22s/it, train_loss=0.00913]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:19,  1.20s/it, train_loss=0.00913]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.20s/it, train_loss=0.0408] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.18s/it, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.18s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.11s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.11s/it, train_loss=0.039] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.06s/it, train_loss=0.039]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.06s/it, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.15s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:12,  1.25s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.25s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:11,  1.31s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:11,  1.31s/it, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:10,  1.30s/it, train_loss=0.02]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:10,  1.30s/it, train_loss=0.00942]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.25s/it, train_loss=0.00942]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.25s/it, train_loss=0.0337] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.19s/it, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:07,  1.19s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.10s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.10s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.04s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.04s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.02s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.02s/it, train_loss=0.0161] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.08s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.08s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.103]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.103]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87 average loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  87%|████████▋ | 87/100 [48:07<07:26, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 87 current AUC: 0.9968 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00533]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.18s/it, train_loss=0.00533]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.18s/it, train_loss=0.0163] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.18s/it, train_loss=0.0163]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.18s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.06s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.06s/it, train_loss=0.0655] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.07s/it, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.07s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.04s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.04s/it, train_loss=0.201] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0808] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.106] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.12s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.12s/it, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.09s/it, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.00309]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.08s/it, train_loss=0.00309]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.08s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.19s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.19s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.20s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.20s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.18s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.18s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.15s/it, train_loss=0.18]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.26s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.26s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.23s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.23s/it, train_loss=0.00446]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.20s/it, train_loss=0.00446]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.20s/it, train_loss=0.0188] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.15s/it, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.14s/it, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.14s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.12s/it, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.14s/it, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.14s/it, train_loss=0.0338]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.11s/it, train_loss=0.0338]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.11s/it, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.04s/it, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.04s/it, train_loss=0.0604] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.11s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.11s/it, train_loss=0.0213] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.12s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.12s/it, train_loss=0.00863]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.14s/it, train_loss=0.00863]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.14s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.14s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.14s/it, train_loss=0.0705]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.08it/s, train_loss=0.0705]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88 average loss: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|████████▊ | 88/100 [48:42<06:55, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 88 current AUC: 0.9945 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.23s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.23s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.0267] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0918]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.0918]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.09s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.16s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.16s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.16s/it, train_loss=0.00482]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.00482]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.0123] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.24s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:12<00:27,  1.24s/it, train_loss=0.044] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:26,  1.26s/it, train_loss=0.044]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:13<00:26,  1.26s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:24,  1.24s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:24,  1.24s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.20s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.20s/it, train_loss=0.00444]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.18s/it, train_loss=0.00444]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:21,  1.18s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.14s/it, train_loss=0.0059] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0059]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:17,  1.09s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.12s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.12s/it, train_loss=0.0184] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.13s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.05s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.05s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.11s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.11s/it, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.07s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.05s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.05s/it, train_loss=0.00451]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.08s/it, train_loss=0.00451]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.08s/it, train_loss=0.00178]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.13s/it, train_loss=0.00178]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.13s/it, train_loss=0.00607]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.16s/it, train_loss=0.00607]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.16s/it, train_loss=0.031]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.15s/it, train_loss=0.031]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.15s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.08s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.08s/it, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.05s/it, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.05s/it, train_loss=0.052]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.052]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.03s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.06s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.06s/it, train_loss=0.021]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.07s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.07s/it, train_loss=0.0504]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.16it/s, train_loss=0.0504]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89 average loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  89%|████████▉ | 89/100 [49:17<06:21, 34.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 89 current AUC: 0.9955 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.26s/it, train_loss=0.0447]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.26s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.16s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.16s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.10s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.10s/it, train_loss=0.00825]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.00825]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.00577]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.00577]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.0239] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.09s/it, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.15s/it, train_loss=0.102] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.14s/it, train_loss=0.0214]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.0214]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.00564]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.05s/it, train_loss=0.00564]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.05s/it, train_loss=0.0955] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.11s/it, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.14s/it, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.15s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.15s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.15s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.15s/it, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.21s/it, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.21s/it, train_loss=0.0706] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.0706]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.19s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.16s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.16s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.12s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.12s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.12s/it, train_loss=0.238] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.13s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.13s/it, train_loss=0.0556]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.0556]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.09s/it, train_loss=0.0063]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.0063]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.08s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.055] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.18s/it, train_loss=0.055]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.18s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.22s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.22s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.16s/it, train_loss=0.215] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.18s/it, train_loss=0.215]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.18s/it, train_loss=0.0526]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.03it/s, train_loss=0.0526]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90 average loss: 0.0449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 90/100 [49:52<05:48, 34.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 90 current AUC: 0.9947 current accuracy: 0.8882 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.00591]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.03it/s, train_loss=0.00591]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.03it/s, train_loss=0.0294] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.06it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.06it/s, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.0272] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.04it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.04it/s, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.03s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.403]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.08s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.08s/it, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.04s/it, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.04s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.13s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.00255]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.11s/it, train_loss=0.00255]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.11s/it, train_loss=0.0462] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.04s/it, train_loss=0.0462]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.00it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.00it/s, train_loss=0.029] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.029]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.05it/s, train_loss=0.013] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.08it/s, train_loss=0.013]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.08it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.05it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.05it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.03it/s, train_loss=0.00991]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.02it/s, train_loss=0.00991]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.02it/s, train_loss=0.0263] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.01s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.01s/it, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.01s/it, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.03it/s, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.00s/it, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.00s/it, train_loss=0.137]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.06s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.02s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.02s/it, train_loss=0.021] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.03it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.03it/s, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.04it/s, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.04it/s, train_loss=0.0496] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.0496]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.0813]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.31it/s, train_loss=0.0813]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91 average loss: 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  91%|█████████ | 91/100 [50:23<05:03, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 91 current AUC: 0.9898 current accuracy: 0.8323 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.17s/it, train_loss=0.0467]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.17s/it, train_loss=0.00948]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.08s/it, train_loss=0.00948]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.08s/it, train_loss=0.0403] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.05s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.05s/it, train_loss=0.00795]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.00795]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.0392] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.13s/it, train_loss=0.0392]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.13s/it, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.13s/it, train_loss=0.0805]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.0805]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.01it/s, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.01it/s, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.0378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.0503]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.08s/it, train_loss=0.0503]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.08s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.05s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.0738] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.05s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.05s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.03s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.03s/it, train_loss=0.00596]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.00596]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0262] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.13s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.15s/it, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.15s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.0363]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0363]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.10s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.10s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.03s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.03s/it, train_loss=0.082] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.09s/it, train_loss=0.082]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.04s/it, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.04s/it, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.02it/s, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.02it/s, train_loss=0.0265] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.03it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.122] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.01s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.01it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.01it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.05it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.05it/s, train_loss=0.00726]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.07it/s, train_loss=0.00726]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.07it/s, train_loss=0.267]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.28it/s, train_loss=0.267]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 average loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  92%|█████████▏| 92/100 [50:56<04:27, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 92 current AUC: 0.9900 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.03it/s, train_loss=0.0117] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.07it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.07it/s, train_loss=0.00794]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s, train_loss=0.00794]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.11it/s, train_loss=0.0143] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.10it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.10it/s, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:27,  1.06s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.06s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.12s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.12s/it, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:29,  1.21s/it, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.21s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:28,  1.24s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:28,  1.24s/it, train_loss=0.0158]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.22s/it, train_loss=0.0158]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.22s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.00535]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.00535]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.0578] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.0578]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.08s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.00361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.00361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.03s/it, train_loss=0.0107] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.01s/it, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.01s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.05s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.05s/it, train_loss=0.0598] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0598]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.0914] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.07s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.06s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.06s/it, train_loss=0.00966]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.10s/it, train_loss=0.00966]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.11s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0461] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.15s/it, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.15s/it, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.15s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.06s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.386] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.21it/s, train_loss=0.386]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93 average loss: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  93%|█████████▎| 93/100 [51:29<03:53, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 93 current AUC: 0.9950 current accuracy: 0.9379 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00182]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.27s/it, train_loss=0.00182]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.27s/it, train_loss=0.0174] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.17s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.17s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.18s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.18s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:33,  1.22s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:33,  1.22s/it, train_loss=0.096] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:31,  1.23s/it, train_loss=0.096]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:31,  1.23s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.16s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.16s/it, train_loss=0.0044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.17s/it, train_loss=0.0044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.17s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.09s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.09s/it, train_loss=0.0151] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.13s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.16s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.16s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.10s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.10s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.08s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.08s/it, train_loss=0.00642]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.00642]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:18,  1.08s/it, train_loss=0.00332]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.13s/it, train_loss=0.00332]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.13s/it, train_loss=0.0928] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.09s/it, train_loss=0.267] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.08s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.04s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.04s/it, train_loss=0.111] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:11,  1.00it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:11,  1.00it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.03s/it, train_loss=0.192]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.03s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.02s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.02s/it, train_loss=0.0333] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:08,  1.01it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:08,  1.01it/s, train_loss=0.139] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.03s/it, train_loss=0.0073]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.0073]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.24]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.08s/it, train_loss=0.24]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.08s/it, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.02s/it, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.02s/it, train_loss=0.0555]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.03it/s, train_loss=0.0555]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.03it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.04it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:01,  1.04it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.03it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.03it/s, train_loss=0.053] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.20it/s, train_loss=0.053]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 average loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  94%|█████████▍| 94/100 [52:03<03:20, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 94 current AUC: 0.9909 current accuracy: 0.8447 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.02s/it, train_loss=0.0516]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.02s/it, train_loss=0.0222]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.07s/it, train_loss=0.0222]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.07s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.04it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.04it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.01it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.01it/s, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.00s/it, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.00s/it, train_loss=0.00434]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.00434]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.0233] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.13s/it, train_loss=0.0233]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.0369]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.14s/it, train_loss=0.0369]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.14s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.05s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.05s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.07s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.07s/it, train_loss=0.00385]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.00385]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.008]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.09s/it, train_loss=0.008]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.14s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.14s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.13s/it, train_loss=0.017] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.11s/it, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.16s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.16s/it, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.06s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.04s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=0.0262] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.017] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.08s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.02s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.02s/it, train_loss=0.28]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.00it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.00it/s, train_loss=0.0741]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.00it/s, train_loss=0.0741]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.00it/s, train_loss=0.0975]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.03s/it, train_loss=0.0975]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.0667]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.09s/it, train_loss=0.0667]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.09s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.21it/s, train_loss=0.0119]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95 average loss: 0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  95%|█████████▌| 95/100 [52:36<02:46, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 95 current AUC: 0.9918 current accuracy: 0.8820 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00407]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.00407]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.00483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.08it/s, train_loss=0.00483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.08it/s, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.01it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.01it/s, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.01s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.03s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.199] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.199]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.056]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.10s/it, train_loss=0.056]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.10s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.11s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.00875]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.07s/it, train_loss=0.00875]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.07s/it, train_loss=0.00342]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.00342]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.0917] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.08s/it, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.00584]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.08s/it, train_loss=0.00584]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.00368]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.08s/it, train_loss=0.00368]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.012]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.05s/it, train_loss=0.012]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.05s/it, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.02s/it, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.02s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.03s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.03s/it, train_loss=0.0293] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.00it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.00it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.00it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.00it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.06s/it, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.06s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.10s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.06s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.15s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.15s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.18s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.18s/it, train_loss=0.00861]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.15s/it, train_loss=0.00861]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.15s/it, train_loss=0.0194] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.13s/it, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.00754]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.00754]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.00197]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.10it/s, train_loss=0.00197]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 average loss: 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  96%|█████████▌| 96/100 [53:09<02:13, 33.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 96 current AUC: 0.9958 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.00367]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.03it/s, train_loss=0.00367]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.03it/s, train_loss=0.0497] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.07it/s, train_loss=0.0497]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.07it/s, train_loss=0.021] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.05it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.05it/s, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.10it/s, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.10it/s, train_loss=0.0288] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:25,  1.03s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.00281]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.12s/it, train_loss=0.00281]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.12s/it, train_loss=0.00853]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.15s/it, train_loss=0.00853]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:24,  1.25s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:24,  1.25s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:23,  1.25s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:23,  1.25s/it, train_loss=0.012]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.21s/it, train_loss=0.012]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.21s/it, train_loss=0.00245]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.00245]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.00581]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.14s/it, train_loss=0.00581]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.14s/it, train_loss=0.0247] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.13s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.13s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.13s/it, train_loss=0.0053]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.12s/it, train_loss=0.0053]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.12s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.13s/it, train_loss=0.00248]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.15s/it, train_loss=0.00248]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.15s/it, train_loss=0.00603]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.00603]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.038]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.038]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.08s/it, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.03s/it, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.03s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.07s/it, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.04s/it, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.04s/it, train_loss=0.00765]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.00765]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0134] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.02it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.05it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:01,  1.05it/s, train_loss=0.0161] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.0151]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.34it/s, train_loss=0.0151]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97 average loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  97%|█████████▋| 97/100 [53:42<01:39, 33.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 97 current AUC: 0.9930 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00586]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.13s/it, train_loss=0.00586]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.13s/it, train_loss=0.00406]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.22s/it, train_loss=0.00406]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.22s/it, train_loss=0.0247] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:34,  1.23s/it, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:34,  1.23s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:34,  1.27s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:34,  1.27s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:29,  1.14s/it, train_loss=0.00372]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.22s/it, train_loss=0.00372]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:30,  1.22s/it, train_loss=0.00221]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.19s/it, train_loss=0.00221]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.19s/it, train_loss=0.00287]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.00287]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.18s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.18s/it, train_loss=0.0688] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.114] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.10s/it, train_loss=0.114]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:21,  1.10s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.17s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.11s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:19,  1.11s/it, train_loss=0.0347] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.11s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:18,  1.11s/it, train_loss=0.00138]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.17s/it, train_loss=0.00138]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.17s/it, train_loss=0.00496]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.16s/it, train_loss=0.00496]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.16s/it, train_loss=0.00762]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.19s/it, train_loss=0.00762]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:21<00:16,  1.19s/it, train_loss=0.00137]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.23s/it, train_loss=0.00137]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:15,  1.23s/it, train_loss=0.00565]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.13s/it, train_loss=0.00565]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:13,  1.13s/it, train_loss=0.0391] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.08s/it, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:11,  1.08s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.04s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.04s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.13s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.13s/it, train_loss=0.00664]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.15s/it, train_loss=0.00664]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.15s/it, train_loss=0.00308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.17s/it, train_loss=0.00308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.17s/it, train_loss=0.00766]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.15s/it, train_loss=0.00766]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.15s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.10s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.10s/it, train_loss=0.0328] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.0328]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.11s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.09s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.09s/it, train_loss=0.00288]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.03s/it, train_loss=0.00288]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.03s/it, train_loss=0.0394] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.18it/s, train_loss=0.0394]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98 average loss: 0.0184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  98%|█████████▊| 98/100 [54:17<01:07, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 98 current AUC: 0.9927 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0026]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.08it/s, train_loss=0.0026]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.08it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.00s/it, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.00s/it, train_loss=0.0347] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.03s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.03s/it, train_loss=0.00405]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.00405]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.09s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.15s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.15s/it, train_loss=0.0144] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.18s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.18s/it, train_loss=0.00142]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.00142]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0153] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.13s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.00253]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.13s/it, train_loss=0.00253]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.00353]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.14s/it, train_loss=0.00353]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.14s/it, train_loss=0.0289] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.20s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.20s/it, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.20s/it, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.20s/it, train_loss=0.0082] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.10s/it, train_loss=0.0082]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.10s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0373] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.15s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.15s/it, train_loss=0.00292]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.20s/it, train_loss=0.00292]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.20s/it, train_loss=0.0367] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.18s/it, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.18s/it, train_loss=0.00222]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.11s/it, train_loss=0.00222]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.11s/it, train_loss=0.0021] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.0023]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.10s/it, train_loss=0.0023]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.10s/it, train_loss=0.00858]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=0.00858]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.04s/it, train_loss=0.0081] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.01it/s, train_loss=0.0081]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:07,  1.01it/s, train_loss=0.00514]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.01s/it, train_loss=0.00514]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.01s/it, train_loss=0.00337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.00it/s, train_loss=0.00337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.00it/s, train_loss=0.0107] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.04it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:04,  1.04it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.04it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.04it/s, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.01s/it, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.01s/it, train_loss=0.0148] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.03s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.0801] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.0801]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 average loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  99%|█████████▉| 99/100 [54:51<00:33, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 99 current AUC: 0.9952 current accuracy: 0.9379 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00614]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.29s/it, train_loss=0.00614]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.29s/it, train_loss=0.00501]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.00501]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.00198]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.14s/it, train_loss=0.00198]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.14s/it, train_loss=0.0325] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.06s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.06s/it, train_loss=0.118]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.19s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.19s/it, train_loss=0.016]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.20s/it, train_loss=0.016]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.20s/it, train_loss=0.00259]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.00259]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.00982]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.27s/it, train_loss=0.00982]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.27s/it, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:25,  1.19s/it, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:25,  1.19s/it, train_loss=0.00084]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:24,  1.20s/it, train_loss=0.00084]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:24,  1.20s/it, train_loss=0.0925] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.19s/it, train_loss=0.0925]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.19s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.14s/it, train_loss=0.0122] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.18s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.18s/it, train_loss=0.00796]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.16s/it, train_loss=0.00796]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.16s/it, train_loss=0.0146] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.19s/it, train_loss=0.0146]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.19s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.14s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.14s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:14,  1.11s/it, train_loss=0.0122] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.13s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.13s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:11,  1.07s/it, train_loss=0.00866]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.00866]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.08s/it, train_loss=0.00144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.05s/it, train_loss=0.00144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.05s/it, train_loss=0.0089] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.04s/it, train_loss=0.0089]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.04s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.03s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.03s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.02s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.02s/it, train_loss=0.143]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.00s/it, train_loss=0.143]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.00s/it, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.01s/it, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.01s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.16s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.16s/it, train_loss=0.0276] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.15s/it, train_loss=0.00772]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.00772]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.14s/it, train_loss=0.197]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.12it/s, train_loss=0.197]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 average loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 100/100 [55:25<00:00, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 100 current AUC: 0.9918 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "train completed, best_metric: 0.9984 at epoch: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=False)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            print('1')\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model_3d.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_3d.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "example = []\n",
    "example_preds = []\n",
    "example_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'].to(device),\n",
    "            test_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "        if len(example) < 10:\n",
    "            example.append(test_images)\n",
    "            example_preds.append(pred)\n",
    "            example_labels.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9857    1.0000    0.9928        69\n",
      "           1     0.6618    0.6618    0.6618        68\n",
      "           2     0.7170    0.5507    0.6230        69\n",
      "           3     0.4872    0.5846    0.5315        65\n",
      "           4     0.5738    0.5385    0.5556        65\n",
      "           5     1.0000    0.9697    0.9846        66\n",
      "           6     0.9333    1.0000    0.9655        28\n",
      "           7     1.0000    1.0000    1.0000        21\n",
      "           8     1.0000    1.0000    1.0000        21\n",
      "           9     0.9014    0.9275    0.9143        69\n",
      "          10     0.9315    0.9855    0.9577        69\n",
      "\n",
      "    accuracy                         0.8049       610\n",
      "   macro avg     0.8356    0.8380    0.8352       610\n",
      "weighted avg     0.8066    0.8049    0.8038       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=True, spatial_dims=3, n_input_channels=1, \n",
    "                 feed_forward=False, shortcut_type='A', bias_downsample=True).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.00005)\n",
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avgpool = nn.Sequential(\n",
    "    model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, n_classes)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:02<?, ?it/s, train_loss=2.58]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<01:02,  2.07s/it, train_loss=2.58]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<01:02,  2.07s/it, train_loss=2.71]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:38,  1.32s/it, train_loss=2.71]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:38,  1.32s/it, train_loss=2.75]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=2.75]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=2.63]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.03it/s, train_loss=2.63]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.03it/s, train_loss=2.84]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.08it/s, train_loss=2.84]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:24,  1.08it/s, train_loss=2.75]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:21,  1.17it/s, train_loss=2.75]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:21,  1.17it/s, train_loss=2.68]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.21it/s, train_loss=2.68]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:19,  1.21it/s, train_loss=2.73]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=2.73]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:18,  1.24it/s, train_loss=2.53]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=2.53]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:17,  1.27it/s, train_loss=2.59]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:16,  1.27it/s, train_loss=2.59]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:16,  1.27it/s, train_loss=2.37]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=2.37]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:15,  1.28it/s, train_loss=2.5] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=2.5]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:14,  1.28it/s, train_loss=2.55]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.29it/s, train_loss=2.55]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:13,  1.29it/s, train_loss=2.39]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.30it/s, train_loss=2.39]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.30it/s, train_loss=2.49]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=2.49]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:12,  1.30it/s, train_loss=2.2] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=2.2]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:14<00:11,  1.27it/s, train_loss=2.58]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=2.58]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:15<00:11,  1.24it/s, train_loss=2.13]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=2.13]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:10,  1.26it/s, train_loss=2.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=2.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=2.17]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=2.17]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.28it/s, train_loss=2.03]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=2.03]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:18<00:07,  1.27it/s, train_loss=2.41]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.30it/s, train_loss=2.41]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:19<00:06,  1.30it/s, train_loss=2.06]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.30it/s, train_loss=2.06]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:20<00:06,  1.30it/s, train_loss=2.25]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.28it/s, train_loss=2.25]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.28it/s, train_loss=2.05]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=2.05]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.30it/s, train_loss=2.06]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=2.06]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:22<00:03,  1.29it/s, train_loss=2.12]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.31it/s, train_loss=2.12]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:23<00:03,  1.31it/s, train_loss=2.13]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.30it/s, train_loss=2.13]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.30it/s, train_loss=1.94]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=1.94]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.29it/s, train_loss=1.75]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=1.75]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=1.87]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=1.87]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average loss: 2.3540\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [00:26<44:12, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.6584 current accuracy: 0.2050 best AUC: 0.6584 at epoch: 1\n",
      "----------\n",
      "epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=1.9]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=1.9]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=1.67]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=1.67]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=1.88]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=1.88]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=1.89]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=1.89]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=1.81]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=1.81]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=1.85]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=1.85]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=1.79]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=1.79]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=1.65]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=1.65]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=1.76]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=1.76]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=1.63]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=1.63]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=1.62]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=1.62]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=1.59]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=1.59]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=1.46]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=1.46]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=1.58]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=1.58]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=1.35]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=1.35]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=1.59]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.20it/s, train_loss=1.59]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.20it/s, train_loss=1.44]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=1.44]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=1.95]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=1.95]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=1.52]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=1.52]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=1.28]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=1.28]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=1.42]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=1.42]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=1.1] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=1.1]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.31it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.31it/s, train_loss=1.16]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=1.16]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=1.37]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=1.37]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=1.36]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=1.36]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=1.29]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=1.29]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=1.3] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=1.3]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.30it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=1.48]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average loss: 1.5477\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [00:52<42:53, 26.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.9533 current accuracy: 0.4907 best AUC: 0.9533 at epoch: 2\n",
      "----------\n",
      "epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=1.01]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.16it/s, train_loss=1.01]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.16it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.16it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.16it/s, train_loss=1.51]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=1.51]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.989]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.989]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=1.35] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=1.35]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.21it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=1.37]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=1.37]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=1.11]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=1.11]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=1.26]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=1.26]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=1.63]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=1.63]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=1.14]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=1.14]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.992]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.992]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=1.15] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=1.15]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.28it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=1.11]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=1.11]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=1.02]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=1.02]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.974]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.974]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=1.06] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=1.06]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.948]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.948]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=1.4]  \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=1.4]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.95]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.95]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.815]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.815]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.96] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.96]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.974]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.974]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=1.1]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=1.1]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=1.06]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=1.06]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.928]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.928]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=1.55] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=1.55]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 average loss: 1.1449\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 3/100 [01:18<42:21, 26.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 3 current AUC: 0.9756 current accuracy: 0.6894 best AUC: 0.9756 at epoch: 3\n",
      "----------\n",
      "epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.714]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.714]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.925]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.925]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=1.32] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=1.1] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=1.1]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.921]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.921]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=1.18] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=1.18]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.956]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.956]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.757]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.757]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=1.06] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=1.06]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.996]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.996]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.998]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.998]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.822]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.822]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=1]    \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=1]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.875]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.875]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.7]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.7]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.848]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.848]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.688]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.688]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.74] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.31it/s, train_loss=0.74]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.31it/s, train_loss=1.03]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.31it/s, train_loss=1.03]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.31it/s, train_loss=0.82]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.82]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.784]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.784]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.83] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.30it/s, train_loss=0.83]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.30it/s, train_loss=0.946]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.946]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.874]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.24it/s, train_loss=0.874]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.782]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.782]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.814]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.814]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.895]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.895]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=1.08] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.927]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.927]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=1.02] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=1.02]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=1.17]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.56it/s, train_loss=1.17]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 average loss: 0.9215\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 4/100 [01:44<41:38, 26.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 4 current AUC: 0.9787 current accuracy: 0.7019 best AUC: 0.9787 at epoch: 4\n",
      "----------\n",
      "epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.852]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.852]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.742]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=0.742]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=0.711]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.711]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.86] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.86]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.842]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.842]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.838]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.838]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=1.08] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.872]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.872]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.788]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.788]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=1.22] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=1.22]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.903]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.903]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.57] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.57]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.678]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.678]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.688]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.688]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.87] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.87]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=1.11]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=1.11]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.725]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.725]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=1.08] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=1.08]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.825]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.825]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.783]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.23it/s, train_loss=0.783]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.61] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.61]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.778]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.778]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.65] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.65]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.811]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.811]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.745]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.745]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.905]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.905]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.722]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.722]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.743]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.743]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.801]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.801]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.739]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.739]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=1.08] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=1.08]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 average loss: 0.8269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 5/100 [02:09<40:30, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 5 current AUC: 0.9722 current accuracy: 0.6770 best AUC: 0.9787 at epoch: 4\n",
      "----------\n",
      "epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.735]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.735]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.804]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.804]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.767]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.767]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.764]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.764]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.708]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.708]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.736]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.736]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.724]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.724]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.795]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.795]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.673]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.673]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.928]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.928]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.983]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.983]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.773]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.773]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.623]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.623]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.767]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.767]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.744]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.744]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=1.24] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=1.24]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.64]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.64]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.861]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.861]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.541]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.22it/s, train_loss=0.541]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.938]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.938]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.934]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.934]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.627]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.627]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.778]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.778]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.816]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.816]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.62] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.62it/s, train_loss=0.62]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 average loss: 0.7520\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 6/100 [02:35<40:13, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 6 current AUC: 0.9810 current accuracy: 0.7453 best AUC: 0.9810 at epoch: 6\n",
      "----------\n",
      "epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.691]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.691]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.819]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:21,  1.32it/s, train_loss=0.819]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:21,  1.32it/s, train_loss=0.901]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.901]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.539]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.539]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.926]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.926]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.658]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.658]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.553]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.553]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=1.12] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=1.12]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.72]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.72]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.694]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.694]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.627]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.627]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.797]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.797]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.855]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.855]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.596]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.596]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.668]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.668]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.78] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.78]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.877]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.877]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.853]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.853]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.495]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.495]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.846]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.846]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.759]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.31it/s, train_loss=0.759]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.31it/s, train_loss=0.681]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.681]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.30it/s, train_loss=0.712]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=0.712]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=0.682]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.682]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.632]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.632]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.917]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.58it/s, train_loss=0.917]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 average loss: 0.7203\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 7/100 [03:00<39:49, 25.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 7 current AUC: 0.9822 current accuracy: 0.7453 best AUC: 0.9822 at epoch: 7\n",
      "----------\n",
      "epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.725]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.725]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.709]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.709]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.813]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.813]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.635]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.635]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.771]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.771]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.838]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.838]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.744]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.744]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.792]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.792]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.677]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.677]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.616]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.616]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.746]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.746]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.743]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.743]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.85] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.85]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.686]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.686]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.699]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.699]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.707]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.707]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.571]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.571]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.56] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.56]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.837]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.31it/s, train_loss=0.837]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.31it/s, train_loss=0.616]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.31it/s, train_loss=0.616]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.31it/s, train_loss=0.725]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.725]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.895]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.895]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.63] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.63]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.27it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.67] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.67]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.87]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.60it/s, train_loss=0.87]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 average loss: 0.6966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 8/100 [03:25<38:47, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 8 current AUC: 0.9719 current accuracy: 0.6832 best AUC: 0.9822 at epoch: 7\n",
      "----------\n",
      "epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.664]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.664]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.567]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.567]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.784]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.784]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.79] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.79]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.665]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.665]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.766]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.766]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.678]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.678]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.754]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.754]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.61] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.61]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.835]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.835]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.562]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.562]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.657]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.657]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.645]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.645]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.653]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.653]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.502]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.502]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.492]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.492]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.825]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.825]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.613]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.613]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.579]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.579]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.427]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.427]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.736]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.736]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.76] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.76]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.61it/s, train_loss=0.496]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 average loss: 0.6183\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 9/100 [03:51<38:40, 25.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 9 current AUC: 0.9858 current accuracy: 0.8075 best AUC: 0.9858 at epoch: 9\n",
      "----------\n",
      "epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.668]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.17it/s, train_loss=0.668]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.17it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.724]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.724]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.553]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.553]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.52] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.52]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.478]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.478]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.617]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.617]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.621]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.621]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.68] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.68]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.71]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.71]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.644]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.644]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.673]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.673]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.578]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.578]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.435]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.435]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.776]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.776]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.443]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.443]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.556]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.556]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.693]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.693]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.668]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.668]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.672]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.672]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.669]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.669]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.543]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 average loss: 0.5929\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 10/100 [04:17<38:25, 25.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 10 current AUC: 0.9871 current accuracy: 0.8137 best AUC: 0.9871 at epoch: 10\n",
      "----------\n",
      "epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.558]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.558]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.497]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.497]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.864]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.864]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.575]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.575]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.487]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.487]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.555]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.555]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.603]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.603]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.665]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.23it/s, train_loss=0.665]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.642]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.642]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.531]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.531]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.486]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.486]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.479]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.479]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.443]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.443]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.53] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.53]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.768]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.768]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.45] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.45]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=1.02] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=1.02]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.823]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.823]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.538]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.538]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 average loss: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 11/100 [04:42<37:39, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 11 current AUC: 0.9764 current accuracy: 0.7019 best AUC: 0.9871 at epoch: 10\n",
      "----------\n",
      "epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.558]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.558]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.526]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=0.526]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=0.776]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:20,  1.35it/s, train_loss=0.776]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:20,  1.35it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.708]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.708]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.417]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.417]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.87] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.87]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.468]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.468]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.772]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.772]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.549]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.549]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.485]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.485]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.563]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.563]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.564]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.564]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=1]    \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=1]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.579]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.579]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.623]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.27it/s, train_loss=0.623]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.675]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.675]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.648]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.648]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.517]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.517]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.51] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.51]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.639]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.639]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.742]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.742]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.589]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.589]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.469]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.469]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=2.65] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=2.65]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 average loss: 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 12/100 [05:06<36:56, 25.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 12 current AUC: 0.9858 current accuracy: 0.8075 best AUC: 0.9871 at epoch: 10\n",
      "----------\n",
      "epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.576]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.576]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.541]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.541]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.456]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.19it/s, train_loss=0.456]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.19it/s, train_loss=0.671]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.671]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.593]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.21it/s, train_loss=0.593]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.555]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.555]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.20it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.20it/s, train_loss=0.53] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.53]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:17,  1.22it/s, train_loss=0.795]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.795]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.515]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.515]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.613]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.613]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:13,  1.22it/s, train_loss=0.544]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.544]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.553]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.553]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.22it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.685]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.685]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.28it/s, train_loss=0.48] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.48]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.644]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.644]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.412]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 average loss: 0.5262\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 13/100 [05:33<37:04, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 13 current AUC: 0.9893 current accuracy: 0.8323 best AUC: 0.9893 at epoch: 13\n",
      "----------\n",
      "epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.42] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.444]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.444]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.538]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.538]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.677]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.677]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.744]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.744]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.487]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.487]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.549]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.549]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.444]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.444]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.685]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.685]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.465]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.465]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=1.21] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=1.21]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 average loss: 0.5079\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 14/100 [05:59<36:51, 25.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 14 current AUC: 0.9904 current accuracy: 0.8571 best AUC: 0.9904 at epoch: 14\n",
      "----------\n",
      "epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.38]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.38]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.27it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.27it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.29it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.29it/s, train_loss=0.572]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.572]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.633]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.633]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.781]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.781]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.472]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.472]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.755]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.755]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.52] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.52]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.602]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.602]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.546]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.546]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.69] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.69]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.47]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.47]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.35] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.35]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.536]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.536]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.972]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.972]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.703]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.703]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.605]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.605]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.527]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.527]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.47] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.27it/s, train_loss=0.47]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.355]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.355]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.337]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 average loss: 0.5348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 15/100 [06:23<35:57, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 15 current AUC: 0.9895 current accuracy: 0.8509 best AUC: 0.9904 at epoch: 14\n",
      "----------\n",
      "epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.808]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.808]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.88] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.88]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.35] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.35]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.29it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.29it/s, train_loss=0.537]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.537]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.495]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.495]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.515]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.515]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.521]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.521]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.509]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.509]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.413]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.413]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.3]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.3]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.473]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.473]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.37] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.37]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.56]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.56]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 average loss: 0.4402\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|█▌        | 16/100 [06:50<35:51, 25.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 16 current AUC: 0.9904 current accuracy: 0.8634 best AUC: 0.9904 at epoch: 16\n",
      "----------\n",
      "epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.38] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.38]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.382]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.21it/s, train_loss=0.382]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.411]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.411]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.348]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.348]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.387]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.387]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.485]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.485]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.705]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.705]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.537]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.537]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.36] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.36]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.598]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.598]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.518]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 average loss: 0.3883\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 17/100 [07:16<35:35, 25.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 17 current AUC: 0.9945 current accuracy: 0.8447 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:21,  1.38it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:21,  1.38it/s, train_loss=0.353]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.353]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:15,  1.32it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:15,  1.32it/s, train_loss=0.557]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.31it/s, train_loss=0.557]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.31it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:13,  1.22it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.619]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.619]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.66] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.66]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.51] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.51]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.738]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.738]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.355]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.355]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.432]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.432]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.419]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 average loss: 0.4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  18%|█▊        | 18/100 [07:40<34:48, 25.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 18 current AUC: 0.9897 current accuracy: 0.8571 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.53]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.53]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.532]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.532]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.44] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:19,  1.21it/s, train_loss=0.44]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:19,  1.21it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:17,  1.21it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.20it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.20it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.21it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.21it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.37] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.37]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:13,  1.23it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.25it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.24it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.248]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 average loss: 0.3901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  19%|█▉        | 19/100 [08:06<34:13, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 19 current AUC: 0.9928 current accuracy: 0.8820 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.411]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.411]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.426]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.426]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.676]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.676]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.451]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.451]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.613]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.613]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.515]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.31it/s, train_loss=0.515]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.31it/s, train_loss=0.481]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.481]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.395]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.395]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.924]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.924]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 average loss: 0.4336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 20/100 [08:30<33:36, 25.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 20 current AUC: 0.9938 current accuracy: 0.9006 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.31it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.31it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.586]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.586]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.45] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.45]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.26it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.49] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.49]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.53]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.53]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.52] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.52]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.28it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.22it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.457]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.457]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.523]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.523]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.453]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.453]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 average loss: 0.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  21%|██        | 21/100 [08:55<32:57, 25.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 21 current AUC: 0.9928 current accuracy: 0.9006 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.435]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.30it/s, train_loss=0.435]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.30it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.478]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.478]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.387]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.387]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.6]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.6]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.469]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.469]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.21it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.56] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.56]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.4]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.52] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.52]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.28it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.512]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 average loss: 0.3886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 22/100 [09:20<32:30, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 22 current AUC: 0.9920 current accuracy: 0.8820 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.494]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.494]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.429]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.429]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.472]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.472]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.28it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.28it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.501]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.501]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.453]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.453]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.612]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.612]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.483]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.483]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.433]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.433]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.533]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.533]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.36] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.36]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.438]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.21it/s, train_loss=0.438]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.592]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.592]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 average loss: 0.3864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 23/100 [09:45<32:02, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 23 current AUC: 0.9912 current accuracy: 0.8882 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.19it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.19it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.33] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:19,  1.20it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:19,  1.20it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.20it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.20it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:17,  1.21it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.288]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.21it/s, train_loss=0.288]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.21it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.516]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.516]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:12,  1.24it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.22it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.21it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.21it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.24it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:22<00:04,  1.24it/s, train_loss=0.51] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.51]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.21it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.21it/s, train_loss=0.36] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.36]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.365]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 average loss: 0.3510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 24/100 [10:10<31:46, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 24 current AUC: 0.9889 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.509]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.509]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.675]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.675]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.34] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.324]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.324]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.66] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.66]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.547]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.514]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.514]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.258]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.258]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.823]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.823]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 average loss: 0.3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 25/100 [10:35<31:16, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 25 current AUC: 0.9921 current accuracy: 0.8509 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:22,  1.17it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:22,  1.17it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.20it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.20it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.29it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.504]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.467]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.467]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.563]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.563]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.4]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.4]\u001b[A\n",
      "                                                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 average loss: 0.3323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|██▌       | 26/100 [11:00<30:46, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 26 current AUC: 0.9911 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.22it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.22it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.527]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.527]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.47] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.47]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.529]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.529]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.39] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.427]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.427]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.35] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.35]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.3]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.3]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.34] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.452]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.452]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.422]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 average loss: 0.3282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 27/100 [11:25<30:17, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 27 current AUC: 0.9902 current accuracy: 0.8820 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.546]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.546]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.493]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.493]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.30it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:09,  1.30it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.414]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.414]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.498]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.498]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 average loss: 0.3196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|██▊       | 28/100 [11:50<29:55, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 28 current AUC: 0.9912 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 17\n",
      "----------\n",
      "epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.485]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.485]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.477]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.477]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.38] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.38]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.466]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.466]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.415]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.429]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.429]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.39] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.293]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.293]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.236]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 average loss: 0.3430\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  29%|██▉       | 29/100 [12:16<29:58, 25.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 29 current AUC: 0.9945 current accuracy: 0.8571 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.353]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.353]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.44] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.44]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.4]  \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.29it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.29it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.30it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.30it/s, train_loss=0.378]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.378]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.481]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.481]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.39] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.433]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.433]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.35] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.35]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 average loss: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 30/100 [12:41<29:21, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 30 current AUC: 0.9900 current accuracy: 0.8509 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.568]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.712]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.712]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.382]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.382]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.347]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.349]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.349]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.38] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.21it/s, train_loss=0.38]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.391]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.3] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.3]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.312]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 average loss: 0.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  31%|███       | 31/100 [13:06<28:50, 25.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 31 current AUC: 0.9918 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.21it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.22] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.22]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.368]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.368]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.30it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.30it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.29it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.256]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 average loss: 0.2470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|███▏      | 32/100 [13:30<28:20, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 32 current AUC: 0.9898 current accuracy: 0.8634 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.16it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.16it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.17it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.17it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.20it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.20it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.33] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.34] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.31] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.31]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.31] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.31]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.418]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.418]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.24]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 average loss: 0.2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 33/100 [13:55<27:53, 24.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 33 current AUC: 0.9923 current accuracy: 0.8634 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.26] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.436]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.436]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.29it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.38] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.38]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.19] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.32it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.32it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.30it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.72] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.55it/s, train_loss=0.72]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 average loss: 0.2915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|███▍      | 34/100 [14:20<27:20, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 34 current AUC: 0.9919 current accuracy: 0.8571 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.423]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.423]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.45] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.45]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.187]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 average loss: 0.2843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 35/100 [14:45<26:53, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 35 current AUC: 0.9936 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.31it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.31it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.31it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.31it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:17,  1.28it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.28it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.30it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.30it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.28it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:17<00:06,  1.27it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.25it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.25it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.334]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.334]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.517]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.56it/s, train_loss=0.517]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 average loss: 0.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 36/100 [15:09<26:21, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 36 current AUC: 0.9934 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.47] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.47]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=0.423]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.423]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.227]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 average loss: 0.2631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 37/100 [15:34<25:58, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 37 current AUC: 0.9943 current accuracy: 0.8944 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.397]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.397]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.323]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.323]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.30it/s, train_loss=0.73] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.62it/s, train_loss=0.73]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 average loss: 0.2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|███▊      | 38/100 [15:59<25:32, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 38 current AUC: 0.9939 current accuracy: 0.8696 best AUC: 0.9945 at epoch: 29\n",
      "----------\n",
      "epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.323]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.323]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.32] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.32] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.395]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.26it/s, train_loss=0.395]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.25it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.715]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.715]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 average loss: 0.2906\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  39%|███▉      | 39/100 [16:24<25:28, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 39 current AUC: 0.9963 current accuracy: 0.9006 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.25it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.25it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.288]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.288]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.588]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.588]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.657]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.657]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.727]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.727]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.528]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.389]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 average loss: 0.3255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 40/100 [16:49<25:00, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 40 current AUC: 0.9953 current accuracy: 0.8882 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.483]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.483]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.23it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.42] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.21it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.323]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.323]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.619]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.619]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 average loss: 0.2842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  41%|████      | 41/100 [17:14<24:34, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 41 current AUC: 0.9941 current accuracy: 0.8323 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.26] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.353]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.353]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.285]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 average loss: 0.2694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 42/100 [17:39<24:07, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 42 current AUC: 0.9946 current accuracy: 0.9130 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.33] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.28it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.28it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.30it/s, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.30it/s, train_loss=0.208] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.30it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.30it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.26it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.22it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.272]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 average loss: 0.2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 43/100 [18:04<23:40, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 43 current AUC: 0.9925 current accuracy: 0.8696 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:21,  1.34it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:21,  1.34it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.442]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.442]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.25]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 average loss: 0.2411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|████▍     | 44/100 [18:29<23:16, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 44 current AUC: 0.9937 current accuracy: 0.9130 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0877]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.0877]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.168] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.39] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.32] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.1] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.533]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.533]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 average loss: 0.2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 45/100 [18:54<22:52, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 45 current AUC: 0.9937 current accuracy: 0.9068 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.32it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.32it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.32it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.32it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.472]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.472]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.423]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.423]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.661]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.661]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 average loss: 0.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|████▌     | 46/100 [19:19<22:24, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 46 current AUC: 0.9936 current accuracy: 0.8696 best AUC: 0.9963 at epoch: 39\n",
      "----------\n",
      "epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.293]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:21,  1.39it/s, train_loss=0.293]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:21,  1.39it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.4]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.22] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.22]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.362]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.446]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.446]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 average loss: 0.2390\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 47/100 [19:45<22:19, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 47 current AUC: 0.9972 current accuracy: 0.9130 best AUC: 0.9972 at epoch: 47\n",
      "----------\n",
      "epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.25it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.25it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.467]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.467]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 average loss: 0.2454\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|████▊     | 48/100 [20:11<22:06, 25.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 48 current AUC: 0.9987 current accuracy: 0.9193 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.28it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.74] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.74]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 average loss: 0.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  49%|████▉     | 49/100 [20:36<21:31, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 49 current AUC: 0.9958 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.29it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0773]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0773]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 average loss: 0.2165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 50/100 [21:01<20:57, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 50 current AUC: 0.9941 current accuracy: 0.8571 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.293]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.293]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.224]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.372]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 average loss: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  51%|█████     | 51/100 [21:25<20:28, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 51 current AUC: 0.9866 current accuracy: 0.8758 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.406]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.22it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.308]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.308]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.531]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.531]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 average loss: 0.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|█████▏    | 52/100 [21:50<20:02, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 52 current AUC: 0.9880 current accuracy: 0.8696 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:21,  1.32it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:21,  1.32it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.26] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.26]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.23it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.32it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.32it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.31it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.31it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.31it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.31it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=1.04] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=1.04]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 average loss: 0.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 53/100 [22:15<19:34, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 53 current AUC: 0.9929 current accuracy: 0.8634 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0787]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.0787]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.254] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.42] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.249] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.25it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.30it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.30it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.59it/s, train_loss=0.158]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 average loss: 0.2119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|█████▍    | 54/100 [22:40<19:03, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 54 current AUC: 0.9937 current accuracy: 0.8882 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.128] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.189] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.171] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.26it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.3]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.59it/s, train_loss=0.3]\u001b[A\n",
      "                                                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 average loss: 0.1948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 55/100 [23:04<18:33, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 55 current AUC: 0.9953 current accuracy: 0.8820 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.11] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0845]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0845]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.222] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.397]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.397]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.283] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.321]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 average loss: 0.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|█████▌    | 56/100 [23:29<18:10, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 56 current AUC: 0.9956 current accuracy: 0.9193 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.27it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.27it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:18,  1.34it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:18,  1.34it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.31it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.31it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.0923]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0923]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.27it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.237] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.28it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.52it/s, train_loss=0.286]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 average loss: 0.1982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  57%|█████▋    | 57/100 [23:54<17:43, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 57 current AUC: 0.9917 current accuracy: 0.8758 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.251]\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.0946]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0946]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.256] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.0846]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0846]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.148] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.0871]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0871]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.125] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.334]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.334]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.139]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61 average loss: 0.1668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  61%|██████    | 61/100 [25:33<16:05, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 61 current AUC: 0.9953 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0833]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.0833]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.0566]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0566]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.31it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.31it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.0826]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0826]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.333] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.333]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.0811]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.0811]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.199] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.22it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.294] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0986]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62 average loss: 0.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 62/100 [25:58<15:40, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 62 current AUC: 0.9964 current accuracy: 0.9441 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.0683]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0683]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.192] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.21it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.28it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.3]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.3]\u001b[A\n",
      "                                                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63 average loss: 0.1626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  63%|██████▎   | 63/100 [26:23<15:17, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 63 current AUC: 0.9910 current accuracy: 0.8696 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.438]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.438]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.0863]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0863]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.295] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.393]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.208] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.398]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 average loss: 0.1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  64%|██████▍   | 64/100 [26:47<14:53, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 64 current AUC: 0.9952 current accuracy: 0.8820 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.237] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.21it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.21it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.324]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.324]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.502]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.502]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 average loss: 0.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 65/100 [27:12<14:30, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 65 current AUC: 0.9934 current accuracy: 0.9068 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:21,  1.35it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:21,  1.35it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.188] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.33] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.126] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.19] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0904]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.0904]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.113] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.425]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66 average loss: 0.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  66%|██████▌   | 66/100 [27:37<14:03, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 66 current AUC: 0.9954 current accuracy: 0.9006 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.20it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.20it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0847]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0847]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.275] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.32it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.32it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.31it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.31it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.19] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.0979]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0979]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.264] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.153]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 average loss: 0.1837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 67/100 [28:02<13:37, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 67 current AUC: 0.9956 current accuracy: 0.9068 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.21it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.21it/s, train_loss=0.0831]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.0831]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.219] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.19it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.19it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.085]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.085]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.29it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.29it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.193] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.0967]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0967]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.145] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.261] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.316]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0772]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0772]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0987]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0987]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.284]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 average loss: 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  68%|██████▊   | 68/100 [28:27<13:15, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 68 current AUC: 0.9932 current accuracy: 0.8820 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.136] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.29it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.278] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.278]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 average loss: 0.1625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  69%|██████▉   | 69/100 [28:52<12:49, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 69 current AUC: 0.9922 current accuracy: 0.8758 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.13it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.13it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.269]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.269]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.41]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.1] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.265] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.22] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.22]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.30it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.30it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.24]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.24]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70 average loss: 0.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 70/100 [29:16<12:25, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 70 current AUC: 0.9967 current accuracy: 0.9379 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.0846]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0846]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.0993]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0993]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0974]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.0974]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.206] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0953]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0953]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0646]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.0646]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.228] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.297] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.20it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.20it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.146] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0894]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.0894]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71 average loss: 0.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  71%|███████   | 71/100 [29:42<12:02, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 71 current AUC: 0.9928 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.21it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.21it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.20it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.20it/s, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.137] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.0851]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0851]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.29it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.29it/s, train_loss=0.0576]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0576]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.138] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.129] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.148] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.436]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.436]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.116]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72 average loss: 0.1425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  72%|███████▏  | 72/100 [30:06<11:35, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 72 current AUC: 0.9958 current accuracy: 0.9441 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:21,  1.35it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:21,  1.35it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.0994]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.0994]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:17,  1.28it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0874]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.27it/s, train_loss=0.0874]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.212] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.32] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.159] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.21it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.124]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 average loss: 0.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  73%|███████▎  | 73/100 [30:31<11:10, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 73 current AUC: 0.9956 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.22it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.22it/s, train_loss=0.064]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.064]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.20it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.20it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.201] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0982]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0982]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74 average loss: 0.1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  74%|███████▍  | 74/100 [30:56<10:46, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 74 current AUC: 0.9947 current accuracy: 0.9130 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.0833]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.0833]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.113] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.152] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.269]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.269]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.095]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.095]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.079]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.079]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.067]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.067]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.228]\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.371] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.30it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.30it/s, train_loss=0.11] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.31] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.31]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.22it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0874]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0874]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.183] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.805]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.805]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79 average loss: 0.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  79%|███████▉  | 79/100 [33:01<08:42, 24.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 79 current AUC: 0.9947 current accuracy: 0.9006 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.19it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.19it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.20it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.20it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.33] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0859]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0859]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.207] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.261] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.145] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.62it/s, train_loss=0.139]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 average loss: 0.1609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 80/100 [33:25<08:17, 24.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 80 current AUC: 0.9943 current accuracy: 0.9068 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.22it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.22it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.186] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.191] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.305] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0984]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.0984]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0785]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.0785]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.0924]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0924]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.154] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:05,  1.20it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:05,  1.20it/s, train_loss=0.0789]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.0789]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.271] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.167] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.118]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81 average loss: 0.1347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  81%|████████  | 81/100 [33:51<07:54, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 81 current AUC: 0.9981 current accuracy: 0.9441 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.179] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.179] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.22it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.155] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.21it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.0769]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0769]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.14]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.618]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.618]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 average loss: 0.1508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 82/100 [34:16<07:30, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 82 current AUC: 0.9946 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.062]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.062]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.254] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0799]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0799]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0882]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0882]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.107] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.133] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.0569]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.0569]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83 average loss: 0.1402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  83%|████████▎ | 83/100 [34:41<07:04, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 83 current AUC: 0.9950 current accuracy: 0.9379 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0683]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.0683]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.152] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.19it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.19it/s, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.22it/s, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.22it/s, train_loss=0.132] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.232] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0701]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.0701]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.0761]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0761]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.126] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.043] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.043]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0923]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0923]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.199] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0631]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.0631]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84 average loss: 0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  84%|████████▍ | 84/100 [35:06<06:39, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 84 current AUC: 0.9973 current accuracy: 0.9193 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.052]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.052]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.056]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.056]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.338] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0591]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0591]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.184] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.201] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.0969]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0969]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.299] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.21it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.21it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0896]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0896]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.065] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.065]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.282]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85 average loss: 0.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  85%|████████▌ | 85/100 [35:30<06:14, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 85 current AUC: 0.9920 current accuracy: 0.9068 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0767]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.0767]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.0504]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.0504]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.172] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.137] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0591]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0591]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0925]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0925]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.209] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.171] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0462]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0462]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0984]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0984]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0912]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0912]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0188]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0188]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86 average loss: 0.1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|████████▌ | 86/100 [35:55<05:49, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 86 current AUC: 0.9914 current accuracy: 0.8758 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.15it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.15it/s, train_loss=0.0738]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.0738]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.269] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.22it/s, train_loss=0.269]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.22it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.18it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.18it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.429] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.429]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.432] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.432]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.235] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0756]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.21it/s, train_loss=0.0756]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s, train_loss=0.132] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0581]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0581]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.211] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.22it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.21it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.25it/s, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.0359]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87 average loss: 0.1407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  87%|████████▋ | 87/100 [36:20<05:24, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 87 current AUC: 0.9956 current accuracy: 0.8758 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.0733]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0733]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0742]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.0742]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.138] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.21it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.21it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.0963]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0963]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0718]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.0718]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.0446]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0446]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.139] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0724]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0724]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0626]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0626]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.083] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.083]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.142] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.209] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.209]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88 average loss: 0.1017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|████████▊ | 88/100 [36:46<05:00, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 88 current AUC: 0.9962 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.265]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.0795]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0795]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.138] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0612]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0612]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.21it/s, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.21it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.29it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0932]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0932]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.169] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.232] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.237]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89 average loss: 0.1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  89%|████████▉ | 89/100 [37:11<04:35, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 89 current AUC: 0.9929 current accuracy: 0.8820 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.228] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.23it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.23it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0825]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.0825]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.223] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.153] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.0839]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0839]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.018] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0663]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0663]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0961]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0961]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0453]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0453]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.766] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.766]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90 average loss: 0.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 90/100 [37:36<04:10, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 90 current AUC: 0.9899 current accuracy: 0.8820 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.31it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.31it/s, train_loss=0.148] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.31it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.31it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.34it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.34it/s, train_loss=0.0841]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:18,  1.36it/s, train_loss=0.0841]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:18,  1.36it/s, train_loss=0.0602]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.32it/s, train_loss=0.0602]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.32it/s, train_loss=0.434] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.30it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.30it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:17,  1.29it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.06] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.26it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0591]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0591]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.123] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.24it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0371]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.28it/s, train_loss=0.0371]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.132] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.30it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.30it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.62it/s, train_loss=0.246]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91 average loss: 0.1366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  91%|█████████ | 91/100 [38:00<03:43, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 91 current AUC: 0.9958 current accuracy: 0.9255 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.087] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.087]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.285] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0815]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.0815]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.335] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.0417]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.0417]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.371] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.137] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.24it/s, train_loss=0.0999]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.0999]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.179] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.0927]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0927]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.262] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.496]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 average loss: 0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  92%|█████████▏| 92/100 [38:25<03:19, 24.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 92 current AUC: 0.9949 current accuracy: 0.8944 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.17it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.17it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0579]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.0579]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.227] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.057]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.057]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.187] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.0411]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.0411]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.4]   \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.0934]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0934]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.092]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.092]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0907]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0907]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.0999]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0999]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.20it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.20it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.50it/s, train_loss=0.0123]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93 average loss: 0.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  93%|█████████▎| 93/100 [38:50<02:54, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 93 current AUC: 0.9942 current accuracy: 0.8696 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.20it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.20it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0792]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.0792]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.187] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.0799]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0799]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.264] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.195] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.0638]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.0638]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.29it/s, train_loss=0.0348]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.0348]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0691]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 average loss: 0.1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  94%|█████████▍| 94/100 [39:15<02:29, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 94 current AUC: 0.9967 current accuracy: 0.9441 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0863]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.0863]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0708]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0708]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0884]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0884]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.187] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.0865]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0865]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.155] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0809]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0809]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0726]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0726]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.428] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.428]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.104]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95 average loss: 0.1039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  95%|█████████▌| 95/100 [39:40<02:04, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 95 current AUC: 0.9920 current accuracy: 0.8758 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0697]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.0697]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.0885]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.31it/s, train_loss=0.0885]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.31it/s, train_loss=0.147] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.0547]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.0547]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.136] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.201] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.152] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.30it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.30it/s, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.20it/s, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.20it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.21it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.21it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.21it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.21it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.21it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.136] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.119] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0193]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 average loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  96%|█████████▌| 96/100 [40:05<01:39, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 96 current AUC: 0.9967 current accuracy: 0.9379 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.034]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.034]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.166] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.0598]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0598]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.221] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.11] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0956]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0956]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.146] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.0619]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0619]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.077]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.077]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.0457]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0457]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.0847]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0847]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0862]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.0862]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.164] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.0985]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97 average loss: 0.0984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  97%|█████████▋| 97/100 [40:30<01:14, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 97 current AUC: 0.9939 current accuracy: 0.9006 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0943]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.35it/s, train_loss=0.0943]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.35it/s, train_loss=0.0841]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.0841]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.0327]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0327]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.091] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.091]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.0758]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0758]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.197] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.0952]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0952]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.0811]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0811]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0776]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0776]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.144] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0726]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0726]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0636]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0636]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.136] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.182] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.0744]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98 average loss: 0.0984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  98%|█████████▊| 98/100 [40:55<00:49, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 98 current AUC: 0.9872 current accuracy: 0.8385 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.0617]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.0617]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.254] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.149] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0422]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:16,  1.31it/s, train_loss=0.0422]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.31it/s, train_loss=0.0995]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0995]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.0537]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.0537]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.244] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.28it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.148] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.137] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0214]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.0214]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.226] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.036]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.036]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.143] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.266]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 average loss: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  99%|█████████▉| 99/100 [41:19<00:24, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 99 current AUC: 0.9937 current accuracy: 0.9193 best AUC: 0.9987 at epoch: 48\n",
      "----------\n",
      "epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0963]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.0963]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0605]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.0605]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.15]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.136] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.29it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.096] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.27it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.27it/s, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0771]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0771]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.167] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.04] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.04]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0913]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.0913]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.166] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.20it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.20it/s, train_loss=0.0305]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.50it/s, train_loss=0.0305]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 average loss: 0.0928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 100/100 [41:44<00:00, 25.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 100 current AUC: 0.9949 current accuracy: 0.9006 best AUC: 0.9987 at epoch: 48\n",
      "train completed, best_metric: 0.9987 at epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=False)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            print('1')\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model_3d_pretrained.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_3d_pretrained.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "example = []\n",
    "example_preds = []\n",
    "example_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'].to(device),\n",
    "            test_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "        if len(example) < 10:\n",
    "            example.append(test_images)\n",
    "            example_preds.append(pred)\n",
    "            example_labels.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9718    1.0000    0.9857        69\n",
      "           1     0.6719    0.6324    0.6515        68\n",
      "           2     0.6301    0.6667    0.6479        69\n",
      "           3     0.5357    0.4615    0.4959        65\n",
      "           4     0.5571    0.6000    0.5778        65\n",
      "           5     0.8841    0.9242    0.9037        66\n",
      "           6     1.0000    0.8214    0.9020        28\n",
      "           7     0.9130    1.0000    0.9545        21\n",
      "           8     1.0000    1.0000    1.0000        21\n",
      "           9     0.9254    0.8986    0.9118        69\n",
      "          10     0.8904    0.9420    0.9155        69\n",
      "\n",
      "    accuracy                         0.7869       610\n",
      "   macro avg     0.8163    0.8133    0.8133       610\n",
      "weighted avg     0.7854    0.7869    0.7850       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
