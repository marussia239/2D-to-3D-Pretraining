{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Classification with MONAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2441\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: cf815ed4e44a5b8ce67e894ab0bc2765279a1a59\n",
      "MONAI __file__: /mnt/hdd/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: 0.25.1\n",
      "scipy version: 1.15.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.17.1\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.48.2\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "import psutil\n",
    "import shutil\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import decollate_batch, DataLoader, Dataset\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121, resnet, resnet18\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    RandGaussianNoise,\n",
    "    RandAdjustContrast,\n",
    "    ScaleIntensity, \n",
    "    Transform,\n",
    "    ToTensor,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue working with OrganMNIST3d 64x64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'organmnist3d'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 70\n",
    "BATCH_SIZE = 8\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', download=download, size=64)\n",
    "val_dataset = DataClass(split='val', download=download, size=64)\n",
    "test_dataset = DataClass(split='test', download=download, size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/hdd/marina/.medmnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset OrganMNIST3D of size 64 (organmnist3d_64)\n",
       "    Number of datapoints: 971\n",
       "    Root location: /mnt/hdd/marina/.medmnist\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 1\n",
       "    Meaning of labels: {'0': 'liver', '1': 'kidney-right', '2': 'kidney-left', '3': 'femur-right', '4': 'femur-left', '5': 'bladder', '6': 'heart', '7': 'lung-right', '8': 'lung-left', '9': 'spleen', '10': 'pancreas'}\n",
       "    Number of samples: {'train': 971, 'val': 161, 'test': 610}\n",
       "    Description: The source of the OrganMNIST3D is the same as that of the Organ{A,C,S}MNIST. Instead of 2D images, we directly use the 3D bounding boxes and process the images into 28×28×28 to perform multi-class classification of 11 body organs. The same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the source training set are used as training and validation set, respectively, and the same 70 CT scans as the Organ{A,C,S}MNIST from the source test set are treated as the test set.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),\n",
    "        RandGaussianNoise(prob=0.5, mean=0.0, std=0.05),\n",
    "        RandAdjustContrast(gamma=(0.7, 1.3), prob=0.5),\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        ToTensor(),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "y_pred_trans = Compose([Activations(softmax=True)])\n",
    "y_trans = Compose([AsDiscrete(to_onehot=n_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _3D_Dataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return {'images': data, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ = _3D_Dataset(train_dataset, transform=train_transforms)\n",
    "val_dataset_ = _3D_Dataset(val_dataset, transform=val_transforms)\n",
    "test_dataset_ = _3D_Dataset(test_dataset, transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False, spatial_dims=3, n_input_channels=1, num_classes=n_classes).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.00005)\n",
    "max_epochs = 110\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.11it/s, train_loss=2.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 1.5370\n",
      "Saved new best metric model\n",
      "Current epoch: 1 | AUC: 0.9594 | Accuracy: 0.4658 | Best AUC: 0.9594 at epoch: 1\n",
      "----------\n",
      "epoch 2/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.09it/s, train_loss=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 1.0144\n",
      "Saved new best metric model\n",
      "Current epoch: 2 | AUC: 0.9766 | Accuracy: 0.6087 | Best AUC: 0.9766 at epoch: 2\n",
      "----------\n",
      "epoch 3/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.16it/s, train_loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.8279\n",
      "Saved new best metric model\n",
      "Current epoch: 3 | AUC: 0.9906 | Accuracy: 0.8261 | Best AUC: 0.9906 at epoch: 3\n",
      "----------\n",
      "epoch 4/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.15it/s, train_loss=1.63] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average loss: 0.6839\n",
      "Current epoch: 4 | AUC: 0.9794 | Accuracy: 0.5590 | Best AUC: 0.9906 at epoch: 3\n",
      "----------\n",
      "epoch 5/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.09it/s, train_loss=1.39]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average loss: 0.5928\n",
      "Current epoch: 5 | AUC: 0.9842 | Accuracy: 0.7143 | Best AUC: 0.9906 at epoch: 3\n",
      "----------\n",
      "epoch 6/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.15it/s, train_loss=0.852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 average loss: 0.5463\n",
      "Saved new best metric model\n",
      "Current epoch: 6 | AUC: 0.9908 | Accuracy: 0.7267 | Best AUC: 0.9908 at epoch: 6\n",
      "----------\n",
      "epoch 7/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.20it/s, train_loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 average loss: 0.5179\n",
      "Saved new best metric model\n",
      "Current epoch: 7 | AUC: 0.9954 | Accuracy: 0.8944 | Best AUC: 0.9954 at epoch: 7\n",
      "----------\n",
      "epoch 8/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.10it/s, train_loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 average loss: 0.5101\n",
      "Current epoch: 8 | AUC: 0.9911 | Accuracy: 0.7640 | Best AUC: 0.9954 at epoch: 7\n",
      "----------\n",
      "epoch 9/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.09it/s, train_loss=0.313] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 average loss: 0.4512\n",
      "Saved new best metric model\n",
      "Current epoch: 9 | AUC: 0.9991 | Accuracy: 0.9317 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 10/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.10it/s, train_loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 average loss: 0.4090\n",
      "Current epoch: 10 | AUC: 0.9987 | Accuracy: 0.9317 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 11/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.11it/s, train_loss=3.91]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 average loss: 0.4319\n",
      "Current epoch: 11 | AUC: 0.9974 | Accuracy: 0.8758 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 12/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.10it/s, train_loss=0.475] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 average loss: 0.4094\n",
      "Current epoch: 12 | AUC: 0.9958 | Accuracy: 0.8447 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 13/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.892] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 average loss: 0.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fc493b09630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1277, in close\n",
      "    if self.last_print_t < self.start_t + self.delay:\n",
      "AttributeError: 'tqdm' object has no attribute 'last_print_t'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fc493b09630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1277, in close\n",
      "    if self.last_print_t < self.start_t + self.delay:\n",
      "AttributeError: 'tqdm' object has no attribute 'last_print_t'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fc493b09630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1277, in close\n",
      "    if self.last_print_t < self.start_t + self.delay:\n",
      "AttributeError: 'tqdm' object has no attribute 'last_print_t'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fc493b09630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1277, in close\n",
      "    if self.last_print_t < self.start_t + self.delay:\n",
      "AttributeError: 'tqdm' object has no attribute 'last_print_t'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fc493b09630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1277, in close\n",
      "    if self.last_print_t < self.start_t + self.delay:\n",
      "AttributeError: 'tqdm' object has no attribute 'last_print_t'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 13 | AUC: 0.9979 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 14/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.11it/s, train_loss=2.55]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 average loss: 0.3550\n",
      "Current epoch: 14 | AUC: 0.9969 | Accuracy: 0.8199 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 15/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.379] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 average loss: 0.3306\n",
      "Current epoch: 15 | AUC: 0.9973 | Accuracy: 0.8944 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 16/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.0307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 average loss: 0.2969\n",
      "Current epoch: 16 | AUC: 0.9957 | Accuracy: 0.9193 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 17/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.99it/s, train_loss=0.0548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 average loss: 0.3214\n",
      "Current epoch: 17 | AUC: 0.9881 | Accuracy: 0.8012 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 18/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.697] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 average loss: 0.2893\n",
      "Current epoch: 18 | AUC: 0.9925 | Accuracy: 0.8509 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 19/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=1.62]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 average loss: 0.2544\n",
      "Current epoch: 19 | AUC: 0.9919 | Accuracy: 0.8385 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 20/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.501] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 average loss: 0.2588\n",
      "Current epoch: 20 | AUC: 0.9971 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 21/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.596] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 average loss: 0.2512\n",
      "Current epoch: 21 | AUC: 0.9924 | Accuracy: 0.9068 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 22/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.0174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 average loss: 0.2408\n",
      "Current epoch: 22 | AUC: 0.9950 | Accuracy: 0.9441 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 23/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 average loss: 0.2113\n",
      "Current epoch: 23 | AUC: 0.9931 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 24/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.489] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 average loss: 0.2084\n",
      "Current epoch: 24 | AUC: 0.9912 | Accuracy: 0.8696 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 25/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 average loss: 0.2271\n",
      "Current epoch: 25 | AUC: 0.9967 | Accuracy: 0.8944 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 26/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.96it/s, train_loss=1.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 average loss: 0.2030\n",
      "Current epoch: 26 | AUC: 0.9987 | Accuracy: 0.8758 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 27/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.0181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 average loss: 0.2310\n",
      "Current epoch: 27 | AUC: 0.9865 | Accuracy: 0.8571 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 28/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.59]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 average loss: 0.1628\n",
      "Current epoch: 28 | AUC: 0.9850 | Accuracy: 0.8571 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 29/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.07it/s, train_loss=0.195]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 average loss: 0.1635\n",
      "Current epoch: 29 | AUC: 0.9964 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 30/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.00it/s, train_loss=0.501]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 average loss: 0.1449\n",
      "Current epoch: 30 | AUC: 0.9925 | Accuracy: 0.8509 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 31/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.968]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 average loss: 0.1660\n",
      "Current epoch: 31 | AUC: 0.9905 | Accuracy: 0.9068 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 32/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.0683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 average loss: 0.1575\n",
      "Current epoch: 32 | AUC: 0.9915 | Accuracy: 0.9255 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 33/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.00it/s, train_loss=0.0239] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 average loss: 0.1319\n",
      "Current epoch: 33 | AUC: 0.9966 | Accuracy: 0.9006 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 34/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.29]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 average loss: 0.1242\n",
      "Current epoch: 34 | AUC: 0.9959 | Accuracy: 0.9068 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 35/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 average loss: 0.1244\n",
      "Current epoch: 35 | AUC: 0.9854 | Accuracy: 0.8696 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 36/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.08it/s, train_loss=0.0832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 average loss: 0.1190\n",
      "Current epoch: 36 | AUC: 0.9822 | Accuracy: 0.8447 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 37/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.00it/s, train_loss=1.34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 average loss: 0.1537\n",
      "Current epoch: 37 | AUC: 0.9930 | Accuracy: 0.9006 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 38/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.0861] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 average loss: 0.1109\n",
      "Current epoch: 38 | AUC: 0.9902 | Accuracy: 0.8758 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 39/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.00it/s, train_loss=0.00485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 average loss: 0.1045\n",
      "Current epoch: 39 | AUC: 0.9852 | Accuracy: 0.8261 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 40/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.0884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 average loss: 0.1120\n",
      "Current epoch: 40 | AUC: 0.9809 | Accuracy: 0.8571 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 41/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.07it/s, train_loss=0.0845] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 average loss: 0.1100\n",
      "Current epoch: 41 | AUC: 0.9951 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 42/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.029]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 average loss: 0.0981\n",
      "Current epoch: 42 | AUC: 0.9970 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 43/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 average loss: 0.0680\n",
      "Current epoch: 43 | AUC: 0.9900 | Accuracy: 0.8696 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 44/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=1.2]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 average loss: 0.0977\n",
      "Current epoch: 44 | AUC: 0.9927 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 45/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=1.62]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 average loss: 0.1480\n",
      "Current epoch: 45 | AUC: 0.9896 | Accuracy: 0.8758 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 46/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.99it/s, train_loss=0.176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 average loss: 0.1215\n",
      "Current epoch: 46 | AUC: 0.9932 | Accuracy: 0.8447 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 47/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.029]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 average loss: 0.0994\n",
      "Current epoch: 47 | AUC: 0.9912 | Accuracy: 0.8758 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 48/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=2.69]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 average loss: 0.1149\n",
      "Current epoch: 48 | AUC: 0.9937 | Accuracy: 0.9006 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 49/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.0291] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 average loss: 0.0887\n",
      "Current epoch: 49 | AUC: 0.9894 | Accuracy: 0.8882 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 50/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=1.24]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 average loss: 0.0986\n",
      "Current epoch: 50 | AUC: 0.9919 | Accuracy: 0.9193 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 51/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.97it/s, train_loss=0.0142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 average loss: 0.0505\n",
      "Current epoch: 51 | AUC: 0.9972 | Accuracy: 0.9565 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 52/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.00494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 average loss: 0.0494\n",
      "Current epoch: 52 | AUC: 0.9942 | Accuracy: 0.9193 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 53/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.315]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 average loss: 0.0725\n",
      "Current epoch: 53 | AUC: 0.9937 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 54/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.0289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 average loss: 0.0526\n",
      "Current epoch: 54 | AUC: 0.9938 | Accuracy: 0.9006 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 55/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.0531] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 average loss: 0.0429\n",
      "Current epoch: 55 | AUC: 0.9985 | Accuracy: 0.9627 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 56/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.00391] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 average loss: 0.0407\n",
      "Current epoch: 56 | AUC: 0.9976 | Accuracy: 0.9255 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 57/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.96it/s, train_loss=0.0292] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 average loss: 0.0482\n",
      "Current epoch: 57 | AUC: 0.9990 | Accuracy: 0.9565 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 58/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.09it/s, train_loss=0.263]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 average loss: 0.0522\n",
      "Current epoch: 58 | AUC: 0.9783 | Accuracy: 0.8634 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 59/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.019]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 average loss: 0.0777\n",
      "Current epoch: 59 | AUC: 0.9918 | Accuracy: 0.8944 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 60/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.00it/s, train_loss=0.191]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 average loss: 0.0625\n",
      "Current epoch: 60 | AUC: 0.9986 | Accuracy: 0.9627 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 61/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.0671] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 average loss: 0.0721\n",
      "Current epoch: 61 | AUC: 0.9989 | Accuracy: 0.9565 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 62/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.00425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 average loss: 0.0563\n",
      "Current epoch: 62 | AUC: 0.9934 | Accuracy: 0.8944 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 63/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.99it/s, train_loss=0.0679] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 average loss: 0.0810\n",
      "Current epoch: 63 | AUC: 0.9983 | Accuracy: 0.9317 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 64/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.0391] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 average loss: 0.0728\n",
      "Current epoch: 64 | AUC: 0.9939 | Accuracy: 0.9006 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 65/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.168]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 average loss: 0.0548\n",
      "Current epoch: 65 | AUC: 0.9942 | Accuracy: 0.9130 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 66/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.96it/s, train_loss=0.00784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 average loss: 0.0397\n",
      "Current epoch: 66 | AUC: 0.9966 | Accuracy: 0.9317 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 67/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.688]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 average loss: 0.0385\n",
      "Current epoch: 67 | AUC: 0.9945 | Accuracy: 0.9193 | Best AUC: 0.9991 at epoch: 9\n",
      "----------\n",
      "epoch 68/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.00338] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 average loss: 0.0803\n",
      "Saved new best metric model\n",
      "Current epoch: 68 | AUC: 0.9994 | Accuracy: 0.9379 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 69/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.244]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 average loss: 0.0443\n",
      "Current epoch: 69 | AUC: 0.9978 | Accuracy: 0.9441 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 70/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.0435]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 average loss: 0.0464\n",
      "Current epoch: 70 | AUC: 0.9988 | Accuracy: 0.9689 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 71/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.00217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 average loss: 0.0526\n",
      "Current epoch: 71 | AUC: 0.9914 | Accuracy: 0.9068 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 72/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.055]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 average loss: 0.0688\n",
      "Current epoch: 72 | AUC: 0.9966 | Accuracy: 0.9379 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 73/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.0173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 average loss: 0.0341\n",
      "Current epoch: 73 | AUC: 0.9913 | Accuracy: 0.8944 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 74/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.364]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 average loss: 0.0529\n",
      "Current epoch: 74 | AUC: 0.9935 | Accuracy: 0.8820 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 75/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.0104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 average loss: 0.0752\n",
      "Current epoch: 75 | AUC: 0.9939 | Accuracy: 0.9068 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 76/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.94it/s, train_loss=0.012]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 average loss: 0.0579\n",
      "Current epoch: 76 | AUC: 0.9949 | Accuracy: 0.9255 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 77/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=1.19]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 average loss: 0.0619\n",
      "Current epoch: 77 | AUC: 0.9913 | Accuracy: 0.8944 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 78/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.00288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 average loss: 0.0503\n",
      "Current epoch: 78 | AUC: 0.9965 | Accuracy: 0.9565 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 79/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.00469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 average loss: 0.0525\n",
      "Current epoch: 79 | AUC: 0.9965 | Accuracy: 0.9130 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 80/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:23<00:00,  5.12it/s, train_loss=0.0297]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 average loss: 0.0248\n",
      "Current epoch: 80 | AUC: 0.9954 | Accuracy: 0.9317 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 81/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.00273] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 average loss: 0.0457\n",
      "Current epoch: 81 | AUC: 0.9943 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 82/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.93it/s, train_loss=0.065]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 average loss: 0.0258\n",
      "Current epoch: 82 | AUC: 0.9989 | Accuracy: 0.9627 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 83/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.00317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 average loss: 0.0342\n",
      "Current epoch: 83 | AUC: 0.9944 | Accuracy: 0.9130 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 84/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.95it/s, train_loss=0.0211] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 average loss: 0.0415\n",
      "Current epoch: 84 | AUC: 0.9937 | Accuracy: 0.9255 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 85/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.993]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 average loss: 0.0569\n",
      "Current epoch: 85 | AUC: 0.9974 | Accuracy: 0.9441 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 86/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.06it/s, train_loss=0.0104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 average loss: 0.0434\n",
      "Current epoch: 86 | AUC: 0.9969 | Accuracy: 0.9565 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 87/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.0132]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 average loss: 0.0301\n",
      "Current epoch: 87 | AUC: 0.9947 | Accuracy: 0.8944 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 88/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.0172]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 average loss: 0.0149\n",
      "Current epoch: 88 | AUC: 0.9945 | Accuracy: 0.9317 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 89/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.07it/s, train_loss=0.0758] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 average loss: 0.0339\n",
      "Current epoch: 89 | AUC: 0.9946 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 90/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.07it/s, train_loss=0.00354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 average loss: 0.0522\n",
      "Current epoch: 90 | AUC: 0.9976 | Accuracy: 0.9379 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 91/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.00403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 average loss: 0.0396\n",
      "Current epoch: 91 | AUC: 0.9958 | Accuracy: 0.9379 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 92/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.07it/s, train_loss=0.0163]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 average loss: 0.0139\n",
      "Current epoch: 92 | AUC: 0.9952 | Accuracy: 0.9441 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 93/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.174]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 average loss: 0.0243\n",
      "Current epoch: 93 | AUC: 0.9969 | Accuracy: 0.9565 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 94/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.99it/s, train_loss=0.0146]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 average loss: 0.0500\n",
      "Current epoch: 94 | AUC: 0.9973 | Accuracy: 0.9317 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 95/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.03it/s, train_loss=0.515]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 average loss: 0.0546\n",
      "Current epoch: 95 | AUC: 0.9976 | Accuracy: 0.9503 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 96/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.08it/s, train_loss=0.0036] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 average loss: 0.0902\n",
      "Current epoch: 96 | AUC: 0.9949 | Accuracy: 0.9130 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 97/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.0052]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 average loss: 0.0319\n",
      "Current epoch: 97 | AUC: 0.9951 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 98/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.00it/s, train_loss=0.495]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 average loss: 0.0378\n",
      "Current epoch: 98 | AUC: 0.9943 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 99/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.01it/s, train_loss=0.254]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 average loss: 0.0428\n",
      "Current epoch: 99 | AUC: 0.9973 | Accuracy: 0.9379 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 100/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.98it/s, train_loss=0.258]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 average loss: 0.0269\n",
      "Current epoch: 100 | AUC: 0.9970 | Accuracy: 0.9503 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 101/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.0301] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101 average loss: 0.0276\n",
      "Current epoch: 101 | AUC: 0.9924 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 102/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.96it/s, train_loss=0.00413] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102 average loss: 0.0278\n",
      "Current epoch: 102 | AUC: 0.9979 | Accuracy: 0.9689 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 103/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.93it/s, train_loss=0.000816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103 average loss: 0.0297\n",
      "Current epoch: 103 | AUC: 0.9973 | Accuracy: 0.9503 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 104/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.00238] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104 average loss: 0.0340\n",
      "Current epoch: 104 | AUC: 0.9944 | Accuracy: 0.9317 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 105/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  4.96it/s, train_loss=0.0352] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105 average loss: 0.0453\n",
      "Current epoch: 105 | AUC: 0.9930 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 106/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.02it/s, train_loss=0.0862]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106 average loss: 0.0200\n",
      "Current epoch: 106 | AUC: 0.9969 | Accuracy: 0.9565 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 107/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=0.000891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 average loss: 0.0303\n",
      "Current epoch: 107 | AUC: 0.9954 | Accuracy: 0.9255 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 108/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.04it/s, train_loss=1.22]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 average loss: 0.0449\n",
      "Current epoch: 108 | AUC: 0.9920 | Accuracy: 0.9193 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 109/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.08it/s, train_loss=0.179]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109 average loss: 0.0665\n",
      "Current epoch: 109 | AUC: 0.9956 | Accuracy: 0.9317 | Best AUC: 0.9994 at epoch: 68\n",
      "----------\n",
      "epoch 110/110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|##########| 122/122 [00:24<00:00,  5.05it/s, train_loss=0.00302] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110 average loss: 0.0257\n",
      "Current epoch: 110 | AUC: 0.9930 | Accuracy: 0.9006 | Best AUC: 0.9994 at epoch: 68\n",
      "Train completed, best_metric: 0.9994 at epoch: 68\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process()\n",
    "start_cpu = process.cpu_times()\n",
    "start_mem = process.memory_info().rss / 1024**2  # В MB\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=True, dynamic_ncols=True, ascii=True)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    tqdm.write(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model_3d.pth\"))\n",
    "                tqdm.write(\"Saved new best metric model\")\n",
    "            tqdm.write(\n",
    "                f\"Current epoch: {epoch + 1} | AUC: {result:.4f} | \"\n",
    "                f\"Accuracy: {acc_metric:.4f} | Best AUC: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "tqdm.write(f\"Train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2742.62 seconds\n",
      "CPU time used: 32788.54 seconds\n",
      "Memory used: 543.55 MB\n",
      "GPU Memory Used: 787.14 MB\n",
      "Max GPU Memory Used: 4506.43 MB\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "end_cpu = process.cpu_times()\n",
    "end_mem = process.memory_info().rss / 1024**2\n",
    "\n",
    "cpu_time = (end_cpu.user + end_cpu.system) - (start_cpu.user + start_cpu.system)\n",
    "memory_used = end_mem - start_mem\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"CPU time used: {cpu_time:.2f} seconds\")\n",
    "print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Max GPU Memory Used: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM currently used: 787.14 MB\n",
      "Max VRAM used during training: 4506.43 MB\n"
     ]
    }
   ],
   "source": [
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    vram_used = torch.cuda.memory_allocated() / 1024**2\n",
    "    vram_max_used = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"VRAM currently used: {vram_used:.2f} MB\")\n",
    "    print(f\"Max VRAM used during training: {vram_max_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9792\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_3d.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_predicted = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    y = torch.tensor([], dtype=torch.long, device=device)\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'],\n",
    "            test_data['label'][:, 0].type(torch.LongTensor),\n",
    "        )\n",
    "\n",
    "        output = model(test_images.to(device))\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_predicted.append(pred[i].item())\n",
    "\n",
    "        y_pred = torch.cat([y_pred, output], dim=0)\n",
    "        y = torch.cat([y, test_labels.to(device)], dim=0)\n",
    "\n",
    "    # Evaluate AUC and accuracy\n",
    "    y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "    y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "    auc_metric(y_pred_act, y_onehot)\n",
    "    result = auc_metric.aggregate()\n",
    "    auc_metric.reset()\n",
    "\n",
    "    print(f\"Validation AUC: {result:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9718    1.0000    0.9857        69\n",
      "           1     0.7231    0.6912    0.7068        68\n",
      "           2     0.6341    0.7536    0.6887        69\n",
      "           3     0.5521    0.8154    0.6584        65\n",
      "           4     0.7027    0.4000    0.5098        65\n",
      "           5     0.9844    0.9545    0.9692        66\n",
      "           6     1.0000    0.8929    0.9434        28\n",
      "           7     1.0000    1.0000    1.0000        21\n",
      "           8     1.0000    1.0000    1.0000        21\n",
      "           9     1.0000    0.8841    0.9385        69\n",
      "          10     0.9851    0.9565    0.9706        69\n",
      "\n",
      "    accuracy                         0.8262       610\n",
      "   macro avg     0.8685    0.8498    0.8519       610\n",
      "weighted avg     0.8418    0.8262    0.8256       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_predicted, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM currently used: 563.02 MB\n",
      "Max VRAM used during training: 12850.32 MB\n"
     ]
    }
   ],
   "source": [
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    vram_used = torch.cuda.memory_allocated() / 1024**2\n",
    "    vram_max_used = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"VRAM currently used: {vram_used:.2f} MB\")\n",
    "    print(f\"Max VRAM used during training: {vram_max_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=True, spatial_dims=3, n_input_channels=1, \n",
    "                 feed_forward=False, shortcut_type='A', bias_downsample=True).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.00005)\n",
    "max_epochs = 70\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avgpool = nn.Sequential(\n",
    "    model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, n_classes)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=2.31]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=2.31]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=2.46]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=2.46]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=2.33]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=2.33]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=2.25]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=2.25]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=2.33]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=2.33]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=2.18]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=2.18]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=2.24]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=2.24]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=2.12]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=2.12]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=2.09]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.31it/s, train_loss=2.09]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.31it/s, train_loss=2.12]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=2.12]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=2.13]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=2.13]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=1.94]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=1.94]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=1.91]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=1.91]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=1.9] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.28it/s, train_loss=1.9]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=2.05]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=2.05]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=1.98]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=1.98]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=2.01]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=2.01]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=1.94]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=1.94]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=1.84]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.29it/s, train_loss=1.84]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=1.84]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=1.84]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=1.74]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=1.74]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=1.83]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=1.83]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=1.99]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=1.99]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=1.79]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.28it/s, train_loss=1.79]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=1.76]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=1.76]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=1.7] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=1.7]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=1.74]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=1.74]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=1.55]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.29it/s, train_loss=1.55]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=1.65]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=1.65]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=1.67]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=1.67]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=1.84]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.63it/s, train_loss=1.84]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average loss: 1.9749\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|▏         | 1/70 [00:25<29:21, 25.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.7737 current accuracy: 0.1677 best AUC: 0.7737 at epoch: 1\n",
      "----------\n",
      "epoch 2/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=1.74]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=1.74]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=1.63]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=1.63]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=1.56]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=1.56]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=1.28]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=1.28]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=1.44]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=1.44]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=1.62]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=1.62]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=1.43]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=1.43]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=1.4] \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=1.4]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=1.7]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=1.7]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=1.36]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=1.36]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=1.32]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=1.32]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=1.68]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=1.68]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=1.46]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=1.46]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=1.36]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=1.36]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=1.53]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=1.53]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=1.43]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=1.43]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=1.51]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.27it/s, train_loss=1.51]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.27it/s, train_loss=1.45]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=1.45]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=1.41]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.28it/s, train_loss=1.41]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=1.28]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=1.28]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=1.3] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=1.3]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=1.2]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=1.2]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=1.38]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=1.38]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=1.43]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.28it/s, train_loss=1.43]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=1.22]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=1.22]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=1.37]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=1.37]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=1.55]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=1.55]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=1.05]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=1.05]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=1.08]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=1.08]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=1.04]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.31it/s, train_loss=1.04]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.31it/s, train_loss=1.13]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.59it/s, train_loss=1.13]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average loss: 1.3986\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 2/70 [00:51<29:01, 25.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.9363 current accuracy: 0.4472 best AUC: 0.9363 at epoch: 2\n",
      "----------\n",
      "epoch 3/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.951]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.951]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=1.13] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=1.13]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=1.18]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=1.18]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=1.28]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.31it/s, train_loss=1.28]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.31it/s, train_loss=1.03]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=1.03]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=1.03]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=1.03]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=1.16]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=1.16]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=1.09]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=1.09]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.872]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.872]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=1.11] \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:17,  1.23it/s, train_loss=1.11]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=1.24]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=1.24]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=1.11]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=1.11]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=1.18]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=1.18]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=1.35]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=1.35]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=1.22]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=1.22]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=1.05]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=1.05]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=1.19]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=1.19]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=1.05]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=1.05]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.29it/s, train_loss=0.924]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.924]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.94] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.94]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.906]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.906]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=1.01] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=1.01]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=1.07]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=1.07]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.991]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.991]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=1.16] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=1.16]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=1.02]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=1.02]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.88]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.88]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=1.15]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=1.15]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.987]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.987]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=1.23] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=1.23]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.859]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.859]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 average loss: 1.0765\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 3/70 [01:17<28:46, 25.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 3 current AUC: 0.9697 current accuracy: 0.6025 best AUC: 0.9697 at epoch: 3\n",
      "----------\n",
      "epoch 4/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.872]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.872]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.987]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.987]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.742]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.742]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.919]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.919]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.781]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.781]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=1.02] \n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=1.02]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=1.08]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=1.08]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.957]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.957]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.89] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.89]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.801]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.801]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.877]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.877]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=1.02] \n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.30it/s, train_loss=1.02]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.30it/s, train_loss=0.789]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.30it/s, train_loss=0.789]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.30it/s, train_loss=0.899]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.899]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=1.16] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.30it/s, train_loss=1.16]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=1.5] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=1.5]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.875]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.875]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.871]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.30it/s, train_loss=0.871]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.30it/s, train_loss=0.986]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.986]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.829]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.829]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.833]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.833]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.744]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.744]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=1.08] \n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=1.08]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.91]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.91]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.704]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.704]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.663]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.663]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.893]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.893]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.949]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.949]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.951]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.27it/s, train_loss=0.951]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.739]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.739]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=1.62] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=1.62]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 average loss: 0.9334\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 4/70 [01:43<28:24, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 4 current AUC: 0.9719 current accuracy: 0.6398 best AUC: 0.9719 at epoch: 4\n",
      "----------\n",
      "epoch 5/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.821]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.821]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.854]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=0.854]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=1.02] \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.31it/s, train_loss=1.02]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.31it/s, train_loss=1.16]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=1.16]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.888]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.888]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.851]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.851]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.785]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.785]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.766]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.766]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.983]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.983]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.677]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.20it/s, train_loss=0.677]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.20it/s, train_loss=0.823]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.22it/s, train_loss=0.823]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.816]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.816]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.741]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.741]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.713]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.713]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.753]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.753]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.755]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.755]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.734]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.734]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.854]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.854]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.957]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.20it/s, train_loss=0.957]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.20it/s, train_loss=0.982]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.982]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.22it/s, train_loss=0.83] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.83]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.969]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.969]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.792]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.792]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.767]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.767]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.907]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.907]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.731]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.731]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.915]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.915]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.824]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.824]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.673]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.673]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.86] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.86]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.821]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.821]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 average loss: 0.8394\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 5/70 [02:09<28:08, 25.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 5 current AUC: 0.9818 current accuracy: 0.7143 best AUC: 0.9818 at epoch: 5\n",
      "----------\n",
      "epoch 6/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.962]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.962]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.575]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.575]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.729]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:23,  1.22it/s, train_loss=0.729]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:23,  1.22it/s, train_loss=0.889]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.889]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.693]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.693]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.795]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.795]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.667]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.667]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.82] \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.82]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.813]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.813]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.648]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.648]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.957]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.957]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.741]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.741]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.888]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.888]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.748]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.748]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.867]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.867]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.799]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.799]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.815]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.815]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.66] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.66]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.932]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.932]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.626]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.626]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.691]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.691]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.561]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.561]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.777]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.777]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.595]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.595]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.722]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.722]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.976]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.976]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.678]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.678]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.693]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.693]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.622]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.622]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.764]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.764]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 average loss: 0.7511\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▊         | 6/70 [02:35<27:45, 26.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 6 current AUC: 0.9847 current accuracy: 0.7640 best AUC: 0.9847 at epoch: 6\n",
      "----------\n",
      "epoch 7/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.445]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.445]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.494]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:21,  1.35it/s, train_loss=0.494]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:21,  1.35it/s, train_loss=0.622]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.622]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.52] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.52]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.712]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.712]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.676]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.676]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.718]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.718]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.666]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.666]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.488]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.488]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=1.08] \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=1.08]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.742]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.29it/s, train_loss=0.742]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.29it/s, train_loss=0.591]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.30it/s, train_loss=0.591]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.30it/s, train_loss=0.604]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.604]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.629]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.29it/s, train_loss=0.629]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.582]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.582]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.885]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.885]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.607]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.607]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.525]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.525]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.23it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.659]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.659]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.8]  \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.8]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=1.01]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=1.01]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.836]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.836]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.51] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.25it/s, train_loss=0.51]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.671]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.671]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.895]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.895]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.571]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.571]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.907]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.907]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.49] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.49]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.608]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.608]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.728]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.56it/s, train_loss=0.728]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 average loss: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 7/70 [02:59<26:49, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 7 current AUC: 0.9777 current accuracy: 0.7329 best AUC: 0.9847 at epoch: 6\n",
      "----------\n",
      "epoch 8/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.796]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.796]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.651]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=0.651]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=0.676]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.676]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.523]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.523]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.777]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.777]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.595]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.595]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.697]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.697]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.599]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.599]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.549]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.549]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.542]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.542]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.829]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.829]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.639]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.639]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.29it/s, train_loss=0.837]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.837]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.656]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.656]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.938]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.938]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.46] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.46]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.518]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.518]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.605]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.605]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.567]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.567]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.592]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.592]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.657]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.657]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.526]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.526]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.746]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.746]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.598]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.598]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.497]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.497]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.759]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.759]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.508]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.508]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.682]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.682]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 average loss: 0.6328\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█▏        | 8/70 [03:25<26:32, 25.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 8 current AUC: 0.9868 current accuracy: 0.8447 best AUC: 0.9868 at epoch: 8\n",
      "----------\n",
      "epoch 9/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.58]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.58]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.511]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.511]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.867]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.867]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.597]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.597]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=1.01] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=1.01]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.478]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.478]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.555]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.555]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.669]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.669]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.533]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.533]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.702]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.702]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.719]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.719]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.624]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.624]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.568]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.28it/s, train_loss=0.568]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.909]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.30it/s, train_loss=0.909]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=0.563]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.563]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.616]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.27it/s, train_loss=0.616]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.27it/s, train_loss=0.639]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.639]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.695]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.27it/s, train_loss=0.695]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.726]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.726]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.712]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.712]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.532]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.532]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.49] \n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.49]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.761]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.30it/s, train_loss=0.761]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.41] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.41]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.865]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.865]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.824]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.28it/s, train_loss=0.824]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.954]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.27it/s, train_loss=0.954]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.6]  \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.58it/s, train_loss=0.6]\n",
      "\u001b[A                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 average loss: 0.6514\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 9/70 [03:51<26:05, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 9 current AUC: 0.9906 current accuracy: 0.8447 best AUC: 0.9906 at epoch: 9\n",
      "----------\n",
      "epoch 10/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.707]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.707]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=1.04] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=1.04]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.476]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.476]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.649]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.649]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.589]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.589]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.31it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.31it/s, train_loss=0.454]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.454]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.687]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.687]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.449]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.449]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.632]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.632]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.396]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.396]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.628]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.628]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.634]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.634]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.473]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.473]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.667]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.667]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.556]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.556]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.23it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.452]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.452]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.552]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.552]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.568]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.568]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.878]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.878]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.527]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.527]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.51] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.51]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.603]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.603]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.657]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.657]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.466]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.466]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.703]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.703]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.438]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.438]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 average loss: 0.5726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 10/70 [04:16<25:22, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 10 current AUC: 0.9899 current accuracy: 0.8385 best AUC: 0.9906 at epoch: 9\n",
      "----------\n",
      "epoch 11/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.609]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.609]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.562]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:24,  1.19it/s, train_loss=0.562]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:24,  1.19it/s, train_loss=0.521]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.521]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.44] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.44]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.616]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.616]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.408]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.408]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.885]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.885]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.737]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.737]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.479]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.479]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.686]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.31it/s, train_loss=0.686]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.31it/s, train_loss=0.751]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.751]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.707]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.707]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.545]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.545]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.497]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.497]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.685]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.685]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.566]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.566]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.524]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.524]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.593]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.593]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.574]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.574]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.552]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.552]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.778]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.778]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.447]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.447]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.30it/s, train_loss=0.494]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.494]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.638]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.25it/s, train_loss=0.638]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.563]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.563]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.502]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.502]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 average loss: 0.5688\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|█▌        | 11/70 [04:42<25:07, 25.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 11 current AUC: 0.9930 current accuracy: 0.8758 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 12/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.425]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.425]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.742]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.742]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.486]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.486]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.5]  \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.5]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.534]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.534]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.448]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.448]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.482]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.482]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.622]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.622]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.956]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.956]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.583]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.416]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.416]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.416]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.416]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.743]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.743]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.706]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.706]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.596]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.596]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.34] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.34]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.762]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.762]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.56] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.56]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.574]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.574]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.625]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.625]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.537]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.537]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.505]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.505]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.618]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.618]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=1.24] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=1.24]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 average loss: 0.5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 12/70 [05:06<24:27, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 12 current AUC: 0.9910 current accuracy: 0.8385 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 13/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.533]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.533]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.456]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.456]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.55] \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.55]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.481]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.481]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.514]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.514]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.444]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.444]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.744]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.744]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.606]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.606]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.515]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.515]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.579]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.579]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.354]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.354]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.714]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.714]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.532]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.532]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.558]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.558]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.692]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.692]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.41] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.41]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.394]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.394]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.602]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.602]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.62] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.62]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.5] \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.5]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.509]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.509]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.437]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.437]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.453]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 average loss: 0.5020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  19%|█▊        | 13/70 [05:31<23:51, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 13 current AUC: 0.9905 current accuracy: 0.8571 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 14/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.46]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.46]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.705]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.705]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.444]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.444]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.481]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.481]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.536]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.536]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.535]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.535]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.525]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.525]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.353]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.353]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.446]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.446]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.341]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.341]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.456]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.456]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.464]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.464]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.39] \n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.35]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.35]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.564]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.564]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.45] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.45]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.5]  \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.5]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.416]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.416]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.354]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.354]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.726]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=0.726]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.30it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.982]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.982]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 average loss: 0.4617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 14/70 [05:56<23:19, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 14 current AUC: 0.9902 current accuracy: 0.8571 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 15/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.635]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.635]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.773]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.773]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.743]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.743]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.445]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.445]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.66] \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.28it/s, train_loss=0.66]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.28it/s, train_loss=0.674]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.674]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.391]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.391]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.458]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.458]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.559]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.559]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.676]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.676]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.577]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.577]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.587]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.29it/s, train_loss=0.587]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.29it/s, train_loss=0.657]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.31it/s, train_loss=0.657]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.31it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.30it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.30it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.26it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.325]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.325]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.398]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.398]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.578]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.578]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.347]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.347]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.26it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.612]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.25it/s, train_loss=0.612]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.633]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.55it/s, train_loss=0.633]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 average loss: 0.4909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  21%|██▏       | 15/70 [06:20<22:46, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 15 current AUC: 0.9917 current accuracy: 0.8571 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 16/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.76] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.76]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.619]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.619]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.521]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.521]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.518]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.518]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.543]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.543]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.479]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.479]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.28] \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.28]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.517]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.517]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.503]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.503]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.486]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.486]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.48] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.48]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.36]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.36]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.701]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.701]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.434]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.27it/s, train_loss=0.434]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.27it/s, train_loss=0.57] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.57]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.29it/s, train_loss=0.485]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.485]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.538]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.30it/s, train_loss=0.538]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.30it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.626]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.626]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.417]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.417]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.33] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.33]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.519]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.519]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.637]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.637]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.34] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.34]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=1.18]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.58it/s, train_loss=1.18]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 average loss: 0.4947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 16/70 [06:45<22:17, 24.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 16 current AUC: 0.9904 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 17/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.424]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.424]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.517]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.517]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.392]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.392]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.46] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.46]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.501]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.501]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.28it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.28it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:06<00:16,  1.31it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.31it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.51] \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.51]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.573]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.573]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.572]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.572]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.584]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.584]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.647]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.29it/s, train_loss=0.647]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.39] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.3] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.3]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.44]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.44]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.44]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.44]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.445]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.445]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.45] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.45]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.575]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.575]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.532]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.532]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.211]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 average loss: 0.4161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 17/70 [07:10<21:52, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 17 current AUC: 0.9921 current accuracy: 0.8571 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 18/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.551]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.551]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.38] \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.38]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.567]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.567]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.538]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.538]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.23] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.23]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.347]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.347]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.517]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.517]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.413]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.413]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.519]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.519]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.41] \n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.41]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.686]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.686]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.381]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.381]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.30it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.30it/s, train_loss=0.75] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.75]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.403]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.403]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.31it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.31it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:06,  1.30it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:06,  1.30it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.30it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.31it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.31it/s, train_loss=0.447]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.447]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.31it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.31it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.31it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.31it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.61it/s, train_loss=0.409]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 average loss: 0.4211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|██▌       | 18/70 [07:34<21:20, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 18 current AUC: 0.9896 current accuracy: 0.8634 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 19/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.247]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.247]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.309]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.31it/s, train_loss=0.309]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.31it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.36] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.36]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.376]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.376]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.509]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.509]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.482]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.482]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.28] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.28]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.405]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.405]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.475]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.475]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.547]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.25it/s, train_loss=0.547]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.463]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.463]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.621]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.621]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.468]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.468]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.239]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.26it/s, train_loss=0.239]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.67] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.67]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.415]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.415]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.424]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.424]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.463]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.463]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.56it/s, train_loss=0.336]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 average loss: 0.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 19/70 [07:59<20:55, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 19 current AUC: 0.9875 current accuracy: 0.8509 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 20/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.441]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.441]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.33] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.33]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.479]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.479]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:19,  1.21it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:19,  1.21it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.20it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.20it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.408]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.408]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.369]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.369]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.37] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.37]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.374]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.374]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.594]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.594]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.429]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.429]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.466]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.31it/s, train_loss=0.466]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:09,  1.31it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.31it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.31it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.30it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.30it/s, train_loss=0.351]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.351]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.628]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.628]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.158]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 average loss: 0.3749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  29%|██▊       | 20/70 [08:23<20:32, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 20 current AUC: 0.9897 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 21/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.51]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.51]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:21,  1.34it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:21,  1.34it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.32it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.32it/s, train_loss=0.351]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.351]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.301]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.301]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.447]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.447]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.312]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.312]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.358]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.358]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.424]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.22it/s, train_loss=0.424]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.47] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.47]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.458]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.458]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.554]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.551]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.551]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.514]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.514]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 average loss: 0.3667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 21/70 [08:48<20:09, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 21 current AUC: 0.9886 current accuracy: 0.8758 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 22/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.688]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.688]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.426]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.426]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.593]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.30it/s, train_loss=0.593]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.30it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.538]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.538]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.527]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.527]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.662]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.662]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.51] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.51]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.421]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.421]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.452]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.27it/s, train_loss=0.452]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.609]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.609]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.341]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.341]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.26it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.402]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.415]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.58it/s, train_loss=0.415]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 average loss: 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  31%|███▏      | 22/70 [09:13<19:43, 24.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 22 current AUC: 0.9865 current accuracy: 0.8323 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 23/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.417]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.417]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.704]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:24,  1.19it/s, train_loss=0.704]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:24,  1.19it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.347]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.347]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.595]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.595]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.39] \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.453]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.454]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.454]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.472]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.472]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.355]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.355]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.329]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.329]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.21] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.558]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.558]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.265]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.265]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.859]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.859]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 average loss: 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 23/70 [09:38<19:22, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 23 current AUC: 0.9892 current accuracy: 0.8696 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 24/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:21,  1.41it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:21,  1.41it/s, train_loss=0.29] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:21,  1.32it/s, train_loss=0.29]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:21,  1.32it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.3] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.3]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.544]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.544]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.515]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.515]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.36] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.36]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.566]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.566]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.43] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.43]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.506]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.528]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.528]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.31it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.31it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:06,  1.30it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:06,  1.30it/s, train_loss=0.478]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.478]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.465]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.465]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.557]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.417]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.31it/s, train_loss=0.417]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.31it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.478]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.59it/s, train_loss=0.478]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 average loss: 0.3847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|███▍      | 24/70 [10:02<18:54, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 24 current AUC: 0.9884 current accuracy: 0.8634 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 25/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.423]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.423]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.323]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.323]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.24] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.24]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.419]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.419]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.249]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.249]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.352]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.352]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.246]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 average loss: 0.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 25/70 [10:27<18:31, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 25 current AUC: 0.9875 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 26/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.422]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.422]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.67] \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.67]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.425]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.425]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.26] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.464]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.464]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.171]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.171]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.476]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.476]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.501]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.501]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.428]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.428]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.351]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.351]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 average loss: 0.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 26/70 [10:52<18:09, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 26 current AUC: 0.9901 current accuracy: 0.8758 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 27/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.455]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.455]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.528]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.528]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.30it/s, train_loss=0.539]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.30it/s, train_loss=0.28] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.28]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.305]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.305]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.36] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.36]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.28it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.392]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.31it/s, train_loss=0.392]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.31it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.32it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.32it/s, train_loss=0.608]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.33it/s, train_loss=0.608]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.33it/s, train_loss=0.415]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.28it/s, train_loss=0.415]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.23it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.345]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.345]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.651]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.54it/s, train_loss=0.651]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 average loss: 0.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  39%|███▊      | 27/70 [11:16<17:41, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 27 current AUC: 0.9908 current accuracy: 0.8571 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 28/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.35]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.35]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.754]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.754]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.555]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.555]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.634]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.634]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.301]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.301]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.326]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.326]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.6]  \n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.6]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.413]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.413]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.41] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.23it/s, train_loss=0.41]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.419]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.419]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.28] \n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.28]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.407]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.278]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.278]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.504]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.504]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 average loss: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 28/70 [11:41<17:19, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 28 current AUC: 0.9916 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 29/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.263]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.263]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.35] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.35]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:23,  1.20it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:23,  1.20it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.525]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.525]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.374]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.374]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.486]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.486]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.477]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.477]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.524]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.524]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.284]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.284]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.46] \n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.46]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.29] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.29]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.507]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 average loss: 0.3251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  41%|████▏     | 29/70 [12:06<16:56, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 29 current AUC: 0.9916 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 30/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.318]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.318]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.425]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.425]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.269]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.269]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.484]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.484]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.301]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.301]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.38] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.38]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.314]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.314]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.4]  \n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.4]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.736]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.736]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.33] \n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.33]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.308]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.308]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.166]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 average loss: 0.3422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 30/70 [12:31<16:31, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 30 current AUC: 0.9898 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 31/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.30it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.30it/s, train_loss=0.366]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:21,  1.33it/s, train_loss=0.366]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:21,  1.33it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.309]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.30it/s, train_loss=0.309]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.30it/s, train_loss=0.23] \n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.23]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.26] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.377]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.377]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.29it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.235]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.27it/s, train_loss=0.235]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.27it/s, train_loss=0.576]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.576]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.265]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.265]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.405]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.405]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.442]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.241]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.25it/s, train_loss=0.241]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.653]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.653]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.513]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.513]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 average loss: 0.3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|████▍     | 31/70 [12:55<16:04, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 31 current AUC: 0.9909 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 32/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.323]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.323]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.312]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.312]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.531]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.531]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.284]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.284]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.352]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.352]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.736]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.21it/s, train_loss=0.736]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.278]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.278]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.283]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.283]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.335]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.335]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 average loss: 0.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|████▌     | 32/70 [13:20<15:40, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 32 current AUC: 0.9890 current accuracy: 0.8385 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 33/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.42] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.42]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.531]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.531]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.21it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.21it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.49] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.49]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.369]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.369]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.433]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.433]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.318]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.318]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.41] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.41]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.385]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.385]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.421]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.421]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.713]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.713]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 average loss: 0.3065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 33/70 [13:45<15:17, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 33 current AUC: 0.9911 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 34/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.31] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.31]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.33]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.33]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.23it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.23it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.375]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.249]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.249]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.423]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.29it/s, train_loss=0.423]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.29it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.412]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.521]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.521]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 average loss: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  49%|████▊     | 34/70 [14:10<14:52, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 34 current AUC: 0.9900 current accuracy: 0.8509 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 35/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.24] \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.24]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.272]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.272]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.239]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.239]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.283]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.283]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.357]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.30it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.30it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.379]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.329]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.329]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.441]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.441]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.155]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.155]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.403]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.403]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=1.36] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=1.36]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 average loss: 0.3138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 35/70 [14:35<14:28, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 35 current AUC: 0.9914 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 36/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.32it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.32it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.508]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.508]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.309]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.309]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.488]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.488]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.354]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.354]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.352]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.352]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.659]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.28it/s, train_loss=0.659]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.38] \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.38]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.241]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.241]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.398]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.398]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.397]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 average loss: 0.3079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  51%|█████▏    | 36/70 [14:59<14:01, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 36 current AUC: 0.9874 current accuracy: 0.8509 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 37/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.31] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.31]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.397]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.4]  \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.4]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.534]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.534]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.369]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.369]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.33] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.33]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.439]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.439]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.396]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.396]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.449]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.449]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.29] \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.29]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.345]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.345]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.331]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.338]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 average loss: 0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 37/70 [15:24<13:38, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 37 current AUC: 0.9871 current accuracy: 0.8509 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 38/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.419]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.419]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.36] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.36]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.30it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.30it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.26it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.361]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 average loss: 0.2627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|█████▍    | 38/70 [15:49<13:13, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 38 current AUC: 0.9881 current accuracy: 0.8509 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 39/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.438]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.438]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.235]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.235]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.122]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.122]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.38] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.38]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.336]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.26] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.366]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.366]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.231]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.231]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.108]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.108]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.334]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.142]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 average loss: 0.2763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|█████▌    | 39/70 [16:14<12:49, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 39 current AUC: 0.9882 current accuracy: 0.8634 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 40/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.152]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.152]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.3]  \n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.3]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0953]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.0953]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.196] \n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.422]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.422]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.30it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.49] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.49]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.222]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.222]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.18]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 average loss: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  57%|█████▋    | 40/70 [16:39<12:26, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 40 current AUC: 0.9922 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 41/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.19] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.19]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.362]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.452]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.452]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.12] \n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.12]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.216]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.216]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.395]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.21it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.272]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.272]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.2]  \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.497]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.497]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.268]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.268]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 average loss: 0.2559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  59%|█████▊    | 41/70 [17:04<12:01, 24.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 41 current AUC: 0.9917 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 42/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.15]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.15]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.295]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.427]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.391]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.391]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.24] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.24]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.176]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 average loss: 0.2275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 42/70 [17:29<11:37, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 42 current AUC: 0.9922 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 43/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.121]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.121]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.0846]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.31it/s, train_loss=0.0846]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.31it/s, train_loss=0.237] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.507]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.32] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.32]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.368]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.368]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.165]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 average loss: 0.2088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  61%|██████▏   | 43/70 [17:54<11:10, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 43 current AUC: 0.9908 current accuracy: 0.8944 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 44/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.117]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.117]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.27it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.27it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.263]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.263]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.23] \n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.23]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.368]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.368]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.353]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.353]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.265]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.265]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.29it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.338]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.155]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.155]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.318]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.318]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.385]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.31it/s, train_loss=0.385]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.31it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.32it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.32it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.29it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.56it/s, train_loss=0.157]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 average loss: 0.2291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  63%|██████▎   | 44/70 [18:18<10:42, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 44 current AUC: 0.9925 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 45/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.371]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.1]  \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.1]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.567]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.567]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.27it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.27it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.26it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.21] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.222]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.222]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.303]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 average loss: 0.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  64%|██████▍   | 45/70 [18:43<10:17, 24.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 45 current AUC: 0.9897 current accuracy: 0.8944 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 46/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.30it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.30it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.26] \n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.183]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.183]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.138]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.138]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.32it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.32it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.30it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.30it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.272]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.272]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.487]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.60it/s, train_loss=0.487]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 average loss: 0.2279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  66%|██████▌   | 46/70 [19:07<09:51, 24.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 46 current AUC: 0.9898 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 47/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.361]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.0995]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0995]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.268] \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.268]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.339]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.339]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.377]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.377]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.121]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.121]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.28it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.28] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.28]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.455]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.455]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.254]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.58it/s, train_loss=0.346]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 average loss: 0.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 47/70 [19:32<09:26, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 47 current AUC: 0.9891 current accuracy: 0.8758 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 48/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.293]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.107]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.107]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.4]  \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.4]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.324]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.283]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.283]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.27it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.27it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.269]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.28it/s, train_loss=0.269]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.328]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.366]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.366]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.319]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.363]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.632]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.632]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 average loss: 0.2587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  69%|██████▊   | 48/70 [19:56<09:02, 24.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 48 current AUC: 0.9893 current accuracy: 0.8758 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 49/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.374]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.374]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0996]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.0996]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.264] \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.348]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.31it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.31it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.31it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.31it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.30it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.30it/s, train_loss=0.403]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.403]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.30it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.30it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.30it/s, train_loss=0.39] \n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.404]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.25it/s, train_loss=0.404]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.303]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.124]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.124]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.322]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.25] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.25]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.239]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.239]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.387]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.276]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.59it/s, train_loss=0.276]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 average loss: 0.2527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 49/70 [20:21<08:36, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 49 current AUC: 0.9890 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 50/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.28it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0911]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.0911]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.202] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.353]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.353]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.459]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0911]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0911]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.193] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.394]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.394]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.249]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.249]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.326]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.326]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.269]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.269]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 average loss: 0.2344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  71%|███████▏  | 50/70 [20:46<08:13, 24.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 50 current AUC: 0.9910 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 51/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.356]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.329]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.329]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.30it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.30it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.436]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.349]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.16] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.278]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.278]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.39] \n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.21it/s, train_loss=0.294]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.21it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.435]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.435]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.384]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.151]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 average loss: 0.2483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  73%|███████▎  | 51/70 [21:11<07:50, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 51 current AUC: 0.9904 current accuracy: 0.8882 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 52/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.25it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.25it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.364]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.28it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.337]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.292]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.16] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.2] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.302]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.132]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.132]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.15] \n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.15]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.323]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.323]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.241]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.241]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.125]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.125]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.245]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 average loss: 0.2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  74%|███████▍  | 52/70 [21:36<07:27, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 52 current AUC: 0.9920 current accuracy: 0.8820 best AUC: 0.9930 at epoch: 11\n",
      "----------\n",
      "epoch 53/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=0.3]  \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.3]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.342]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.305]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.305]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.308]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.308]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.298]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.298]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.326]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.326]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.259]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.0901]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.0901]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.224] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.23it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.736]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.736]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 average loss: 0.2445\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  76%|███████▌  | 53/70 [22:02<07:07, 25.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 53 current AUC: 0.9959 current accuracy: 0.9068 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 54/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.284]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.284]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.321]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.289]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.355]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.355]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.365]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.365]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.248]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.285]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.325]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.325]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.171]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.171]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.125]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.125]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.233]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.344]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.344]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 average loss: 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  77%|███████▋  | 54/70 [22:27<06:41, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 54 current AUC: 0.9910 current accuracy: 0.8634 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 55/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.122]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.122]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.258]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.102]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.102]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.27] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.27]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:17,  1.23it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.086]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.086]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.11] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.11]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.333]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.296]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.23]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.23]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.307]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.264]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.343]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.434]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.434]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 average loss: 0.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  79%|███████▊  | 55/70 [22:51<06:14, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 55 current AUC: 0.9929 current accuracy: 0.8882 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 56/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.234]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.298]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.298]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.26] \n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.304]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.409]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.106]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.106]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.132]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.132]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.279]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.315]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.19] \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.19]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.21] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.2] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.131]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 average loss: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 56/70 [23:16<05:49, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 56 current AUC: 0.9911 current accuracy: 0.8758 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 57/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.235]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.235]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.103]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.103]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.21] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.225]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0616]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0616]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.181] \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.238]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.107]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:13,  1.22it/s, train_loss=0.107]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.11] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.28it/s, train_loss=0.11]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.161]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.246]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.0831]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.0831]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.14]  \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.60it/s, train_loss=0.14]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 average loss: 0.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  81%|████████▏ | 57/70 [23:41<05:22, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 57 current AUC: 0.9902 current accuracy: 0.9068 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 58/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.25it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.25it/s, train_loss=0.247]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.247]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.222]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.222]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.19] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.19]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.253]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.112]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.112]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.275]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.216]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.216]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.124]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.124]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.30it/s, train_loss=0.281]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.30it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.30it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.30it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.30it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.29it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.153]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.155]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.155]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.183]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.183]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.164]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.164]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0653]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0653]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.174] \n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.29it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.0998]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.0998]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.30it/s, train_loss=0.274] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.274]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 average loss: 0.1783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  83%|████████▎ | 58/70 [24:05<04:56, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 58 current AUC: 0.9906 current accuracy: 0.8634 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 59/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:21,  1.33it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:21,  1.33it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.106]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.106]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.114]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.114]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.111]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:17,  1.23it/s, train_loss=0.111]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.21] \n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.21]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.261]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.2]  \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.216]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.216]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.461]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.461]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.19] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.19]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.316]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.206]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59 average loss: 0.2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  84%|████████▍ | 59/70 [24:30<04:32, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 59 current AUC: 0.9918 current accuracy: 0.8944 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 60/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.132]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.132]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.195]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.3]  \n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.3]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.372]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.29it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.29it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.252]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.31it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.31it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.30it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.30it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.32it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.32it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.2] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.27it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.32it/s, train_loss=0.317]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.32it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.31it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.31it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.31it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.31it/s, train_loss=0.16] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.30it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.30it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.57it/s, train_loss=0.137]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 average loss: 0.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|████████▌ | 60/70 [24:54<04:06, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 60 current AUC: 0.9916 current accuracy: 0.8882 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 61/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.168]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.168]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.383]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.145]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.346]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.119]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.119]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.0534]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.0534]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.096] \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.096]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.439]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.439]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.12] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.12]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.286]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.138]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.138]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.219]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.237]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.182]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.2]  \n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.172]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.135]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.439]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.439]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61 average loss: 0.2091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  87%|████████▋ | 61/70 [25:19<03:42, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 61 current AUC: 0.9928 current accuracy: 0.8758 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 62/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0919]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0919]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.373] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.373]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.13] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.13]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.15]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.15]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.332]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0608]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.0608]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.146] \n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.20it/s, train_loss=0.245]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.20it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.20it/s, train_loss=0.208]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.20it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0746]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0746]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.218] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.451]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.189]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.28it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.144]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.09] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.09]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62 average loss: 0.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  89%|████████▊ | 62/70 [25:44<03:18, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 62 current AUC: 0.9881 current accuracy: 0.8571 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 63/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.156]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.229]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0566]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0566]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.181] \n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.218]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.311]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.193]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.115]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.115]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.124]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.124]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.31it/s, train_loss=0.266]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.31it/s, train_loss=0.0433]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.0433]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.255] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.255]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.179]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.13] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.13]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.152]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.152]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.16] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.359]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.217]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.31it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.31it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.04] \n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:23<00:00,  1.62it/s, train_loss=0.04]\n",
      "\u001b[A                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63 average loss: 0.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 63/70 [26:09<02:52, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 63 current AUC: 0.9906 current accuracy: 0.8820 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 64/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.209]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.143]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.406]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.128]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.198]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.177]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0361]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.0361]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.196] \n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.211]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.169]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.203]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.30it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.30it/s, train_loss=0.0811]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.0811]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.201] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0682]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0682]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.113] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.131]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.327]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.0766]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.0766]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.106] \n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.106]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.299]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.50it/s, train_loss=0.118]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 average loss: 0.1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  91%|█████████▏| 64/70 [26:33<02:28, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 64 current AUC: 0.9891 current accuracy: 0.8820 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 65/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.086]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.086]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.11] \n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.11]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.199]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.068]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.068]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.138]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.138]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.147]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.147]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0954]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0954]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.122] \n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.122]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.377]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.377]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0997]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0997]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.163] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.147]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.147]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.393]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.158]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.313]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.282]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.256]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0992]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0992]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.185] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.247]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.247]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.386]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.306]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.592]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.592]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 average loss: 0.2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  93%|█████████▎| 65/70 [26:58<02:03, 24.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 65 current AUC: 0.9916 current accuracy: 0.8758 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 66/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.223]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.414]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.414]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.38] \n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.38]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.213]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.214]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.431]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.163]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.187]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.267]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0917]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0917]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.133] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.20it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.20it/s, train_loss=0.168]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.168]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.251]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.262]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.26] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.411]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.181]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.226]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.244]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.22] \n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.22]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.159]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.15] \n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.15]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.191]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.191]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66 average loss: 0.2235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  94%|█████████▍| 66/70 [27:23<01:39, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 66 current AUC: 0.9906 current accuracy: 0.8758 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 67/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.232]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.271]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.291]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.105]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.105]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.13] \n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.13]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.19] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.19]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.174]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.142]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.31it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.31it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.31it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.31it/s, train_loss=0.126]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.126]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.31it/s, train_loss=0.141]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.31it/s, train_loss=0.0907]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.28it/s, train_loss=0.0907]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.277] \n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.277]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.268]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.268]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.171]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=0.171]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.30it/s, train_loss=0.147]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.25it/s, train_loss=0.147]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.634]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.634]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 average loss: 0.1869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  96%|█████████▌| 67/70 [27:48<01:14, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 67 current AUC: 0.9918 current accuracy: 0.8944 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 68/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.139]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.162]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.16] \n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.16]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0814]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0814]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.207] \n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.207]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.167]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.113]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.287]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.22it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.242]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.149]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.228]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.236]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.0762]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0762]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.274] \n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.274]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.206]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.133]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.114]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.114]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.205]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.2]  \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.2]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0969]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.0969]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.202] \n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.29] \n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.29]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.201]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.39] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.39]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.204]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.273]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.273]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 average loss: 0.1949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  97%|█████████▋| 68/70 [28:13<00:49, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 68 current AUC: 0.9952 current accuracy: 0.9068 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 69/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.178]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.104]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.104]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.184]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.367]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.137]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.13] \n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.13]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.257]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.185]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.221]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.186]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.146]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.17] \n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.17]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.297]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.197]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.288]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.202]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.224]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.196]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.188]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.175]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.14] \n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.14]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.26]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.129]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.29it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.148]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.165]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.61it/s, train_loss=0.165]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 average loss: 0.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  99%|█████████▊| 69/70 [28:38<00:24, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 69 current AUC: 0.9918 current accuracy: 0.8944 best AUC: 0.9959 at epoch: 53\n",
      "----------\n",
      "epoch 70/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aining Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.136]\n",
      "\u001b[Aining Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.227]\n",
      "\u001b[Aining Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.0875]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.0875]\n",
      "\u001b[Aining Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.12]  \n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.12]\n",
      "\u001b[Aining Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.134]\n",
      "\u001b[Aining Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.126]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.126]\n",
      "\u001b[Aining Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.118]\n",
      "\u001b[Aining Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.192]\n",
      "\u001b[Aining Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.709]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.709]\n",
      "\u001b[Aining Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.194]\n",
      "\u001b[Aining Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.176]\n",
      "\u001b[Aining Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.173]\n",
      "\u001b[Aining Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.212]\n",
      "\u001b[Aining Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0713]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0713]\n",
      "\u001b[Aining Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.114] \n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.114]\n",
      "\u001b[Aining Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.157]\n",
      "\u001b[Aining Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.154]\n",
      "\u001b[Aining Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.0872]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0872]\n",
      "\u001b[Aining Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0998]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.23it/s, train_loss=0.0998]\n",
      "\u001b[Aining Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.123] \n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.123]\n",
      "\u001b[Aining Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.18] \n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.18]\n",
      "\u001b[Aining Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.109]\n",
      "\u001b[Aining Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.30it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.166]\n",
      "\u001b[Aining Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=0.116]\n",
      "\u001b[Aining Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.215]\n",
      "\u001b[Aining Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.243]\n",
      "\u001b[Aining Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.151]\n",
      "\u001b[Aining Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.101]\n",
      "\u001b[Aining Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.605]\n",
      "\u001b[Aining Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.605]\n",
      "\u001b[A                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70 average loss: 0.1794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 70/70 [29:02<00:00, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 70 current AUC: 0.9917 current accuracy: 0.8758 best AUC: 0.9959 at epoch: 53\n",
      "train completed, best_metric: 0.9959 at epoch: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process()\n",
    "start_cpu = process.cpu_times()\n",
    "start_mem = process.memory_info().rss / 1024**2  # В MB\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=False)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            print('1')\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model_3d_pretrained.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1742.94 seconds\n",
      "CPU time used: 14937.51 seconds\n",
      "Memory used: 391.32 MB\n",
      "GPU Memory Used: 793.94 MB\n",
      "Max GPU Memory Used: 13104.66 MB\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "end_cpu = process.cpu_times()\n",
    "end_mem = process.memory_info().rss / 1024**2\n",
    "\n",
    "cpu_time = (end_cpu.user + end_cpu.system) - (start_cpu.user + start_cpu.system)\n",
    "memory_used = end_mem - start_mem\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"CPU time used: {cpu_time:.2f} seconds\")\n",
    "print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Max GPU Memory Used: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM currently used: 793.94 MB\n",
      "Max VRAM used during training: 13104.66 MB\n"
     ]
    }
   ],
   "source": [
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    vram_used = torch.cuda.memory_allocated() / 1024**2\n",
    "    vram_max_used = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"VRAM currently used: {vram_used:.2f} MB\")\n",
    "    print(f\"Max VRAM used during training: {vram_max_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9838\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_3d_pretrained.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_predicted = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    y = torch.tensor([], dtype=torch.long, device=device)\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'],\n",
    "            test_data['label'][:, 0].type(torch.LongTensor),\n",
    "        )\n",
    "\n",
    "        output = model(test_images.to(device))\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_predicted.append(pred[i].item())\n",
    "\n",
    "        y_pred = torch.cat([y_pred, output], dim=0)\n",
    "        y = torch.cat([y, test_labels.to(device)], dim=0)\n",
    "\n",
    "    # Evaluate AUC and accuracy\n",
    "    y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "    y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "    auc_metric(y_pred_act, y_onehot)\n",
    "    result = auc_metric.aggregate()\n",
    "    auc_metric.reset()\n",
    "\n",
    "    print(f\"Validation AUC: {result:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9857    1.0000    0.9928        69\n",
      "           1     0.7069    0.6029    0.6508        68\n",
      "           2     0.6329    0.7246    0.6757        69\n",
      "           3     0.7115    0.5692    0.6325        65\n",
      "           4     0.6429    0.6923    0.6667        65\n",
      "           5     0.9394    0.9394    0.9394        66\n",
      "           6     1.0000    0.8214    0.9020        28\n",
      "           7     1.0000    1.0000    1.0000        21\n",
      "           8     1.0000    1.0000    1.0000        21\n",
      "           9     0.9041    0.9565    0.9296        69\n",
      "          10     0.8961    1.0000    0.9452        69\n",
      "\n",
      "    accuracy                         0.8262       610\n",
      "   macro avg     0.8563    0.8460    0.8486       610\n",
      "weighted avg     0.8262    0.8262    0.8237       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_predicted, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM currently used: 540.74 MB\n",
      "Max VRAM used during training: 13104.66 MB\n"
     ]
    }
   ],
   "source": [
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "    vram_used = torch.cuda.memory_allocated() / 1024**2\n",
    "    vram_max_used = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"VRAM currently used: {vram_used:.2f} MB\")\n",
    "    print(f\"Max VRAM used during training: {vram_max_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
