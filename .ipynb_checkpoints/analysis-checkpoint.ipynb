{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e11ee1-790d-47c3-841e-5fc237a45b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marus\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\marus\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "C:\\Users\\marus\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23bf741b-622a-4b47-ac2b-0cba71bf9e04",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3139001555.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\marus\\AppData\\Local\\Temp\\ipykernel_10828\\3139001555.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    print(metrics)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def extract_metrics(file_path, has_auc=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if has_auc:\n",
    "        metrics = {\n",
    "        'accuracy': df.loc[df.index[-4], 'precision'],\n",
    "        'f1_macro': df.loc[df.index[-3], 'f1-score'],\n",
    "        'auc': df.loc[df.index[-1], 'precision']\n",
    "    }\n",
    "    else:\n",
    "        metrics = {\n",
    "        'accuracy': df.loc[df.index[-3], 'precision'],\n",
    "        'f1_macro': df.loc[df.index[-2], 'f1-score']\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1a8e19-8ce4-4bce-bfcb-62f39cb553a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_random_pretrained', 'coronal_random_pretrained', \n",
    "               'sagittal_random_pretrained', 'combined_pretrained_random',\n",
    "               'weighted_pretrained_random', 'balanced_pretrained_random']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"{model}*.csv\")\n",
    "    for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431a467a-dde2-4512-a317-9a09d8f2966a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10828\\2348205299.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_macro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     comparison[metric] = {\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;34m'before_combined'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;34m'combined'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'combined_pretrained'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;34m'weighted'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weighted_pretrained'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10828\\2348205299.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_macro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     comparison[metric] = {\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;34m'before_combined'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;34m'combined'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'combined_pretrained'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;34m'weighted'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weighted_pretrained'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_pretrained_random', \n",
    "                                                           'weighted_pretrained_random',\n",
    "                                                           'balanced_pretrained_random']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-3]],\n",
    "        'combined': summary['combined_pretrained']['mean'][metric],\n",
    "        'weighted': summary['weighted_pretrained']['mean'][metric],\n",
    "        'balanced': summary['balanced_pretrained']['mean'][metric]\n",
    "    }\n",
    "    \n",
    "\n",
    "if 'auc' in summary['combined_pretrained']['mean'] and 'auc' in summary['weighted_pretrained']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_pretrained']['mean']['auc'],\n",
    "        'weighted': summary['weighted_pretrained']['mean']['auc'],\n",
    "        'balanced': summary['balanced_pretrained']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb0fd18-d000-42d5-b450-735746a853ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['axial_random_pretrained',\n",
       " 'coronal_random_pretrained',\n",
       " 'sagittal_random_pretrained']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7164808f-db41-4d06-ac27-413628dd4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_full_pretrained\n",
      "Mean:\n",
      "accuracy    0.871475\n",
      "f1_macro    0.888182\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.016981\n",
      "f1_macro    0.016026\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_full_pretrained\n",
      "Mean:\n",
      "accuracy    0.781311\n",
      "f1_macro    0.805876\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.014083\n",
      "f1_macro    0.017650\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_full_pretrained\n",
      "Mean:\n",
      "accuracy    0.760328\n",
      "f1_macro    0.784956\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.025381\n",
      "f1_macro    0.023288\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_pretrained\n",
      "Mean:\n",
      "accuracy    0.847360\n",
      "f1_macro    0.821272\n",
      "auc         0.956430\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.020793\n",
      "f1_macro    0.011342\n",
      "auc         0.071599\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_pretrained\n",
      "Mean:\n",
      "accuracy    0.858830\n",
      "f1_macro    0.821501\n",
      "auc         0.985380\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.017338\n",
      "f1_macro    0.016866\n",
      "auc         0.003168\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8714754098360654, 0.781311475409836, 0.7603278688524591]\n",
      "Combined: 0.8473599761806841\n",
      "Weighted: 0.8588303904176033\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8881820450709098, 0.8058760445067431, 0.7849564386177429]\n",
      "Combined: 0.8212721669749746\n",
      "Weighted: 0.8215007824601115\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9564304295536997\n",
      "Weighted: 0.9853799999999999\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4121534-a549-41fc-8b64-ff2c64ce46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_full_unpretrained', 'coronal_full_unpretrained', \n",
    "               'sagittal_full_unpretrained', 'combined_unpretrained', 'weighted_unpretrained']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"{model}*.csv\")\n",
    "    for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8362e97c-a5a7-44f6-be9c-b61f36d7ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_unpretrained', \n",
    "                                                           'weighted_unpretrained']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-2]],\n",
    "        'combined': summary['combined_unpretrained']['mean'][metric],\n",
    "        'weighted': summary['weighted_unpretrained']['mean'][metric]\n",
    "    }\n",
    "\n",
    "if 'auc' in summary['combined_unpretrained']['mean'] and 'auc' in summary['weighted_unpretrained']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_unpretrained']['mean']['auc'],\n",
    "        'weighted': summary['weighted_unpretrained']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f504866a-951f-4120-9576-213f5b607de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_full_unpretrained\n",
      "Mean:\n",
      "accuracy    0.845574\n",
      "f1_macro    0.861371\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.043364\n",
      "f1_macro    0.041646\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_full_unpretrained\n",
      "Mean:\n",
      "accuracy    0.247541\n",
      "f1_macro    0.200427\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.027554\n",
      "f1_macro    0.042027\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_full_unpretrained\n",
      "Mean:\n",
      "accuracy    0.548197\n",
      "f1_macro    0.515055\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.042585\n",
      "f1_macro    0.057108\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_unpretrained\n",
      "Mean:\n",
      "accuracy    0.889884\n",
      "f1_macro    0.874525\n",
      "auc         0.972017\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.019434\n",
      "f1_macro    0.022977\n",
      "auc         0.047795\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_unpretrained\n",
      "Mean:\n",
      "accuracy    0.885446\n",
      "f1_macro    0.863117\n",
      "auc         0.990220\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.014425\n",
      "f1_macro    0.020344\n",
      "auc         0.002541\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8455737704918033, 0.24754098360655732, 0.5481967213114753]\n",
      "Combined: 0.8898843723038238\n",
      "Weighted: 0.885445844312309\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8613705475407917, 0.20042662410844864, 0.5150549274888242]\n",
      "Combined: 0.8745251409244393\n",
      "Weighted: 0.8631166294107425\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9720166886581225\n",
      "Weighted: 0.9902200000000001\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4a4d7-ab77-455d-80d3-f16df7cbbf53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
