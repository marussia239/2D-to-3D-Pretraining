{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e11ee1-790d-47c3-841e-5fc237a45b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23bf741b-622a-4b47-ac2b-0cba71bf9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(file_path, has_auc=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    metrics = {\n",
    "        'accuracy': df.loc[df.index[-3], 'precision'],\n",
    "        'f1_macro': df.loc[df.index[-2], 'f1-score']\n",
    "    }\n",
    "    if has_auc:\n",
    "        metrics['auc'] = df.loc[df.index[-1], 'precision']\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd1a8e19-8ce4-4bce-bfcb-62f39cb553a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_full_pretrained', 'coronal_full_pretrained', \n",
    "               'sagittal_full_pretrained', 'combined_pretrained', 'weighted_pretrained']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"{model}*.csv\")\n",
    "    for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431a467a-dde2-4512-a317-9a09d8f2966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_pretrained', \n",
    "                                                           'weighted_pretrained']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-2]],\n",
    "        'combined': summary['combined_pretrained']['mean'][metric],\n",
    "        'weighted': summary['weighted_pretrained']['mean'][metric]\n",
    "    }\n",
    "\n",
    "if 'auc' in summary['combined_pretrained']['mean'] and 'auc' in summary['weighted_pretrained']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_pretrained']['mean']['auc'],\n",
    "        'weighted': summary['weighted_pretrained']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7164808f-db41-4d06-ac27-413628dd4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_full_pretrained\n",
      "Mean:\n",
      "accuracy    0.871475\n",
      "f1_macro    0.888182\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.016981\n",
      "f1_macro    0.016026\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_full_pretrained\n",
      "Mean:\n",
      "accuracy    0.781311\n",
      "f1_macro    0.805876\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.014083\n",
      "f1_macro    0.017650\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_full_pretrained\n",
      "Mean:\n",
      "accuracy    0.760328\n",
      "f1_macro    0.784956\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.025381\n",
      "f1_macro    0.023288\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_pretrained\n",
      "Mean:\n",
      "accuracy    0.847360\n",
      "f1_macro    0.821272\n",
      "auc         0.956430\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.020793\n",
      "f1_macro    0.011342\n",
      "auc         0.071599\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_pretrained\n",
      "Mean:\n",
      "accuracy    0.858830\n",
      "f1_macro    0.821501\n",
      "auc         0.985380\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.017338\n",
      "f1_macro    0.016866\n",
      "auc         0.003168\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8714754098360654, 0.781311475409836, 0.7603278688524591]\n",
      "Combined: 0.8473599761806841\n",
      "Weighted: 0.8588303904176033\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8881820450709098, 0.8058760445067431, 0.7849564386177429]\n",
      "Combined: 0.8212721669749746\n",
      "Weighted: 0.8215007824601115\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9564304295536997\n",
      "Weighted: 0.9853799999999999\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4121534-a549-41fc-8b64-ff2c64ce46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['axial_full_unpretrained', 'coronal_full_unpretrained', \n",
    "               'sagittal_full_unpretrained', 'combined_unpretrained', 'weighted_unpretrained']\n",
    "csv_files = {\n",
    "    model: glob.glob(f\"{model}*.csv\")\n",
    "    for model in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8362e97c-a5a7-44f6-be9c-b61f36d7ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model, files in csv_files.items():\n",
    "    model_metrics = []\n",
    "    for file in files:\n",
    "        metrics = extract_metrics(file, has_auc=(model in ['combined_unpretrained', \n",
    "                                                           'weighted_unpretrained']))\n",
    "        model_metrics.append(metrics)\n",
    "    results[model] = pd.DataFrame(model_metrics)\n",
    "\n",
    "summary = {}\n",
    "for model, metrics_df in results.items():\n",
    "    summary[model] = {\n",
    "        'mean': metrics_df.mean(),\n",
    "        'std': metrics_df.std()\n",
    "    }\n",
    "\n",
    "comparison = {}\n",
    "for metric in ['accuracy', 'f1_macro']:\n",
    "    comparison[metric] = {\n",
    "        'before_combined': [summary[model]['mean'][metric] for model in model_names[:-2]],\n",
    "        'combined': summary['combined_unpretrained']['mean'][metric],\n",
    "        'weighted': summary['weighted_unpretrained']['mean'][metric]\n",
    "    }\n",
    "\n",
    "if 'auc' in summary['combined_unpretrained']['mean'] and 'auc' in summary['weighted_unpretrained']['mean']:\n",
    "    comparison['auc'] = {\n",
    "        'before_combined': None,\n",
    "        'combined': summary['combined_unpretrained']['mean']['auc'],\n",
    "        'weighted': summary['weighted_unpretrained']['mean']['auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f504866a-951f-4120-9576-213f5b607de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: axial_full_unpretrained\n",
      "Mean:\n",
      "accuracy    0.845574\n",
      "f1_macro    0.861371\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.043364\n",
      "f1_macro    0.041646\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: coronal_full_unpretrained\n",
      "Mean:\n",
      "accuracy    0.247541\n",
      "f1_macro    0.200427\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.027554\n",
      "f1_macro    0.042027\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: sagittal_full_unpretrained\n",
      "Mean:\n",
      "accuracy    0.548197\n",
      "f1_macro    0.515055\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.042585\n",
      "f1_macro    0.057108\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: combined_unpretrained\n",
      "Mean:\n",
      "accuracy    0.889884\n",
      "f1_macro    0.874525\n",
      "auc         0.972017\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.019434\n",
      "f1_macro    0.022977\n",
      "auc         0.047795\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: weighted_unpretrained\n",
      "Mean:\n",
      "accuracy    0.885446\n",
      "f1_macro    0.863117\n",
      "auc         0.990220\n",
      "dtype: float64\n",
      "Std:\n",
      "accuracy    0.014425\n",
      "f1_macro    0.020344\n",
      "auc         0.002541\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Comparison of metrics:\n",
      "Metric: accuracy\n",
      "Before combined: [0.8455737704918033, 0.24754098360655732, 0.5481967213114753]\n",
      "Combined: 0.8898843723038238\n",
      "Weighted: 0.885445844312309\n",
      "========================================\n",
      "Metric: f1_macro\n",
      "Before combined: [0.8613705475407917, 0.20042662410844864, 0.5150549274888242]\n",
      "Combined: 0.8745251409244393\n",
      "Weighted: 0.8631166294107425\n",
      "========================================\n",
      "Metric: auc\n",
      "Before combined: None\n",
      "Combined: 0.9720166886581225\n",
      "Weighted: 0.9902200000000001\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for model, stats in summary.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Mean:\\n{stats['mean']}\")\n",
    "    print(f\"Std:\\n{stats['std']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"Comparison of metrics:\")\n",
    "for metric, values in comparison.items():\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"Before combined: {values['before_combined']}\")\n",
    "    print(f\"Combined: {values['combined']}\")\n",
    "    print(f\"Weighted: {values['weighted']}\")\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4a4d7-ab77-455d-80d3-f16df7cbbf53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
