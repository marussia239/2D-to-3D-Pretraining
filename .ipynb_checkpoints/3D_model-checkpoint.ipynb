{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Classification with MONAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2441\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: cf815ed4e44a5b8ce67e894ab0bc2765279a1a59\n",
      "MONAI __file__: /mnt/hdd/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.17.1\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.46.2\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import shutil\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import decollate_batch, DataLoader, Dataset\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121, resnet, resnet18\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    RandGaussianNoise,\n",
    "    RandAdjustContrast,\n",
    "    ScaleIntensity, \n",
    "    Transform,\n",
    "    ToTensor,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue working with OrganMNIST3d 64x64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'organmnist3d'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', download=download, size=64)\n",
    "val_dataset = DataClass(split='val', download=download, size=64)\n",
    "test_dataset = DataClass(split='test', download=download, size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/hdd/marina/.medmnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset OrganMNIST3D of size 64 (organmnist3d_64)\n",
       "    Number of datapoints: 971\n",
       "    Root location: /mnt/hdd/marina/.medmnist\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 1\n",
       "    Meaning of labels: {'0': 'liver', '1': 'kidney-right', '2': 'kidney-left', '3': 'femur-right', '4': 'femur-left', '5': 'bladder', '6': 'heart', '7': 'lung-right', '8': 'lung-left', '9': 'spleen', '10': 'pancreas'}\n",
       "    Number of samples: {'train': 971, 'val': 161, 'test': 610}\n",
       "    Description: The source of the OrganMNIST3D is the same as that of the Organ{A,C,S}MNIST. Instead of 2D images, we directly use the 3D bounding boxes and process the images into 28×28×28 to perform multi-class classification of 11 body organs. The same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the source training set are used as training and validation set, respectively, and the same 70 CT scans as the Organ{A,C,S}MNIST from the source test set are treated as the test set.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),\n",
    "        RandGaussianNoise(prob=0.5, mean=0.0, std=0.05),\n",
    "        RandAdjustContrast(gamma=(0.7, 1.3), prob=0.5),\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        ToTensor(),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "y_pred_trans = Compose([Activations(softmax=True)])\n",
    "y_trans = Compose([AsDiscrete(to_onehot=n_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _3D_Dataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return {'images': data, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ = _3D_Dataset(train_dataset, transform=train_transforms)\n",
    "val_dataset_ = _3D_Dataset(val_dataset, transform=val_transforms)\n",
    "test_dataset_ = _3D_Dataset(test_dataset, transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False, spatial_dims=3, n_input_channels=1, num_classes=n_classes).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.00005)\n",
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:02<?, ?it/s, train_loss=2.45]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<01:22,  2.74s/it, train_loss=2.45]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:03<01:22,  2.74s/it, train_loss=2.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:50,  1.74s/it, train_loss=2.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:04<00:50,  1.74s/it, train_loss=2.19]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:38,  1.36s/it, train_loss=2.19]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:05<00:38,  1.36s/it, train_loss=2.27]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.18s/it, train_loss=2.27]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:31,  1.18s/it, train_loss=2.18]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.18s/it, train_loss=2.18]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.18s/it, train_loss=2.13]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=2.13]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:27,  1.10s/it, train_loss=1.99]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=1.99]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:25,  1.05s/it, train_loss=1.92]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.00it/s, train_loss=1.92]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:22,  1.00it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.02it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:21,  1.02it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:19,  1.07it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:19,  1.07it/s, train_loss=1.6] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:18,  1.10it/s, train_loss=1.6]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:18,  1.10it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:17,  1.09it/s, train_loss=1.77]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:17,  1.09it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.11it/s, train_loss=1.83]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:16,  1.11it/s, train_loss=1.91]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.11it/s, train_loss=1.91]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:15,  1.11it/s, train_loss=1.71]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.08it/s, train_loss=1.71]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:14,  1.08it/s, train_loss=1.46]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.05it/s, train_loss=1.46]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.05it/s, train_loss=1.6] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.03it/s, train_loss=1.6]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.03it/s, train_loss=1.7]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.01s/it, train_loss=1.7]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.01s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=1.4] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.13s/it, train_loss=1.4]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=1.58]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.10s/it, train_loss=1.58]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.10s/it, train_loss=1.29]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.04s/it, train_loss=1.29]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=1.3] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.04s/it, train_loss=1.3]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.05s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.05s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.05s/it, train_loss=1.26]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.05s/it, train_loss=1.38]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.08s/it, train_loss=1.38]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.08s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.07s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=1.11]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.02s/it, train_loss=1.11]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.02s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.05s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=1.85]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.16it/s, train_loss=1.85]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average loss: 1.6798\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [00:34<56:19, 34.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.5794 current accuracy: 0.1118 best AUC: 0.5794 at epoch: 1\n",
      "----------\n",
      "epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.01it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.01it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=1.05]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.05it/s, train_loss=1.05]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.05it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.02it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=1.34]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=1.22]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=1.22]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=1.17]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.11s/it, train_loss=1.17]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.11s/it, train_loss=0.9] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.9]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.11s/it, train_loss=1.16]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=1.02]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.15s/it, train_loss=1.02]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.921]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.17s/it, train_loss=0.921]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.17s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:23,  1.24s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:23,  1.24s/it, train_loss=1.04] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:22,  1.24s/it, train_loss=1.04]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:22,  1.24s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.19s/it, train_loss=0.849]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.19s/it, train_loss=0.955]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.23s/it, train_loss=0.955]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.23s/it, train_loss=0.84] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.20s/it, train_loss=0.84]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.20s/it, train_loss=0.788]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.18s/it, train_loss=0.788]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.18s/it, train_loss=0.751]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.751]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.807]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.807]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.623]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.623]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.07s/it, train_loss=0.722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.04s/it, train_loss=0.739]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.00s/it, train_loss=0.739]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.00s/it, train_loss=0.83] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.83]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.05s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.08s/it, train_loss=0.927]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.927]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.10s/it, train_loss=0.61] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.61]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.18s/it, train_loss=0.501]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.18s/it, train_loss=0.501]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.18s/it, train_loss=0.856]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.17s/it, train_loss=0.856]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.17s/it, train_loss=0.747]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.747]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.15s/it, train_loss=0.829]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.13s/it, train_loss=0.829]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.13s/it, train_loss=0.535]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.07it/s, train_loss=0.535]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average loss: 0.8960\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [01:10<57:45, 35.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.9498 current accuracy: 0.5590 best AUC: 0.9498 at epoch: 2\n",
      "----------\n",
      "epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.945]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.12s/it, train_loss=0.945]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.12s/it, train_loss=0.683]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.14s/it, train_loss=0.683]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.14s/it, train_loss=0.661]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.661]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.863]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.12s/it, train_loss=0.863]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.12s/it, train_loss=0.897]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.12s/it, train_loss=0.897]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.12s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.09s/it, train_loss=0.708]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.16s/it, train_loss=0.708]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.16s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.12s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.12s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.619]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.619]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.09s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.703]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.16s/it, train_loss=0.703]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.16s/it, train_loss=0.868]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.17s/it, train_loss=0.868]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.17s/it, train_loss=0.635]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.16s/it, train_loss=0.635]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.16s/it, train_loss=0.613]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.20s/it, train_loss=0.613]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.20s/it, train_loss=0.636]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:18,  1.33s/it, train_loss=0.636]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:21<00:18,  1.33s/it, train_loss=0.471]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.30s/it, train_loss=0.471]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:16,  1.30s/it, train_loss=0.834]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.24s/it, train_loss=0.834]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.24s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.14s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.14s/it, train_loss=0.854]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.854]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.984]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.01s/it, train_loss=0.984]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.01s/it, train_loss=0.695]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.06it/s, train_loss=0.695]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:07,  1.06it/s, train_loss=0.735]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.07it/s, train_loss=0.735]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:06,  1.07it/s, train_loss=0.604]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.01s/it, train_loss=0.604]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.01s/it, train_loss=0.624]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.624]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:04,  1.03it/s, train_loss=0.645]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.645]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.549]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.13s/it, train_loss=0.549]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.13s/it, train_loss=0.649]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.17s/it, train_loss=0.649]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.17s/it, train_loss=0.693]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.19s/it, train_loss=0.693]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.19s/it, train_loss=0.938]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.05it/s, train_loss=0.938]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 average loss: 0.7023\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 3/100 [01:46<58:04, 35.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 3 current AUC: 0.9904 current accuracy: 0.6957 best AUC: 0.9904 at epoch: 3\n",
      "----------\n",
      "epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.14s/it, train_loss=0.599]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.14s/it, train_loss=0.618]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.21s/it, train_loss=0.618]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.21s/it, train_loss=0.446]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.446]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.599]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.15s/it, train_loss=0.599]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.15s/it, train_loss=0.528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.774]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.774]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.13s/it, train_loss=0.553]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.13s/it, train_loss=0.553]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.13s/it, train_loss=0.878]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.878]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.10s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.15s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.15s/it, train_loss=0.615]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.615]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.659]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.13s/it, train_loss=0.659]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.13s/it, train_loss=0.69] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.69]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.672]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.17s/it, train_loss=0.672]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.17s/it, train_loss=0.571]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.17s/it, train_loss=0.571]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.17s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.16s/it, train_loss=0.546]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.16s/it, train_loss=0.781]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.781]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.08s/it, train_loss=0.946]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.946]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.09s/it, train_loss=0.52] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.06s/it, train_loss=0.52]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.06s/it, train_loss=0.486]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.486]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.07s/it, train_loss=0.862]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.11s/it, train_loss=0.862]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.11s/it, train_loss=0.47] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.13s/it, train_loss=0.47]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.851]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.851]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.875]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.875]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.634]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.08s/it, train_loss=0.614]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.614]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.11s/it, train_loss=0.782]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.782]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.573]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.10s/it, train_loss=0.573]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.10s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.06s/it, train_loss=0.674]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.10s/it, train_loss=0.674]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.10s/it, train_loss=0.468]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.12s/it, train_loss=0.468]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.12s/it, train_loss=0.644]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.11it/s, train_loss=0.644]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 average loss: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 4/100 [02:21<56:41, 35.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 4 current AUC: 0.9694 current accuracy: 0.6335 best AUC: 0.9904 at epoch: 3\n",
      "----------\n",
      "epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.702]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.702]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.71] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.16it/s, train_loss=0.71]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.16it/s, train_loss=0.545]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.03s/it, train_loss=0.545]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.03s/it, train_loss=0.584]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.11s/it, train_loss=0.584]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.11s/it, train_loss=0.595]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.13s/it, train_loss=0.595]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.13s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.653]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.13s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.09s/it, train_loss=0.434]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.434]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.15s/it, train_loss=0.538]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.15s/it, train_loss=0.538]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.15s/it, train_loss=0.592]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.17s/it, train_loss=0.592]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.17s/it, train_loss=1.21] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.15s/it, train_loss=1.21]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.15s/it, train_loss=0.363]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.363]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.576]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.576]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.05it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.05it/s, train_loss=0.894]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:13,  1.09it/s, train_loss=0.894]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:13,  1.09it/s, train_loss=0.7]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.08it/s, train_loss=0.7]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:12,  1.08it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.06it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.06it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.03it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.03it/s, train_loss=0.382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.01s/it, train_loss=0.382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.388]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.614]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.04it/s, train_loss=0.614]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.04it/s, train_loss=0.554]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.02s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.02s/it, train_loss=0.807]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.02it/s, train_loss=0.807]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.02it/s, train_loss=0.667]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.05it/s, train_loss=0.667]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.05it/s, train_loss=0.561]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.09it/s, train_loss=0.561]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.09it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.07it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.07it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.04it/s, train_loss=0.641]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.04it/s, train_loss=0.633]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.07it/s, train_loss=0.633]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.07it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.09it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.09it/s, train_loss=1.09] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.35it/s, train_loss=1.09]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 average loss: 0.6060\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 5/100 [02:53<54:19, 34.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 5 current AUC: 0.9954 current accuracy: 0.8075 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.15it/s, train_loss=0.404]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.15it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.14it/s, train_loss=0.518]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.14it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.17it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.17it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.12it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.12it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.11it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.11it/s, train_loss=0.64] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.08it/s, train_loss=0.64]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.08it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.04it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.04it/s, train_loss=0.442]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:23,  1.04s/it, train_loss=0.442]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.04s/it, train_loss=0.831]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:23,  1.05s/it, train_loss=0.831]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.667]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.15s/it, train_loss=0.667]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.493]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.12s/it, train_loss=0.493]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.442]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.11s/it, train_loss=0.442]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.461]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.12s/it, train_loss=0.461]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.12s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.742]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.12s/it, train_loss=0.742]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.625]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.10s/it, train_loss=0.625]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.10s/it, train_loss=0.675]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.14s/it, train_loss=0.675]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.54] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.11s/it, train_loss=0.54]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.11s/it, train_loss=0.541]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.07s/it, train_loss=0.541]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.689]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.14s/it, train_loss=0.689]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.14s/it, train_loss=0.491]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.10s/it, train_loss=0.491]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.10s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.409]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.12s/it, train_loss=0.409]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.506]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.12s/it, train_loss=0.506]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.12s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.554]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.419]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.419]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.02s/it, train_loss=0.65] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.05s/it, train_loss=0.65]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.05s/it, train_loss=0.374]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.07s/it, train_loss=0.374]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.06s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.06s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.552]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.552]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 average loss: 0.5079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 6/100 [03:26<53:02, 33.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 6 current AUC: 0.9858 current accuracy: 0.7143 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.43]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.11s/it, train_loss=0.43]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.11s/it, train_loss=0.421]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.421]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.453]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.02s/it, train_loss=0.453]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.02s/it, train_loss=0.375]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.06it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.06it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.09it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.09it/s, train_loss=0.68] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.04it/s, train_loss=0.68]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.04it/s, train_loss=0.499]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.04it/s, train_loss=0.499]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.04it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.02it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.02it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.02s/it, train_loss=0.24]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.513]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.01it/s, train_loss=0.513]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.01it/s, train_loss=0.445]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.02s/it, train_loss=0.445]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.407]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.407]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.729]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.10s/it, train_loss=0.729]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.495]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.05s/it, train_loss=0.495]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.358]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.12s/it, train_loss=0.358]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.441]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.13s/it, train_loss=0.441]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.10s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.383]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.13s/it, train_loss=0.383]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.13s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.08s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.08s/it, train_loss=0.319]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.10s/it, train_loss=0.319]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.467]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.467]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.12s/it, train_loss=0.34] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.34]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.10s/it, train_loss=0.874]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.526]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.526]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.03s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.03s/it, train_loss=0.529]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.06s/it, train_loss=0.529]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.18s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.18s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.11s/it, train_loss=0.632]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.11s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.675]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.10it/s, train_loss=0.675]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 average loss: 0.4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 7/100 [04:00<52:09, 33.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 7 current AUC: 0.9855 current accuracy: 0.7764 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.27s/it, train_loss=0.337]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.27s/it, train_loss=0.4]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.21s/it, train_loss=0.4]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.21s/it, train_loss=0.364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.11s/it, train_loss=0.364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.11s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.317]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.317]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.339]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.339]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.01it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:23,  1.01it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.06it/s, train_loss=0.524]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:21,  1.06it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.04it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.04it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.04it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.04it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.01it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.01it/s, train_loss=0.646]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.02it/s, train_loss=0.646]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.02it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.03it/s, train_loss=0.402]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.03it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.07it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.07it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.10it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.10it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.04it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.04it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.05it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.05it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.05it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.06it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.06it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:11,  1.01s/it, train_loss=0.412]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.01s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:10,  1.01s/it, train_loss=0.482]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.34] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.00it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.00it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.04it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.04it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.01s/it, train_loss=0.314]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.379]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.05s/it, train_loss=0.379]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.05s/it, train_loss=0.31] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.13s/it, train_loss=0.31]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.13s/it, train_loss=0.402]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.16s/it, train_loss=0.402]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.16s/it, train_loss=0.432]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.17s/it, train_loss=0.432]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.17s/it, train_loss=0.525]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.19s/it, train_loss=0.525]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.19s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.11s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.11s/it, train_loss=0.535]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.08it/s, train_loss=0.535]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 average loss: 0.3947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 8/100 [04:32<50:51, 33.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 8 current AUC: 0.9922 current accuracy: 0.7888 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.10s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.10s/it, train_loss=0.597]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.597]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.04it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.04it/s, train_loss=0.427]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.04s/it, train_loss=0.427]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.04s/it, train_loss=0.543]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.05it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.05it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.02it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.02it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.05it/s, train_loss=0.363]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.05it/s, train_loss=0.426]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.05it/s, train_loss=0.426]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.05it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.05it/s, train_loss=0.503]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.05it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.02it/s, train_loss=0.482]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.02it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.09s/it, train_loss=0.263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.09s/it, train_loss=0.523]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.06s/it, train_loss=0.523]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.06s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.11s/it, train_loss=0.388]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.11s/it, train_loss=0.35] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.08s/it, train_loss=0.35]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.484]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:17,  1.17s/it, train_loss=0.484]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.17s/it, train_loss=0.666]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.14s/it, train_loss=0.666]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.35] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.15s/it, train_loss=0.35]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.15s/it, train_loss=0.353]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.09s/it, train_loss=0.353]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.09s/it, train_loss=0.51] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.20s/it, train_loss=0.51]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.20s/it, train_loss=0.489]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:12,  1.24s/it, train_loss=0.489]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.24s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:11,  1.27s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.27s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.23s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.23s/it, train_loss=0.394]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.22s/it, train_loss=0.394]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.22s/it, train_loss=0.433]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:07,  1.24s/it, train_loss=0.433]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.24s/it, train_loss=0.479]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.17s/it, train_loss=0.479]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.17s/it, train_loss=0.477]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.14s/it, train_loss=0.477]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.14s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.17s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.17s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.21s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.21s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.24s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.24s/it, train_loss=0.519]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.03it/s, train_loss=0.519]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 average loss: 0.4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 9/100 [05:06<51:01, 33.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 9 current AUC: 0.9875 current accuracy: 0.6149 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.03it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.566]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.488]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.27] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.04s/it, train_loss=0.27]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.04s/it, train_loss=0.42]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.42]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.02s/it, train_loss=0.341]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.02it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.02it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.01s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.01s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.01s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.01s/it, train_loss=0.391]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.391]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.349]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.349]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.385]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.385]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.458]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.02it/s, train_loss=0.458]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.02it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.417]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.417]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.347]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.04s/it, train_loss=0.347]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.487]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.13s/it, train_loss=0.487]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.405]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:16,  1.16s/it, train_loss=0.405]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.16s/it, train_loss=0.42] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.14s/it, train_loss=0.42]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.502]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.502]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.422]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.422]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.14s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.14s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.07s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.03s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.02it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.02it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.04s/it, train_loss=0.292]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.04s/it, train_loss=0.28] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.01s/it, train_loss=0.28]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.01s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.01s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.344]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.01it/s, train_loss=0.344]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.01it/s, train_loss=0.312]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.03s/it, train_loss=0.312]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.33] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.02it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.02it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.28it/s, train_loss=0.33]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 average loss: 0.3817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 10/100 [05:39<49:48, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 10 current AUC: 0.9937 current accuracy: 0.9068 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.308]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.308]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.453]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.453]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.04it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.04it/s, train_loss=0.418]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.00s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.00s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.05s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.05s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.448]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.443]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.443]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=0.381]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.381]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.491]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.04it/s, train_loss=0.491]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.04it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.05s/it, train_loss=0.299]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.10s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.12s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.367]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.367]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.14s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.14s/it, train_loss=0.713]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.713]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.09s/it, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.09s/it, train_loss=0.49] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.49]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.296]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.10s/it, train_loss=0.296]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.08s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.08s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.08s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.08s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.07s/it, train_loss=0.439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.07s/it, train_loss=0.308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.06s/it, train_loss=0.308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.261]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.14s/it, train_loss=0.261]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.14s/it, train_loss=0.283]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.18s/it, train_loss=0.283]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.15s/it, train_loss=0.396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.14s/it, train_loss=0.396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.14s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.12s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.12s/it, train_loss=0.368]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.12s/it, train_loss=0.368]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.12s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.09it/s, train_loss=0.217]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 average loss: 0.3499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 11/100 [06:12<49:31, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 11 current AUC: 0.9937 current accuracy: 0.8385 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.298]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.03s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.06s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.07s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.07s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.11s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.11s/it, train_loss=0.37] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.37]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.02s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.02s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.38] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.38]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.483]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.483]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.309]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.03it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.03it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.00s/it, train_loss=0.236]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.00s/it, train_loss=0.231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.382]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.08s/it, train_loss=0.382]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.08s/it, train_loss=0.518]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.13s/it, train_loss=0.518]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.13s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.09s/it, train_loss=0.418]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.12s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.12s/it, train_loss=0.33] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.13s/it, train_loss=0.33]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.08s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.08s/it, train_loss=0.32] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.08s/it, train_loss=0.32]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.08s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.06s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.06s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.09s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.392]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.12s/it, train_loss=0.392]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.327]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.327]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.06s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.05s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.05s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.01s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.01s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.03s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.03s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.02it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.01s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.01s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.01s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.01s/it, train_loss=0.365]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.18it/s, train_loss=0.365]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 average loss: 0.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 12/100 [06:45<48:42, 33.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 12 current AUC: 0.9930 current accuracy: 0.8634 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.23s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.23s/it, train_loss=0.305]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.24s/it, train_loss=0.305]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.24s/it, train_loss=0.206]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:35,  1.27s/it, train_loss=0.206]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:05<00:35,  1.27s/it, train_loss=0.258]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:33,  1.25s/it, train_loss=0.258]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:33,  1.25s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:33,  1.29s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:33,  1.29s/it, train_loss=0.185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.15s/it, train_loss=0.185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.15s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.15s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.15s/it, train_loss=0.373]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.373]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.10s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.04s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.01s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.02s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.04s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.04s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.03s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.03s/it, train_loss=0.236]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.236]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.175]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.175]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.08s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.08s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.18s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.18s/it, train_loss=0.415]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.15s/it, train_loss=0.415]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.15s/it, train_loss=0.457]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.18s/it, train_loss=0.457]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.211]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.14s/it, train_loss=0.211]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.14s/it, train_loss=0.354]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.13s/it, train_loss=0.354]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.10s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.3]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.3]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.182]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.16s/it, train_loss=0.182]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.16s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.07s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.07s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.07s/it, train_loss=0.352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.536]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.07s/it, train_loss=0.536]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.07s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.13s/it, train_loss=0.25] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.25]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.09it/s, train_loss=0.153]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 average loss: 0.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 13/100 [07:20<48:46, 33.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 13 current AUC: 0.9946 current accuracy: 0.8758 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.06s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:31,  1.06s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.18s/it, train_loss=0.361]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.18s/it, train_loss=0.243]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.21s/it, train_loss=0.243]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.21s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:33,  1.23s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:33,  1.23s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.14s/it, train_loss=0.215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.04s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.05s/it, train_loss=0.204]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:23,  1.05s/it, train_loss=0.301]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.08s/it, train_loss=0.301]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.08s/it, train_loss=0.237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.10s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.345]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.06s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.06s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.06s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.06s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.351]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.04s/it, train_loss=0.286]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.286]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.09s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.09s/it, train_loss=0.498]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=0.498]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.06s/it, train_loss=0.376]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.08s/it, train_loss=0.376]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.08s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.09s/it, train_loss=0.357]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.357]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.10s/it, train_loss=0.287]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.14s/it, train_loss=0.287]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.14s/it, train_loss=0.306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.12s/it, train_loss=0.306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.12s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.15s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.15s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.24] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.23s/it, train_loss=0.24]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.23s/it, train_loss=0.734]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.00s/it, train_loss=0.734]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 average loss: 0.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 14/100 [07:54<48:35, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 14 current AUC: 0.9942 current accuracy: 0.8261 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.03it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.05s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.05s/it, train_loss=0.298]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.10s/it, train_loss=0.298]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.10s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.13s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.13s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.413]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.11s/it, train_loss=0.413]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.11s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.23s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.23s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.227]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.15s/it, train_loss=0.227]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.15s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.04s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.00s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.00s/it, train_loss=0.331]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.331]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.15s/it, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.365]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.365]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.292]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.08s/it, train_loss=0.292]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.08s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.06s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.11s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.11s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.338]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.07s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.09s/it, train_loss=0.44] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.44]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.08s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.08s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.476]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.07s/it, train_loss=0.476]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.08s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.08s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.04s/it, train_loss=0.359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.04s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.06s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.01s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.01s/it, train_loss=0.399]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.399]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 average loss: 0.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 15/100 [08:28<47:55, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 15 current AUC: 0.9911 current accuracy: 0.8137 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.10it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.10it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.02it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:28,  1.02it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.07s/it, train_loss=0.249]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.07s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.07s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.07s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.14s/it, train_loss=0.805]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.16s/it, train_loss=0.805]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.16s/it, train_loss=0.449]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.449]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.09s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.03s/it, train_loss=0.21] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.03it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.03it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.00s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.00s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.05s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.03s/it, train_loss=0.325]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.03s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.02it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.02it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.04it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.04it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.04it/s, train_loss=0.55] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.02it/s, train_loss=0.55]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.02it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.05it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.05it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.03it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.03it/s, train_loss=0.37] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.37]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.03it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.331]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.05it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.05it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.02it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.02it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.07it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.07it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.08it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.08it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.07it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.07it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.08it/s, train_loss=0.341]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.08it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.01it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.01it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.01s/it, train_loss=0.356]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.03s/it, train_loss=0.241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.03s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.17it/s, train_loss=0.202]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 average loss: 0.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|█▌        | 16/100 [08:59<46:15, 33.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 16 current AUC: 0.9947 current accuracy: 0.8509 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.21s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.21s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.05s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.05s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.00it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.03it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.03it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.03s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.18] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.09s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.09s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.183]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.00it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.00it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.01it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.01it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.06it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.06it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.06it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.06it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.06it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.06it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:12,  1.09it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.09it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.12it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:11,  1.12it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:10,  1.12it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:10,  1.12it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.08it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:08,  1.12it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:08,  1.12it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:07,  1.14it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:07,  1.14it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:06,  1.15it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:06,  1.15it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.11it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.11it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.08it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.08it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.07it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.07it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.01it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.01it/s, train_loss=0.231]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.07s/it, train_loss=0.231]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.07s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.15s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.15s/it, train_loss=0.253]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.23s/it, train_loss=0.253]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.23s/it, train_loss=0.539]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.02it/s, train_loss=0.539]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 average loss: 0.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 17/100 [09:30<44:54, 32.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 17 current AUC: 0.9909 current accuracy: 0.8323 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.08s/it, train_loss=0.237]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.08s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.03s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.03s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.02it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.02s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.06s/it, train_loss=0.428]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.06s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.229]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.169]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.15s/it, train_loss=0.169]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.15s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.15s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.14s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.10s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.10s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.07s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.07s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.07s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.07s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.297]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.304]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.01it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.01it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.00s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.00s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.05it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.00s/it, train_loss=0.183]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.00s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.01s/it, train_loss=0.272]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.04s/it, train_loss=0.369]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.29] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.15s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.15s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.15s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.12s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.12s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.14s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.14s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.09s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.09s/it, train_loss=0.341]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.341]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 average loss: 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  18%|█▊        | 18/100 [10:03<44:31, 32.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 18 current AUC: 0.9895 current accuracy: 0.7950 best AUC: 0.9954 at epoch: 5\n",
      "----------\n",
      "epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.22s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.22s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.14s/it, train_loss=0.242]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.14s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.323]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.323]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.15s/it, train_loss=0.276]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.15s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.16s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.16s/it, train_loss=0.398]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.14s/it, train_loss=0.398]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.14s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.09s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.09s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.11s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.04s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.04s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.05s/it, train_loss=0.191]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.05s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.22] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.17s/it, train_loss=0.22]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.17s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.18s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.17s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:16,  1.23s/it, train_loss=0.362]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.23s/it, train_loss=0.527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.22s/it, train_loss=0.527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.22s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.19s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.19s/it, train_loss=0.21] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.16s/it, train_loss=0.21]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.16s/it, train_loss=0.45]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.45]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.15s/it, train_loss=0.27]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.10s/it, train_loss=0.27]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.10s/it, train_loss=0.266]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.10s/it, train_loss=0.0867]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.03s/it, train_loss=0.0867]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.03s/it, train_loss=0.232] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.232]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.06s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.12s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.15s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.15s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.10s/it, train_loss=0.426]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.10s/it, train_loss=0.135]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.17it/s, train_loss=0.135]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 average loss: 0.2520\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  19%|█▉        | 19/100 [10:40<45:27, 33.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 19 current AUC: 0.9980 current accuracy: 0.8447 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.27s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.27s/it, train_loss=0.212]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.20s/it, train_loss=0.212]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.20s/it, train_loss=0.445]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.18s/it, train_loss=0.445]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.18s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.18s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.18s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.281]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:28,  1.09s/it, train_loss=0.25] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.25]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.16s/it, train_loss=0.116]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:29,  1.29s/it, train_loss=0.116]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:29,  1.29s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.26s/it, train_loss=0.138]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:12<00:27,  1.26s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:26,  1.26s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:13<00:26,  1.26s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:24,  1.23s/it, train_loss=0.324]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:24,  1.23s/it, train_loss=0.14] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.16s/it, train_loss=0.14]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.16s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.14s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.21s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.21s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.18s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:20<00:16,  1.09s/it, train_loss=0.275]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.14s/it, train_loss=0.275]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:21<00:15,  1.14s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.24s/it, train_loss=0.254]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:16,  1.24s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.216]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.18s/it, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.23s/it, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:25<00:13,  1.23s/it, train_loss=0.26] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.19s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:26<00:11,  1.19s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.14s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:27<00:10,  1.14s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.16s/it, train_loss=0.222]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:28<00:09,  1.16s/it, train_loss=0.29] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.14s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:29<00:07,  1.14s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.16s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:30<00:06,  1.16s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.19s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:31<00:05,  1.19s/it, train_loss=0.316]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.15s/it, train_loss=0.316]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.15s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.282]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.11s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.10s/it, train_loss=0.342]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.10s/it, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.04s/it, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:35<00:01,  1.04s/it, train_loss=0.507] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:35<00:00,  1.21it/s, train_loss=0.507]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 average loss: 0.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 20/100 [11:15<45:46, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 20 current AUC: 0.9971 current accuracy: 0.8634 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.212]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.05s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.05s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.278]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.245]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.04s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.244]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.04s/it, train_loss=0.256]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.04s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.321]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.04s/it, train_loss=0.321]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.04s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.01it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.01it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.01it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.01it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.119]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.519]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.05it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.05it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.05it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.05it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.05it/s, train_loss=0.235] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.235]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.04s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.04s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.01it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.01it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.197]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.08s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.08s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:09,  1.14s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.14s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:08,  1.16s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.16s/it, train_loss=0.429]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.429]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.11s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.11s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.03s/it, train_loss=0.176]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.03s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.07s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.02s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.258]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.16it/s, train_loss=0.258]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 average loss: 0.2309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  21%|██        | 21/100 [11:48<44:29, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 21 current AUC: 0.9973 current accuracy: 0.8571 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.29]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.21s/it, train_loss=0.218]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.21s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.21s/it, train_loss=0.257]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.21s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.214]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.19s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.145]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.03s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.11] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.11s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.16s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.16s/it, train_loss=0.081]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.15s/it, train_loss=0.081]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.15s/it, train_loss=0.304]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.15s/it, train_loss=0.304]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.15s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.221]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.09s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.273]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.04s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.03s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.03s/it, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.01it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:13,  1.01it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.01it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:12,  1.01it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:11,  1.01it/s, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:10,  1.04it/s, train_loss=0.247] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:09,  1.03it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.05it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:08,  1.05it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.05it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.05it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.08s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.07s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.02s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.03it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.03it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.02it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.04it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.04it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.05it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.05it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.26it/s, train_loss=0.181]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 average loss: 0.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 22/100 [12:20<43:23, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 22 current AUC: 0.9840 current accuracy: 0.7764 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.14it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.14it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:29,  1.00s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.00s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.03s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.03s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.10s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.10s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:33,  1.30s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:33,  1.30s/it, train_loss=0.167]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:31,  1.26s/it, train_loss=0.167]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:31,  1.26s/it, train_loss=0.314]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.24s/it, train_loss=0.314]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.24s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.123]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.26s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.26s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:25,  1.21s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:25,  1.21s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.14s/it, train_loss=0.255]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.14s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.279]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.04s/it, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:16,  1.02it/s, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.07it/s, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.07it/s, train_loss=0.221] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:13,  1.09it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:13,  1.09it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:12,  1.10it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:12,  1.10it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:11,  1.12it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:11,  1.12it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:10,  1.13it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:10,  1.13it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:09,  1.12it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:09,  1.12it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:08,  1.14it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:08,  1.14it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:07,  1.13it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:07,  1.13it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:06,  1.16it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:06,  1.16it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:05,  1.17it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:05,  1.17it/s, train_loss=0.0951]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.08it/s, train_loss=0.0951]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.08it/s, train_loss=0.185] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.00it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.00it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.00it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.00s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.00s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.03it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.03it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.06it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.06it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.35it/s, train_loss=0.175]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 average loss: 0.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 23/100 [12:52<42:00, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 23 current AUC: 0.9937 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.01s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.01s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.04it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.04it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.01it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.01it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.03s/it, train_loss=0.117]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.02it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.02it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.00s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.00s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.03it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.03it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.05it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.05it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.06it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.06it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.09it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.09it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.11it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.11it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:13,  1.09it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:13,  1.09it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.07it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.07it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.03it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.07it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.07it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.09it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.09it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:10,  1.07s/it, train_loss=0.313]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.07s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.01s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.01s/it, train_loss=0.246]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.03it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.03it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.03it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.04it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.04it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.04it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.04it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.04it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.04it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.06it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.06it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.03it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.03it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.05s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.05s/it, train_loss=0.315]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.20it/s, train_loss=0.315]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 average loss: 0.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 24/100 [13:22<40:41, 32.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 24 current AUC: 0.9955 current accuracy: 0.9193 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0775]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.11s/it, train_loss=0.0775]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.11s/it, train_loss=0.204] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.01it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:28,  1.01it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.02s/it, train_loss=0.219]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.02s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.08s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.08s/it, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.1]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.14s/it, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.12s/it, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.12s/it, train_loss=0.147] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.19s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.19s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.12] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.17s/it, train_loss=0.12]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.17s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.06s/it, train_loss=0.179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.06s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.02s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.02s/it, train_loss=0.107] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.04it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.04it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.06it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.06it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.00s/it, train_loss=0.294]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.00s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.08it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.08it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.10it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.10it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.06it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.06it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.08it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.08it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.06it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.06it/s, train_loss=0.134] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.03it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.01it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.01it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.05it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.05it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.09it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.09it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.08it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.08it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.08it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.08it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.29it/s, train_loss=0.217]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 average loss: 0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 25/100 [13:53<39:46, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 25 current AUC: 0.9955 current accuracy: 0.8944 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.00it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.00it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.01s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.01s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.08it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.08it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.02it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:31,  1.20s/it, train_loss=0.21]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:31,  1.20s/it, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:30,  1.20s/it, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.20s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.17s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.17s/it, train_loss=0.179] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.179]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.03it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.03it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.05it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.05it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.03it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.03it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.03it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.04it/s, train_loss=0.0896]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.03s/it, train_loss=0.0896]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.03s/it, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.02it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.02it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.04it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.04it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.04it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.04it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.03it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.06it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.06it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.05it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.05it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.01it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.01it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.01s/it, train_loss=0.159]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.03s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.03s/it, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.02s/it, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.01it/s, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.01it/s, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.05s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.05s/it, train_loss=0.148] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.10s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.10s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:01,  1.12s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.12s/it, train_loss=0.557]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.09it/s, train_loss=0.557]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 average loss: 0.1580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|██▌       | 26/100 [14:25<39:18, 31.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 26 current AUC: 0.9907 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.02s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.02s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.04s/it, train_loss=0.089]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.089]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.00s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.00s/it, train_loss=0.134] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.387]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.226]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.04s/it, train_loss=0.226]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.259]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.00s/it, train_loss=0.259]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.00s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.02it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.00it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.03it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.03it/s, train_loss=0.253] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.06it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.06it/s, train_loss=0.0924]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.10it/s, train_loss=0.0924]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.10it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.07it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.07it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:12,  1.10it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.10it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.10it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:11,  1.10it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:10,  1.12it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:10,  1.12it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:09,  1.13it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:09,  1.13it/s, train_loss=0.419] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.09it/s, train_loss=0.419]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.09it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.09it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.09it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.08it/s, train_loss=0.221]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.08it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.02it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.02it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.07it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.07it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.07it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.07it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.09it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.09it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:02,  1.11it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.11it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.08it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.08it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.13it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.13it/s, train_loss=0.38] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.44it/s, train_loss=0.38]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 average loss: 0.1815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 27/100 [14:55<37:54, 31.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 27 current AUC: 0.9947 current accuracy: 0.8385 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.20it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.20it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.357]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.19it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:17,  1.19it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:15,  1.14it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:15,  1.14it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:15,  1.09it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.09it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:16,  1.02s/it, train_loss=0.194]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.02s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:14<00:15,  1.06s/it, train_loss=0.213]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:15,  1.06s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:15<00:14,  1.01s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.01s/it, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:12,  1.01it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:12,  1.01it/s, train_loss=0.11] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:11,  1.06it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.06it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:10,  1.01it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:10,  1.01it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:18<00:09,  1.03it/s, train_loss=0.247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:19<00:09,  1.03it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:19<00:08,  1.01it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:20<00:08,  1.01it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:20<00:07,  1.01it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.01it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:21<00:06,  1.05it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.05it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:22<00:05,  1.07it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.07it/s, train_loss=0.0971]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:23<00:04,  1.04it/s, train_loss=0.0971]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.04it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:24<00:03,  1.07it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:03,  1.07it/s, train_loss=0.122] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:25<00:03,  1.00s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:03,  1.00s/it, train_loss=0.11] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:26<00:01,  1.04it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.04it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:01,  1.08s/it, train_loss=0.135]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:01,  1.08s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.18it/s, train_loss=0.163]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 average loss: 0.1525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|██▊       | 28/100 [15:24<36:36, 30.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 28 current AUC: 0.9963 current accuracy: 0.8634 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.10it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.10it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.11it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.09it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:24,  1.09it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.10s/it, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.321] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.321]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.04s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.04s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.01s/it, train_loss=0.23] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:18,  1.06it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:18,  1.06it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:17,  1.09it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.09it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.12it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.12it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.10it/s, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.10it/s, train_loss=0.144] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.10it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:14,  1.10it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:16,  1.08s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.08s/it, train_loss=0.12] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.06s/it, train_loss=0.12]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.06s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.04s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.04s/it, train_loss=0.206] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.02it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.02it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.07it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.07it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.09it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.09it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.11it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.11it/s, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:06,  1.15it/s, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:06,  1.15it/s, train_loss=0.196] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.01it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.01it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:06,  1.01s/it, train_loss=0.252]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.01s/it, train_loss=0.16] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.01s/it, train_loss=0.16]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.01s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.00it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.02it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.02it/s, train_loss=0.203] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.02it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.02it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.02it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.02it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.29it/s, train_loss=0.185]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 average loss: 0.1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  29%|██▉       | 29/100 [15:54<36:01, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 29 current AUC: 0.9881 current accuracy: 0.8696 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0499]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.0499]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.15it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.15it/s, train_loss=0.177] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.07it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.07it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.10it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.10it/s, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.03it/s, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.03it/s, train_loss=0.227] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:25,  1.01s/it, train_loss=0.227]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.01s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.12s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.19] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.02it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:18,  1.08it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:18,  1.08it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:17,  1.09it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.09it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.07it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.07it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.05it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.05it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.04it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.0653]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.06it/s, train_loss=0.0653]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.06it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:12,  1.09it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.09it/s, train_loss=0.0969]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.11it/s, train_loss=0.0969]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:11,  1.11it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:17<00:10,  1.15it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:10,  1.15it/s, train_loss=0.143] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:09,  1.14it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:09,  1.14it/s, train_loss=0.0839]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.03it/s, train_loss=0.0839]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.03it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.03it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.04it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.04it/s, train_loss=0.184] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.03s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.03s/it, train_loss=0.13] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:06,  1.05s/it, train_loss=0.13]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.05s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.06s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.06s/it, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.00it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.01it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.01it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.01s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.205]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.04s/it, train_loss=0.205]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.04s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.18it/s, train_loss=0.0361]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 average loss: 0.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 30/100 [16:25<35:30, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 30 current AUC: 0.9926 current accuracy: 0.9006 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0644]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.06s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:31,  1.06s/it, train_loss=0.0723]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.0723]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.0753]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.14s/it, train_loss=0.0753]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.14s/it, train_loss=0.121] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.11s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.11s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.271]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.02s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.02s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.12s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.02it/s, train_loss=0.0932]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.05it/s, train_loss=0.0932]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.05it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.00s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.00s/it, train_loss=0.129] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.0674]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.0674]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.03it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.01s/it, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.01s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.05s/it, train_loss=0.274]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.05s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.0898]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.04s/it, train_loss=0.0898]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.04s/it, train_loss=0.132] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.03s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.03s/it, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.00s/it, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.00s/it, train_loss=0.132] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.07s/it, train_loss=0.181]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.11s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.0579]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.13s/it, train_loss=0.0579]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.13s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.054]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.054]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.06s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.04s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.04s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.05s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.05s/it, train_loss=0.137] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.05s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.05s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.07s/it, train_loss=0.0815]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.17it/s, train_loss=0.0815]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 average loss: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  31%|███       | 31/100 [16:57<35:43, 31.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 31 current AUC: 0.9894 current accuracy: 0.8696 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.14s/it, train_loss=0.135]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.14s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.06s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.06s/it, train_loss=0.263]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.07s/it, train_loss=0.263]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.07s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.00s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.00s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.03s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.03it/s, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.03it/s, train_loss=0.063] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.06it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.06it/s, train_loss=0.065]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.07it/s, train_loss=0.065]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.07it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:19,  1.06it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:19,  1.06it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:18,  1.08it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:18,  1.08it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.04it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.04it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:17,  1.01it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.068] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.03it/s, train_loss=0.068]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.01it/s, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.01it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.01it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.03it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.03it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.02it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.02it/s, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.03it/s, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.03it/s, train_loss=0.0589]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.0589]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.08it/s, train_loss=0.066] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.04it/s, train_loss=0.066]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.04it/s, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.00s/it, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.00s/it, train_loss=0.122] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:08,  1.02s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.02s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.02s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.02s/it, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.00it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.00it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.00s/it, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.00s/it, train_loss=0.143] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.04it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.04it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.08it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.08it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.09it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.09it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.11it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.11it/s, train_loss=0.0458]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.32it/s, train_loss=0.0458]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 average loss: 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|███▏      | 32/100 [17:27<34:56, 30.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 32 current AUC: 0.9943 current accuracy: 0.9379 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.07it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:27,  1.07it/s, train_loss=0.0942]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:36,  1.26s/it, train_loss=0.0942]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:36,  1.26s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.20s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.20s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.11s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.11s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.08s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.08s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.128] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.04s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.04s/it, train_loss=0.0818]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.09s/it, train_loss=0.0818]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.09s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.0938]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.0938]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.00it/s, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.168] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.01s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.01s/it, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.04it/s, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.04it/s, train_loss=0.0989]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.06it/s, train_loss=0.0989]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.06it/s, train_loss=0.045] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:12,  1.09it/s, train_loss=0.045]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:12,  1.09it/s, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.05it/s, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.05it/s, train_loss=0.139] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.01it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.02it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.02it/s, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.04it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.04it/s, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.03it/s, train_loss=0.074]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.06it/s, train_loss=0.074]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.06it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.08it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.08it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.09it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.09it/s, train_loss=0.176] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.07it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.07it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.12it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.12it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.14it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.14it/s, train_loss=0.0728]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.30it/s, train_loss=0.0728]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 average loss: 0.0790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 33/100 [17:58<34:25, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 33 current AUC: 0.9899 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.194] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.13it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.13it/s, train_loss=0.0927]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s, train_loss=0.0927]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.11it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.07it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.07it/s, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.10it/s, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.10it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:21,  1.15it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:21,  1.15it/s, train_loss=0.0279]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:21,  1.13it/s, train_loss=0.0279]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:21,  1.13it/s, train_loss=0.0624]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:20,  1.13it/s, train_loss=0.0624]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:20,  1.13it/s, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:19,  1.12it/s, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:19,  1.12it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:19,  1.05it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:19,  1.05it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.02s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.02s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:20,  1.09s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.09s/it, train_loss=0.108] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:19,  1.07s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.07s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:17,  1.00s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.00s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.02s/it, train_loss=0.156]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.02s/it, train_loss=0.0505]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:15,  1.04s/it, train_loss=0.0505]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.04s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.03s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.135] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.03it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.05it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.05it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.04it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.07it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.07it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.06it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.06it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.08it/s, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.08it/s, train_loss=0.203] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.02it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.02it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.02it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.02it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.07it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.07it/s, train_loss=0.163] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:03,  1.08it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.08it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.01s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.01s/it, train_loss=0.27] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.03it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.03it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.02s/it, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:01,  1.02s/it, train_loss=0.0655]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:29<00:00,  1.26it/s, train_loss=0.0655]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 average loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|███▍      | 34/100 [18:28<33:38, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 34 current AUC: 0.9978 current accuracy: 0.9441 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0887]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.0887]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.0822]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.0822]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.159] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.01s/it, train_loss=0.159]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.05it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.05it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.07it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.07it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:22,  1.05it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:22,  1.05it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.06it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.06it/s, train_loss=0.046] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:21,  1.04it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.04it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.01it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.01it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.05it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.05it/s, train_loss=0.13]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.04it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.04it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:17,  1.06it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.06it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.07it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.07it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.03it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.03it/s, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.03it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.03it/s, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.00it/s, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.00it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.03it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.05it/s, train_loss=0.0734]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.05it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.08it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:19<00:09,  1.10it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.10it/s, train_loss=0.0796]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:20<00:08,  1.09it/s, train_loss=0.0796]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.09it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.07it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.07it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.11it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.11it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.12it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.12it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.14it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.14it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:03,  1.13it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.13it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:02,  1.16it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.16it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.15it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.15it/s, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.10it/s, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.10it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.39it/s, train_loss=0.116]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 average loss: 0.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 35/100 [18:57<32:36, 30.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 35 current AUC: 0.9906 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.15it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.15it/s, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:24,  1.13it/s, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:24,  1.13it/s, train_loss=0.0608] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.11it/s, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.11it/s, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.07it/s, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.07it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.06it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.06it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:22,  1.05it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:22,  1.05it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.05it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.05it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.08it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.08it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:18,  1.12it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:18,  1.12it/s, train_loss=0.15]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:17,  1.14it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:17,  1.14it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:16,  1.13it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:16,  1.13it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:15,  1.14it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:15,  1.14it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.15it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:14,  1.15it/s, train_loss=0.073]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:14,  1.13it/s, train_loss=0.073]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:14,  1.13it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:14<00:14,  1.06it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.06it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:15<00:13,  1.01it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.01it/s, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:16<00:13,  1.01s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.01s/it, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:17<00:12,  1.00s/it, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:12,  1.00s/it, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:18<00:10,  1.03it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.03it/s, train_loss=0.171] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:19<00:09,  1.05it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.05it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:20<00:08,  1.04it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:08,  1.04it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:21<00:07,  1.01it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:07,  1.01it/s, train_loss=0.179] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:22<00:06,  1.02it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.02it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:23<00:05,  1.08it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.08it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:24<00:04,  1.01it/s, train_loss=0.0857]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.01it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:25<00:04,  1.01s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:04,  1.01s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:26<00:03,  1.00s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:03,  1.00s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:27<00:01,  1.04it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:01,  1.04it/s, train_loss=0.0421]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.06it/s, train_loss=0.0421]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:28<00:00,  1.06it/s, train_loss=0.133] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:28<00:00,  1.34it/s, train_loss=0.133]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 average loss: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 36/100 [19:27<31:54, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 36 current AUC: 0.9728 current accuracy: 0.7640 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.07it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.07it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.02it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.02it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.02it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.152] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.00it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.00it/s, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.052] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.052]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.03s/it, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.02it/s, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.02it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:21,  1.03it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.03it/s, train_loss=0.128] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.05it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.05it/s, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.02s/it, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.103] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.10s/it, train_loss=0.103]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.10s/it, train_loss=0.0612]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.13s/it, train_loss=0.0612]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.13s/it, train_loss=0.196] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.09s/it, train_loss=0.196]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.12s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.04s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.02s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.02s/it, train_loss=0.115] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.01s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.01s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.04it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.04it/s, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.01it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.01it/s, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.02it/s, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:07,  1.02it/s, train_loss=0.401] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.00it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.00it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.02it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.02it/s, train_loss=0.099] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.02s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.02s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.04it/s, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.04it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.05it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.05it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.06it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.06it/s, train_loss=0.187] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.03it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.03it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.26it/s, train_loss=0.179]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 average loss: 0.0901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 37/100 [19:58<31:47, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 37 current AUC: 0.9912 current accuracy: 0.8634 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0908]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.07it/s, train_loss=0.0908]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.07it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.00it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.06it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.06it/s, train_loss=0.024] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.02s/it, train_loss=0.024]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.04it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.04it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:22,  1.07it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:22,  1.07it/s, train_loss=0.0554]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.08it/s, train_loss=0.0554]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.08it/s, train_loss=0.025] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:19,  1.11it/s, train_loss=0.025]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:19,  1.11it/s, train_loss=0.0803]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:18,  1.12it/s, train_loss=0.0803]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:18,  1.12it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.03s/it, train_loss=0.0294]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.00it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.075] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:18,  1.03s/it, train_loss=0.075]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.03s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.02it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.02it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.02it/s, train_loss=0.0826]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:15,  1.05s/it, train_loss=0.0826]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.05s/it, train_loss=0.121] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.01s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.01s/it, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.04it/s, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.04it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.02it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.02it/s, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:10,  1.03it/s, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:20<00:09,  1.01it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.01it/s, train_loss=0.019] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.03s/it, train_loss=0.019]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.03s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:22<00:08,  1.05s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.05s/it, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:07,  1.07s/it, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.07s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:06,  1.03s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.03s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:05,  1.07s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.07s/it, train_loss=0.0729]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.16s/it, train_loss=0.0729]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.16s/it, train_loss=0.161] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.24s/it, train_loss=0.161]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.24s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.28s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.28s/it, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.18s/it, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.18s/it, train_loss=0.218] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.05it/s, train_loss=0.218]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 average loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|███▊      | 38/100 [20:30<31:53, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 38 current AUC: 0.9904 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.07s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.07s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.02s/it, train_loss=0.0711]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.0711]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.06it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.06it/s, train_loss=0.072] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.05it/s, train_loss=0.072]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.05it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.07it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.07it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.01it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.01it/s, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.00it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.00it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.01s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:20,  1.00it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.00it/s, train_loss=0.166] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.02s/it, train_loss=0.166]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.02s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:19,  1.01s/it, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.01s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:18,  1.01s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.0912]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.0912]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.262] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.01it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.01it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.02it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.02it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.01it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.01it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.01s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.01s/it, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.04s/it, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.04s/it, train_loss=0.302] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:11,  1.08s/it, train_loss=0.302]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0702]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.0702]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.027] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.05s/it, train_loss=0.027]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.01s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.134] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.03s/it, train_loss=0.134]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.03s/it, train_loss=0.0713]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.14s/it, train_loss=0.0713]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.14s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.12s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.12s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.16s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.16s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.16s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.16s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.17s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.17s/it, train_loss=0.159] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.21s/it, train_loss=0.159]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.21s/it, train_loss=0.0364]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.08it/s, train_loss=0.0364]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 average loss: 0.0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  39%|███▉      | 39/100 [21:03<31:54, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 39 current AUC: 0.9850 current accuracy: 0.6832 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.01s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.01s/it, train_loss=0.0834]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.06it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.06it/s, train_loss=0.161] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.161]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.04s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.04s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.016] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.016]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.13s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.0506]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0506]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.07s/it, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.07s/it, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.03s/it, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.03s/it, train_loss=0.175] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.06s/it, train_loss=0.175]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.06s/it, train_loss=0.0605]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.03s/it, train_loss=0.0605]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.03s/it, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.01s/it, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.01s/it, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.03it/s, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.03it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.06it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.06it/s, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.06it/s, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.06it/s, train_loss=0.217] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.06it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.06it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.05it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.05it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.05s/it, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.04s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.10s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.0819]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.08s/it, train_loss=0.0819]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.08s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.14s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.14s/it, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.09s/it, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.09s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.10s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.10s/it, train_loss=0.0582]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.14s/it, train_loss=0.0582]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.14s/it, train_loss=0.26]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.15s/it, train_loss=0.26]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.15s/it, train_loss=0.207]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.03it/s, train_loss=0.207]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 average loss: 0.0822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 40/100 [21:36<31:53, 31.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 40 current AUC: 0.9884 current accuracy: 0.8323 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.01it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.01it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:29,  1.02s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.02s/it, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.07s/it, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.07s/it, train_loss=0.202] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.0907]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.01s/it, train_loss=0.0907]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.01s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.04it/s, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:23,  1.04it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.0669]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.01s/it, train_loss=0.0669]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.01s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.173] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.02it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.02it/s, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.13s/it, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.13s/it, train_loss=0.05]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:19,  1.14s/it, train_loss=0.05]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0816]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.09s/it, train_loss=0.0816]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.141] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.05s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.05s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.03s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.076]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.10s/it, train_loss=0.076]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.10s/it, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.09s/it, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.09s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.15s/it, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.15s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.14s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.14s/it, train_loss=0.0323]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:09,  1.24s/it, train_loss=0.0323]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.24s/it, train_loss=0.101] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.19s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.19s/it, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:07,  1.18s/it, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.18s/it, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.14s/it, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.14s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.09s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.217] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.08s/it, train_loss=0.217]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.08s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.03s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.01it/s, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.01it/s, train_loss=0.0338]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.28it/s, train_loss=0.0338]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 average loss: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  41%|████      | 41/100 [22:09<31:45, 32.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 41 current AUC: 0.9864 current accuracy: 0.8571 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.26s/it, train_loss=0.0236]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.26s/it, train_loss=0.0635]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.23s/it, train_loss=0.0635]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.23s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.10s/it, train_loss=0.112]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.10s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.10s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.10s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.163] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.163]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.04s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.16s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.16s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.099] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.07s/it, train_loss=0.099]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.07s/it, train_loss=0.072]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.072]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.12s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.19s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.19s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.21s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.21s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.19s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.19s/it, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.22s/it, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.22s/it, train_loss=0.142] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.24s/it, train_loss=0.142]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.24s/it, train_loss=0.0843]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.22s/it, train_loss=0.0843]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.22s/it, train_loss=0.051] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.051]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.19s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.12s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.13s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.10s/it, train_loss=0.132] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.22s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.22s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.13s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.13s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.10s/it, train_loss=0.122] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.06s/it, train_loss=0.0499]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.0499]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.0775]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.0775]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.06s/it, train_loss=0.0406]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:01,  1.01it/s, train_loss=0.0406]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:01,  1.01it/s, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.05s/it, train_loss=0.06]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.05s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.22it/s, train_loss=0.126]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 average loss: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 42/100 [22:44<31:54, 33.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 42 current AUC: 0.9935 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.17s/it, train_loss=0.0281]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.17s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.091] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.12s/it, train_loss=0.091]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.12s/it, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.03s/it, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.03s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.04s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.04s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.08s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.08s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.08s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.08s/it, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.05s/it, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.05s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.12s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.15s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.15s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.17s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.17s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.17s/it, train_loss=0.108] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.16s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.16s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.18s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.23s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.23s/it, train_loss=0.0952]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:16,  1.23s/it, train_loss=0.0952]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.23s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.20s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.20s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.24s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.24s/it, train_loss=0.0532] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.23s/it, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:12,  1.23s/it, train_loss=0.00915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.20s/it, train_loss=0.00915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.20s/it, train_loss=0.223]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.19s/it, train_loss=0.223]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.19s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.19s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:29<00:08,  1.19s/it, train_loss=0.0665]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:07,  1.21s/it, train_loss=0.0665]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:30<00:07,  1.21s/it, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:06,  1.23s/it, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:31<00:06,  1.23s/it, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.15s/it, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.15s/it, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.14s/it, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.14s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.19s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.19s/it, train_loss=0.209] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.18s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:35<00:01,  1.18s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:35<00:00,  1.06it/s, train_loss=0.0238]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 average loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 43/100 [23:20<32:13, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 43 current AUC: 0.9888 current accuracy: 0.8447 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.02s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.02s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.11s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.11s/it, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.139] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.03s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.03s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.171] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.04s/it, train_loss=0.0646]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.0646]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=0.105] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.14s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.0733]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0733]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.02]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.153]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.0592]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.0592]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.10s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.08s/it, train_loss=0.05]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.05]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.15s/it, train_loss=0.0755]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.14s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.10s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.10s/it, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.09s/it, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.09s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.13s/it, train_loss=0.0648]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.13s/it, train_loss=0.172] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.172]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.115] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.06s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.16s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.16s/it, train_loss=0.119]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.16s/it, train_loss=0.119]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.16s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.23s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.23s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.19s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.19s/it, train_loss=0.125] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.13s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.06s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.06s/it, train_loss=0.0526]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.19it/s, train_loss=0.0526]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 average loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|████▍     | 44/100 [23:54<31:40, 33.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 44 current AUC: 0.9973 current accuracy: 0.9130 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0761]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.28s/it, train_loss=0.0761]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.28s/it, train_loss=0.0943]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.23s/it, train_loss=0.0943]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.23s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.18s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.18s/it, train_loss=0.162] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.21s/it, train_loss=0.162]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.21s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.19s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.12s/it, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.04s/it, train_loss=0.202] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.202]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.01s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.03s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.01s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.04s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.04s/it, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.00it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:17,  1.00it/s, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.08s/it, train_loss=0.154] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.06s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.06s/it, train_loss=0.102] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.09s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.09s/it, train_loss=0.31]  \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.31]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.12s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.11s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.11s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.04s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.0496]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.0496]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.01s/it, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.02s/it, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.02s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.09s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.09s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.08s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.08s/it, train_loss=0.109] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.10s/it, train_loss=0.109]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.10s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.10s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.10s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.11it/s, train_loss=0.0155]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 average loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 45/100 [24:27<31:03, 33.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 45 current AUC: 0.9897 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.09it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.09it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.00it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.00it/s, train_loss=0.196] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.00it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.00it/s, train_loss=0.05] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.01it/s, train_loss=0.05]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.03it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.03it/s, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.08s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.08s/it, train_loss=0.157] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.12s/it, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.06s/it, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.06s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.0894]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.00it/s, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.00it/s, train_loss=0.022] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.04it/s, train_loss=0.022]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.04it/s, train_loss=0.061]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.00it/s, train_loss=0.061]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.00it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.03s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.03s/it, train_loss=0.0353]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.10s/it, train_loss=0.0353]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.10s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.02s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.02s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.05it/s, train_loss=0.191] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.00s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.00s/it, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.03it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.03it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.01it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.01it/s, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.05s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.05s/it, train_loss=0.00912]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.07s/it, train_loss=0.00912]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.0455] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.02s/it, train_loss=0.0455]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.02s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.06s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.152] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.06s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.06s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.03s/it, train_loss=0.165]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.03s/it, train_loss=0.0581]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.0581]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.01s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.01it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.01it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.21it/s, train_loss=0.0206]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 average loss: 0.0648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|████▌     | 46/100 [24:59<29:51, 33.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 46 current AUC: 0.9926 current accuracy: 0.8882 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.11s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.11s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.13s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.13s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.06s/it, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.06s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.205] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.15s/it, train_loss=0.205]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.15s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.19s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.19s/it, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.16s/it, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.16s/it, train_loss=0.0232]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.20s/it, train_loss=0.0232]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.20s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.21s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.21s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.06s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.06s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.11s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.10s/it, train_loss=0.00785]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.00785]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.04s/it, train_loss=0.0467] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.01it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.01it/s, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.05it/s, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:14,  1.05it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.07it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:13,  1.07it/s, train_loss=0.145] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.04it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:12,  1.04it/s, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.05s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.05s/it, train_loss=0.064] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.16s/it, train_loss=0.064]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.16s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.06s/it, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.05s/it, train_loss=0.057]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.05s/it, train_loss=0.057]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.01s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.01s/it, train_loss=0.124]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.07s/it, train_loss=0.188] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.12s/it, train_loss=0.188]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.12s/it, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.16s/it, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.12]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.13it/s, train_loss=0.288]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 average loss: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 47/100 [25:32<29:24, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 47 current AUC: 0.9905 current accuracy: 0.8944 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.08it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.08it/s, train_loss=0.321] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.321]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.02it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.01s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.02it/s, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.02it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.03it/s, train_loss=0.0405]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.03it/s, train_loss=0.044] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.02it/s, train_loss=0.044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.02it/s, train_loss=0.0987]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:21,  1.07it/s, train_loss=0.0987]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:21,  1.07it/s, train_loss=0.023] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:20,  1.10it/s, train_loss=0.023]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:20,  1.10it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:19,  1.09it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:19,  1.09it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.00it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.00it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.01it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.01it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:17,  1.02it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.02it/s, train_loss=0.126] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.01it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.01it/s, train_loss=0.042]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.05s/it, train_loss=0.042]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.05s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.13s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.14s/it, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.0684]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:15,  1.16s/it, train_loss=0.0684]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.16s/it, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.16s/it, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.16s/it, train_loss=0.00733]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:13,  1.19s/it, train_loss=0.00733]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.19s/it, train_loss=0.0192] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:11,  1.12s/it, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.10s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.10s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.08s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.08s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.13s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.13s/it, train_loss=0.102] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.06s/it, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.01s/it, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.147] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.03s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.03s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.03s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.193] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.08s/it, train_loss=0.193]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.08s/it, train_loss=0.186]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.14it/s, train_loss=0.186]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 average loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|████▊     | 48/100 [26:05<28:37, 33.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 48 current AUC: 0.9960 current accuracy: 0.9255 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0287]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.10s/it, train_loss=0.0287]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.10s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.17s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.17s/it, train_loss=0.107] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.0295]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.0295]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.13s/it, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.18s/it, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.18s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.22s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.22s/it, train_loss=0.104] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.19s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.19s/it, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:26,  1.25s/it, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:26,  1.25s/it, train_loss=0.2]   \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.18s/it, train_loss=0.2]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.18s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.13s/it, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.06s/it, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.06s/it, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:17,  1.05s/it, train_loss=0.154] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.154]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.15s/it, train_loss=0.0683]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.0683]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.15s/it, train_loss=0.103] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.18s/it, train_loss=0.103]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.18s/it, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:15,  1.17s/it, train_loss=0.149] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.21s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.21s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.18s/it, train_loss=0.147]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.18s/it, train_loss=0.069]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.17s/it, train_loss=0.069]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.17s/it, train_loss=0.039]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.039]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.12s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.14s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.14s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.14s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.14s/it, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.10s/it, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.07s/it, train_loss=0.028]  \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.04s/it, train_loss=0.028]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.04s/it, train_loss=0.0942]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.0942]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.03s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.02s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.02s/it, train_loss=0.153] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.21it/s, train_loss=0.153]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 average loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  49%|████▉     | 49/100 [26:40<28:31, 33.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 49 current AUC: 0.9959 current accuracy: 0.9255 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00513]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.00513]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.0738] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.13s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.13s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.05s/it, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.05s/it, train_loss=0.078] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.078]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.09s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.09s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.203] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.203]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.03s/it, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.03s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.02s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.01s/it, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.01s/it, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.06s/it, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.157] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.08s/it, train_loss=0.157]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.08s/it, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.08s/it, train_loss=0.0223] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.07s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.13s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.13s/it, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.07s/it, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.07s/it, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.10s/it, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.129] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.07s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.08s/it, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.08s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.08s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.08s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.125] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.09it/s, train_loss=0.125]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 average loss: 0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 50/100 [27:13<27:57, 33.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 50 current AUC: 0.9957 current accuracy: 0.9565 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00706]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.18s/it, train_loss=0.00706]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.18s/it, train_loss=0.062]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:37,  1.30s/it, train_loss=0.062]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:37,  1.30s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.20s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.20s/it, train_loss=0.034]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.20s/it, train_loss=0.034]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.20s/it, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.18s/it, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.18s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.21s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:30,  1.21s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.13s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.17s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.17s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.16s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.16s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.16s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.16s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:22,  1.10s/it, train_loss=0.00493]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.00493]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.17s/it, train_loss=0.0721] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.15s/it, train_loss=0.0721]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.15s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.13s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.13s/it, train_loss=0.151] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.13s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.13s/it, train_loss=0.092]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.10s/it, train_loss=0.092]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.10s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.13s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.20s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:15,  1.20s/it, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.21s/it, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.21s/it, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.14s/it, train_loss=0.0488]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.14s/it, train_loss=0.177] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.09s/it, train_loss=0.177]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.09s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.13s/it, train_loss=0.209]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.13s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.15s/it, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.15s/it, train_loss=0.015] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.06s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.06s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.06s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.06s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.06s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.06s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.04s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.04s/it, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.00s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.00s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.22it/s, train_loss=0.017]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 average loss: 0.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  51%|█████     | 51/100 [27:48<27:41, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 51 current AUC: 0.9919 current accuracy: 0.9006 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00721]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.09it/s, train_loss=0.00721]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.09it/s, train_loss=0.153]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.17it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.17it/s, train_loss=0.077]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.12it/s, train_loss=0.077]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.12it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.11it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.11it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:23,  1.06it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:23,  1.06it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.04it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.04it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:23,  1.03s/it, train_loss=0.117]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.02s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.108] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:22,  1.07s/it, train_loss=0.108]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.07s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.14s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.14s/it, train_loss=0.0745]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.12s/it, train_loss=0.0745]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.05s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.05s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:17,  1.00s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.00s/it, train_loss=0.0664] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.00s/it, train_loss=0.0664]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.00s/it, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.02it/s, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.02it/s, train_loss=0.0225] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:13,  1.03it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.03it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:12,  1.07it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.07it/s, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:18<00:11,  1.04it/s, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.04it/s, train_loss=0.0645]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:19<00:11,  1.01s/it, train_loss=0.0645]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.05s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.05s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:21<00:09,  1.02s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.02s/it, train_loss=0.101] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.06s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.06s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.12s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.12s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.11s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.11s/it, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.10s/it, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.10s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.08s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.08s/it, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.14s/it, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.14s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.17s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.17s/it, train_loss=0.0963]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.19s/it, train_loss=0.0963]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.19s/it, train_loss=0.0214]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.06it/s, train_loss=0.0214]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 average loss: 0.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|█████▏    | 52/100 [28:20<26:46, 33.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 52 current AUC: 0.9947 current accuracy: 0.9441 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0201]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.01it/s, train_loss=0.0201]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.01it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.08s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.08s/it, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.15s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.15s/it, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.16s/it, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.16s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.18s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.18s/it, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.24s/it, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.24s/it, train_loss=0.00378]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.00378]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.0145] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.18s/it, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.18s/it, train_loss=0.288] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.288]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.20s/it, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.20s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.07s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:18,  1.09s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.10s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:17,  1.10s/it, train_loss=0.0822]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.07s/it, train_loss=0.0822]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.07s/it, train_loss=0.0037]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.0037]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.09s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.11s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.01s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.01s/it, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:10,  1.00it/s, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:10,  1.00it/s, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.01s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.01s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.02s/it, train_loss=0.12] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.00it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:07,  1.00it/s, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.11s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.05s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.05s/it, train_loss=0.074] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.074]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.01s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.03s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.00701]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.01s/it, train_loss=0.00701]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.01s/it, train_loss=0.038]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.038]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.30it/s, train_loss=0.0297]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 average loss: 0.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 53/100 [28:54<26:10, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 53 current AUC: 0.9979 current accuracy: 0.9130 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.00886]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.05it/s, train_loss=0.00886]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.05it/s, train_loss=0.146]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.01s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.03s/it, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.125] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.04s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:24,  1.01s/it, train_loss=0.141]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.01s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.02s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.02s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.04s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.07s/it, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.07s/it, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.00it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.07s/it, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.0622]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.05s/it, train_loss=0.0622]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.03s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.03s/it, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.00it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.00it/s, train_loss=0.076]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.02it/s, train_loss=0.076]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.02it/s, train_loss=0.00481]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.05s/it, train_loss=0.00481]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.05s/it, train_loss=0.0881] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.02s/it, train_loss=0.0881]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.02s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.04it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.05it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.05it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:08,  1.03it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:08,  1.03it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.03it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:07,  1.03it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:23<00:06,  1.05it/s, train_loss=0.0766]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.05it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:24<00:05,  1.05it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.05it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:25<00:04,  1.05it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.05it/s, train_loss=0.234] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:26<00:03,  1.07it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.07it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:27<00:02,  1.03it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.03it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:28<00:02,  1.01s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:02,  1.01s/it, train_loss=0.165] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:29<00:00,  1.01it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.01it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.25it/s, train_loss=0.132]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 average loss: 0.0631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|█████▍    | 54/100 [29:25<25:02, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 54 current AUC: 0.9968 current accuracy: 0.8944 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0214]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.0214]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.03it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:29,  1.00s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:29,  1.00s/it, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.01it/s, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.01it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.01it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.01it/s, train_loss=0.044] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.00it/s, train_loss=0.044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.00it/s, train_loss=0.0611]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:22,  1.04it/s, train_loss=0.0611]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.04it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:21,  1.02it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.02it/s, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.06s/it, train_loss=0.0564]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.06s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:23,  1.15s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.15s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.11s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.14s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.14s/it, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.09s/it, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.07s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.07s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.07s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.07s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.09s/it, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.14s/it, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.00458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.00458]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0238] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.09s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.09s/it, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.08s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.08s/it, train_loss=0.0329] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.07s/it, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.07s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.00s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.00s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.04s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.04s/it, train_loss=0.0076]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.00s/it, train_loss=0.0076]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.00s/it, train_loss=0.104] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.02s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.02s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.23it/s, train_loss=0.0123]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 average loss: 0.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 55/100 [29:57<24:32, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 55 current AUC: 0.9950 current accuracy: 0.9379 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.14it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.14it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.05it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.05it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.01it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.01it/s, train_loss=0.00676]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.07it/s, train_loss=0.00676]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.07it/s, train_loss=0.0556] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.03s/it, train_loss=0.0556]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.06s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.06s/it, train_loss=0.0157] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.0903]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.09s/it, train_loss=0.0903]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.0647]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.09s/it, train_loss=0.0647]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.00751]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:20,  1.14s/it, train_loss=0.00751]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.0166] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.18s/it, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.18s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:19,  1.22s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.22s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:18,  1.20s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.20s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.15s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.15s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.12s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.12s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.17s/it, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.17s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.12s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.14s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.14s/it, train_loss=0.066] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.10s/it, train_loss=0.066]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0872]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.12s/it, train_loss=0.0872]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.12s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.11s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.06s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.06s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.07s/it, train_loss=0.00412]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.10s/it, train_loss=0.00412]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.10s/it, train_loss=0.168]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.08s/it, train_loss=0.168]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.08s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.048] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.08it/s, train_loss=0.048]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 average loss: 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|█████▌    | 56/100 [30:32<24:19, 33.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 56 current AUC: 0.9947 current accuracy: 0.9255 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00616]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.00616]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.05it/s, train_loss=0.0816] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.10s/it, train_loss=0.0816]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.10s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.12s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.12s/it, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.00888]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0177] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.19s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.16s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.16s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.00905]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.00905]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.10s/it, train_loss=0.046]  \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.046]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.14s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.11s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.14s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.14s/it, train_loss=0.0148] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.10s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.10s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.04s/it, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.10s/it, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.10s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.02s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.02s/it, train_loss=0.14]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.03s/it, train_loss=0.14]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.03s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.13s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.05s/it, train_loss=0.0777]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.05s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.00s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.00s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.00466]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.00466]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0838] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.0838]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.09s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.09s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.04s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.04s/it, train_loss=0.0197] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.03it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.03it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.00508]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.00508]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.0146] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.16it/s, train_loss=0.0146]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 average loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  57%|█████▋    | 57/100 [31:05<23:53, 33.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 57 current AUC: 0.9902 current accuracy: 0.9130 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00845]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.18s/it, train_loss=0.00845]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.18s/it, train_loss=0.0225] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.059] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.059]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.13s/it, train_loss=0.329]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.13s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.00484]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.14s/it, train_loss=0.00484]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.14s/it, train_loss=0.0402] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.21s/it, train_loss=0.0402]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.21s/it, train_loss=0.148] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.11s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.11s/it, train_loss=0.0774]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.07s/it, train_loss=0.0774]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.07s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.11s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.049] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.049]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.07s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.14s/it, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.20s/it, train_loss=0.0791]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.20s/it, train_loss=0.065] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.20s/it, train_loss=0.065]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.20s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.19s/it, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.19s/it, train_loss=0.0545]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.16s/it, train_loss=0.0545]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.16s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.18s/it, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.0829]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=0.0829]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.13s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.09s/it, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.10s/it, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.10s/it, train_loss=0.0715] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.10s/it, train_loss=0.0715]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.275] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.08s/it, train_loss=0.275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.08s/it, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.08s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.17s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.17s/it, train_loss=0.00759]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.19s/it, train_loss=0.00759]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.19s/it, train_loss=0.148]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.17s/it, train_loss=0.148]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.17s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.20s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.20s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.0215]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.03it/s, train_loss=0.0215]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 average loss: 0.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  58%|█████▊    | 58/100 [31:41<23:47, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 58 current AUC: 0.9911 current accuracy: 0.8820 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.03it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.02it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.02it/s, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.02it/s, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.02it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.04s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.04s/it, train_loss=0.032] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.09s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.12s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.12s/it, train_loss=0.0861]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.13s/it, train_loss=0.0861]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.00734]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:27,  1.18s/it, train_loss=0.00734]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.18s/it, train_loss=0.0126] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.08s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.08s/it, train_loss=0.013] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.013]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.174]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.07it/s, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:17,  1.07it/s, train_loss=0.238] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:17,  1.01it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:17,  1.01it/s, train_loss=0.06] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.04s/it, train_loss=0.06]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:18,  1.13s/it, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.13s/it, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.06s/it, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.0663]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.09s/it, train_loss=0.0663]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.09s/it, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.12s/it, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.12s/it, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.08s/it, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.08s/it, train_loss=0.0401] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.00s/it, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.00s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.03s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.03s/it, train_loss=0.0402]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.03s/it, train_loss=0.0402]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.03s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.01s/it, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.04s/it, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.04s/it, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.16]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.00928]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.10s/it, train_loss=0.00928]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.10s/it, train_loss=0.0608] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.07s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.07s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.07s/it, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.006] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.06s/it, train_loss=0.006]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.04s/it, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.04s/it, train_loss=0.015] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.22it/s, train_loss=0.015]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59 average loss: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  59%|█████▉    | 59/100 [32:14<22:56, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 59 current AUC: 0.9934 current accuracy: 0.8509 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.05it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.07it/s, train_loss=0.0174] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.05s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.05s/it, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.19s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.19s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:24,  1.03it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.03it/s, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.02s/it, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.02s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.06s/it, train_loss=0.0178]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.06s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.09s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.09s/it, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.14s/it, train_loss=0.06]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.14s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.14s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.14s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.16s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.16s/it, train_loss=0.122] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.11s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.11s/it, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.07s/it, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.07s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.04s/it, train_loss=0.00692]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.04s/it, train_loss=0.014]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.01it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.01it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.03it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.03it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.01it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.01it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.02it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.02it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.01s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.01s/it, train_loss=0.00869]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.00s/it, train_loss=0.00869]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.00s/it, train_loss=0.0201] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.01s/it, train_loss=0.0201]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.01s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.02s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.02s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:05,  1.02it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.02it/s, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:04,  1.09it/s, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.09it/s, train_loss=0.0169] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:03,  1.00it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.00it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.02it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.13]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.18s/it, train_loss=0.13]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.18s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.09s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.09s/it, train_loss=0.0644]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.15it/s, train_loss=0.0644]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 average loss: 0.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 60/100 [32:46<22:08, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 60 current AUC: 0.9929 current accuracy: 0.9193 best AUC: 0.9980 at epoch: 19\n",
      "----------\n",
      "epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.08it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.08it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:25,  1.13it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:25,  1.13it/s, train_loss=0.083] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.01it/s, train_loss=0.083]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.01it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.05s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.05s/it, train_loss=0.00469]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.00469]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.0908] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.08s/it, train_loss=0.0908]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.08s/it, train_loss=0.0768]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.0768]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.03s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.0596] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.09s/it, train_loss=0.0596]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.057] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.03s/it, train_loss=0.057]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.03s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.05s/it, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.0899]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.00s/it, train_loss=0.0899]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.00s/it, train_loss=0.00812]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.00812]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.0732] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.07s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.07s/it, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.12s/it, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.12s/it, train_loss=0.00549]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.16s/it, train_loss=0.00549]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.16s/it, train_loss=0.0196] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.19s/it, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:14,  1.19s/it, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.19s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.11s/it, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.11s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.05s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.05s/it, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.03s/it, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.0788]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.02it/s, train_loss=0.0788]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.02it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.03it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.03it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.03it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.00512]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.03it/s, train_loss=0.00512]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.03it/s, train_loss=0.0264] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.02s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.07s/it, train_loss=0.0254]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.07s/it, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.12s/it, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.12s/it, train_loss=0.012] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.13it/s, train_loss=0.012]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61 average loss: 0.0454\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  61%|██████    | 61/100 [33:20<21:47, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 61 current AUC: 0.9984 current accuracy: 0.9565 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.12s/it, train_loss=0.228]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.12s/it, train_loss=0.0853]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.07it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.07s/it, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.07s/it, train_loss=0.0286] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:31,  1.23s/it, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:31,  1.23s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.23s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:30,  1.23s/it, train_loss=0.149] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.22s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:29,  1.22s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.26s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.26s/it, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:25,  1.20s/it, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:25,  1.20s/it, train_loss=0.0809]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.08s/it, train_loss=0.0809]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.08s/it, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.01s/it, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.01s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.07s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.07s/it, train_loss=0.015] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.02s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.02s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.08s/it, train_loss=0.116] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.116]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.08s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.11s/it, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.0414] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.15s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.19s/it, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.19s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.16s/it, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.16s/it, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0873]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.07s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.12s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.12s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.19s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.19s/it, train_loss=0.104] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.19s/it, train_loss=0.104]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.19s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.18s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.18s/it, train_loss=0.0911]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.0911]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.20s/it, train_loss=0.223] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.02it/s, train_loss=0.223]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62 average loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 62/100 [33:56<21:36, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 62 current AUC: 0.9932 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.13s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.13s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.05it/s, train_loss=0.0218]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:27,  1.05it/s, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.00s/it, train_loss=0.0315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.00s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.10s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:30,  1.22s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.22s/it, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.18s/it, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.18s/it, train_loss=0.0476]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:28,  1.24s/it, train_loss=0.0476]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:28,  1.24s/it, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.16s/it, train_loss=0.0523]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.16s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.08s/it, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.08s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.101] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.11s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.11s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.17s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.17s/it, train_loss=0.036] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.16s/it, train_loss=0.036]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.16s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.17s/it, train_loss=0.15]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.15]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.14s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.16s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.16s/it, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.21s/it, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.21s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.19s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.19s/it, train_loss=0.0308] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.28s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:11,  1.28s/it, train_loss=0.058] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:10,  1.28s/it, train_loss=0.058]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:10,  1.28s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.26s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:29<00:08,  1.26s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:07,  1.19s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:30<00:07,  1.19s/it, train_loss=0.225] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.15s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:31<00:05,  1.15s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.16s/it, train_loss=0.105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.16s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.16s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.16s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.17s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.17s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:35<00:01,  1.15s/it, train_loss=0.00876]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:35<00:00,  1.00s/it, train_loss=0.00876]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63 average loss: 0.0522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  63%|██████▎   | 63/100 [34:32<21:25, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 63 current AUC: 0.9955 current accuracy: 0.9317 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.25s/it, train_loss=0.063]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.25s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.05s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.05s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:34,  1.22s/it, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:34,  1.22s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.20s/it, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.20s/it, train_loss=0.0383]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.17s/it, train_loss=0.0383]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.17s/it, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.21]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.02s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.02s/it, train_loss=0.022] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.022]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.01s/it, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.03it/s, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:20,  1.03it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.10s/it, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.1]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.08s/it, train_loss=0.109] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.109]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0459]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.06s/it, train_loss=0.0459]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.00s/it, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.00s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.03s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.03s/it, train_loss=0.139] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.06s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.0742] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.02s/it, train_loss=0.0742]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.01s/it, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.00it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.00it/s, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.01s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.01s/it, train_loss=0.00536]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.02it/s, train_loss=0.00536]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.02it/s, train_loss=0.0671] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.00s/it, train_loss=0.0671]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.00s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.02s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.05s/it, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.05s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.08s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.187] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.187]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 average loss: 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  64%|██████▍   | 64/100 [35:05<20:32, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 64 current AUC: 0.9958 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.10s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.10s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.19s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.19s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.12s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.12s/it, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.07s/it, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.07s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.08s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.08s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.14s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.14s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.15s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.15s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.14s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.14s/it, train_loss=0.0706]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.0706]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.11s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.15s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.15s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.15s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.15s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.00891]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.10s/it, train_loss=0.00891]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.10s/it, train_loss=0.0231] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.12s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.12s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.13s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.13s/it, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.113] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.12s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.07s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.04s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.02s/it, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.02s/it, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.02it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:06,  1.02it/s, train_loss=0.063] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.04it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.04it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.00s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.00s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.01it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.01it/s, train_loss=0.0062]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.04it/s, train_loss=0.0062]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.04it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.03s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.00436]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.00436]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 average loss: 0.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 65/100 [35:39<19:53, 34.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 65 current AUC: 0.9928 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:33,  1.13s/it, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:33,  1.13s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.12s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.12s/it, train_loss=0.127] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.02it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.02it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.03it/s, train_loss=0.0727]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.03it/s, train_loss=0.013] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.013]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.02s/it, train_loss=0.073]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.073]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.13s/it, train_loss=0.187]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.13s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.14s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.0134] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.13s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.063] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.06s/it, train_loss=0.063]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.06s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.037] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.00s/it, train_loss=0.037]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.00s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.00it/s, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.00it/s, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:16,  1.02s/it, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.02s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.02it/s, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.02it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.01s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.01s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.05s/it, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.05s/it, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.21s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.21s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.11s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.11s/it, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.10s/it, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.10s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.04s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.04s/it, train_loss=0.0814]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.08s/it, train_loss=0.0814]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.08s/it, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.18s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.146] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.13s/it, train_loss=0.146]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.13s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.07s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.07s/it, train_loss=0.00485]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.00485]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.107]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.01s/it, train_loss=0.107]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.01s/it, train_loss=0.00786]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.23it/s, train_loss=0.00786]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66 average loss: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  66%|██████▌   | 66/100 [36:12<19:08, 33.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 66 current AUC: 0.9964 current accuracy: 0.9317 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.15s/it, train_loss=0.011]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.15s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.0736]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.08s/it, train_loss=0.0736]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.08s/it, train_loss=0.0689]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0689]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.01s/it, train_loss=0.00802]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.04s/it, train_loss=0.00802]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.04s/it, train_loss=0.0767] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0136] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.06s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.06s/it, train_loss=0.269] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.05s/it, train_loss=0.269]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.10s/it, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.10s/it, train_loss=0.00498]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.10s/it, train_loss=0.00498]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.10s/it, train_loss=0.015]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.00989]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.07s/it, train_loss=0.00989]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.07s/it, train_loss=0.0473] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.12s/it, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.12s/it, train_loss=0.024] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.11s/it, train_loss=0.024]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.11s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.13s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.077] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.19s/it, train_loss=0.077]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.16s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.16s/it, train_loss=0.0158] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.21s/it, train_loss=0.0158]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.21s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.18s/it, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.18s/it, train_loss=0.0043]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.15s/it, train_loss=0.0043]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.00639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.24s/it, train_loss=0.00639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.24s/it, train_loss=0.069]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.22s/it, train_loss=0.069]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.22s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.14s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.14s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.06s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.13s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.13s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.14s/it, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.14s/it, train_loss=0.0574]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.17s/it, train_loss=0.0574]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.17s/it, train_loss=0.811] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.09it/s, train_loss=0.811]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 average loss: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 67/100 [36:47<18:44, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 67 current AUC: 0.9960 current accuracy: 0.9255 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.05s/it, train_loss=0.0978]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:31,  1.05s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.14s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.14s/it, train_loss=0.00542]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.20s/it, train_loss=0.00542]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.20s/it, train_loss=0.00306]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.00306]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.157]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.03s/it, train_loss=0.157]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.12s/it, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.12s/it, train_loss=0.23]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.16s/it, train_loss=0.23]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.04s/it, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.03]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.03]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.13s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.13s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.052] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.16s/it, train_loss=0.052]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.16s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.133]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.111]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.11s/it, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.11s/it, train_loss=0.155] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.15] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.15]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.10s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.10s/it, train_loss=0.137] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.03s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.03s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.00s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.00s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.02s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.0937]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.00s/it, train_loss=0.0937]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.00s/it, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.00it/s, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.00it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.06it/s, train_loss=0.0992]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.06it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.09it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.09it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.08it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.08it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.06it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.06it/s, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.04it/s, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.04it/s, train_loss=0.0851]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.06s/it, train_loss=0.0851]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.148] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.148]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 average loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  68%|██████▊   | 68/100 [37:20<18:00, 33.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 68 current AUC: 0.9966 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0852]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.19s/it, train_loss=0.0852]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.19s/it, train_loss=0.128] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:36,  1.25s/it, train_loss=0.128]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:36,  1.25s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:33,  1.19s/it, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:33,  1.19s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.17s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.17s/it, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:32,  1.29s/it, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:32,  1.29s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.13s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.09s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.09s/it, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.06s/it, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.06s/it, train_loss=0.169] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.169]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.01s/it, train_loss=0.049]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.01s/it, train_loss=0.049]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.01s/it, train_loss=0.0571]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.07s/it, train_loss=0.0571]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.07s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.01it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.01it/s, train_loss=0.0468]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.09s/it, train_loss=0.0468]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.00855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.14s/it, train_loss=0.00855]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.14s/it, train_loss=0.0186] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.10s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.10s/it, train_loss=0.143] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.09s/it, train_loss=0.143]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.09s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.09s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.07s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0608]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.10s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.11s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.09s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.09s/it, train_loss=0.011] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.011]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.02s/it, train_loss=0.113]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.02s/it, train_loss=0.00862]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.08s/it, train_loss=0.00862]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.08s/it, train_loss=0.132]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.04s/it, train_loss=0.132]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.04s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.0174]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 average loss: 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  69%|██████▉   | 69/100 [37:53<17:25, 33.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 69 current AUC: 0.9944 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00778]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.03s/it, train_loss=0.0361] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.12s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.12s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.11s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.11s/it, train_loss=0.0104] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.12s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.12s/it, train_loss=0.00983]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.00983]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.00881]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.0419] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.08s/it, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.08s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.12s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.12s/it, train_loss=0.0527] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.17s/it, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.17s/it, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.031] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.13s/it, train_loss=0.031]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.13s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.05s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.05s/it, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.09s/it, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.05s/it, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.05s/it, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.06s/it, train_loss=0.0607]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.03s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.02it/s, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:12,  1.02it/s, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.04it/s, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:11,  1.04it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.04it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:10,  1.04it/s, train_loss=0.0499] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.05it/s, train_loss=0.0499]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:09,  1.05it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.01s/it, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.01s/it, train_loss=0.0637]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:09,  1.14s/it, train_loss=0.0637]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.14s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.112]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.07s/it, train_loss=0.037]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.09s/it, train_loss=0.037]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.12s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.0342] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.12s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.14s/it, train_loss=0.0973]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.14s/it, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.19s/it, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.19s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0654]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.0654]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70 average loss: 0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 70/100 [38:27<16:50, 33.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 70 current AUC: 0.9938 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.07it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.07it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.02s/it, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.02s/it, train_loss=0.00944]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.08s/it, train_loss=0.00944]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.08s/it, train_loss=0.0132] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.0941]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.0941]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.04s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.04s/it, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.03it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.03it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:21,  1.02it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:21,  1.02it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.03s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.03s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.09s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.08s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.01s/it, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.02it/s, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:16,  1.02it/s, train_loss=0.00614]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.05it/s, train_loss=0.00614]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.05it/s, train_loss=0.0206] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.02s/it, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.02s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.08s/it, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.08s/it, train_loss=0.00745]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.10s/it, train_loss=0.00745]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.10s/it, train_loss=0.0141] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:12,  1.13s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.13s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:10,  1.06s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.04s/it, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.04s/it, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.07s/it, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.07s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.06s/it, train_loss=0.0704]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.0348]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.07s/it, train_loss=0.0348]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:06,  1.20s/it, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:06,  1.20s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.20s/it, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.20s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.15s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.15s/it, train_loss=0.00583]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.15s/it, train_loss=0.00583]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.0426] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.07it/s, train_loss=0.0151]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71 average loss: 0.0347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  71%|███████   | 71/100 [39:00<16:14, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 71 current AUC: 0.9972 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00556]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.00556]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.05it/s, train_loss=0.0427] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.04s/it, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:28,  1.01s/it, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0437]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.02s/it, train_loss=0.0437]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.02s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.02s/it, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.02s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.14s/it, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.14s/it, train_loss=0.0079]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.13s/it, train_loss=0.0079]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.13s/it, train_loss=0.155] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:25,  1.16s/it, train_loss=0.155]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.16s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.00662]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.18s/it, train_loss=0.00662]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.18s/it, train_loss=0.00652]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.18s/it, train_loss=0.00652]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.18s/it, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.21s/it, train_loss=0.115]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.21s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.19s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.19s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.21s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.21s/it, train_loss=0.0165] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.23s/it, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.23s/it, train_loss=0.00323]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.25s/it, train_loss=0.00323]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.25s/it, train_loss=0.114]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:16,  1.26s/it, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:16,  1.26s/it, train_loss=0.00472]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.00472]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.0382] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:12,  1.15s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.14s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.14s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.15s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.16s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.16s/it, train_loss=0.246] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.07s/it, train_loss=0.246]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.07s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.09s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.09s/it, train_loss=0.0395] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.13s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.13s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.18s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.18s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.19s/it, train_loss=0.0483]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.19s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.16s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.17s/it, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.17s/it, train_loss=0.00441]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.07it/s, train_loss=0.00441]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72 average loss: 0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  72%|███████▏  | 72/100 [39:36<15:56, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 72 current AUC: 0.9923 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00498]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:32,  1.09s/it, train_loss=0.00498]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:32,  1.09s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.13s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.13s/it, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.013]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.10s/it, train_loss=0.013]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.10s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.10s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.10s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.11s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.11s/it, train_loss=0.00464]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.12s/it, train_loss=0.00464]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0139] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:23,  1.09s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.0562]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.00595]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.10s/it, train_loss=0.00595]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.10s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.13s/it, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.00819]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.00819]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.0129] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.18s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.18s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.22s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.22s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.09s/it, train_loss=0.0697]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.09s/it, train_loss=0.185] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.06s/it, train_loss=0.185]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:12,  1.06s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.06s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.06s/it, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.09s/it, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.09s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.02s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.02s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.05s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.13s/it, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.13s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.09s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.09s/it, train_loss=0.00505]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.11s/it, train_loss=0.00505]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0411] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.03s/it, train_loss=0.0411]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.03s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.11s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.12s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.12s/it, train_loss=0.0398] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.11s/it, train_loss=0.0364]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.12it/s, train_loss=0.0364]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 average loss: 0.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  73%|███████▎  | 73/100 [40:10<15:24, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 73 current AUC: 0.9955 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0743]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.15s/it, train_loss=0.0743]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.15s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.22s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.22s/it, train_loss=0.00322]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.12s/it, train_loss=0.00322]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.12s/it, train_loss=0.00296]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.19s/it, train_loss=0.00296]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:32,  1.19s/it, train_loss=0.0062] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.18s/it, train_loss=0.0062]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.18s/it, train_loss=0.029] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.18s/it, train_loss=0.029]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.18s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.12s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.02s/it, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.02s/it, train_loss=0.00207]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.01s/it, train_loss=0.00207]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.01s/it, train_loss=0.1]    \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.06s/it, train_loss=0.1]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.06s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.05s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.0484]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:19,  1.05s/it, train_loss=0.088] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.088]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.02s/it, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.04s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.05s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.05s/it, train_loss=0.00702]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.09s/it, train_loss=0.00702]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.0179] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.08s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.05s/it, train_loss=0.0308]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.05s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.00s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.00s/it, train_loss=0.0122] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.01s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.01s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.00s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.00s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.09s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.09s/it, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.12s/it, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.11s/it, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.11s/it, train_loss=0.121] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.10s/it, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.13s/it, train_loss=0.127]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.13s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.12s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.09s/it, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.09s/it, train_loss=0.00711]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.09s/it, train_loss=0.00711]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.09s/it, train_loss=0.0299] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.06s/it, train_loss=0.00788]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.00788]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74 average loss: 0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  74%|███████▍  | 74/100 [40:44<14:44, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 74 current AUC: 0.9948 current accuracy: 0.9565 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.07it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.07it/s, train_loss=0.0953]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.0953]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.01s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.01s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.05s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.05s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.08s/it, train_loss=0.0836]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.08s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.0529] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.16s/it, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:27,  1.18s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.18s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.20s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:26,  1.20s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.19s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.19s/it, train_loss=0.00239]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.17s/it, train_loss=0.00239]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.17s/it, train_loss=0.0168] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.14s/it, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.14s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.12s/it, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.12s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.14s/it, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.13s/it, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.13s/it, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.14s/it, train_loss=0.0379]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.14s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.00977]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.16s/it, train_loss=0.00977]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.16s/it, train_loss=0.0361] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.00325]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.00325]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.00573]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.08s/it, train_loss=0.00573]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.00366]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.06s/it, train_loss=0.00366]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.06s/it, train_loss=0.0141] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.03s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.00it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:06,  1.00it/s, train_loss=0.0072]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.01it/s, train_loss=0.0072]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.01it/s, train_loss=0.00804]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.00804]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:04,  1.03it/s, train_loss=0.0325] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.08it/s, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.08it/s, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.01s/it, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.01s/it, train_loss=0.0105] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.06s/it, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.06s/it, train_loss=0.0568]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.0568]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.08s/it, train_loss=0.0084]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.14it/s, train_loss=0.0084]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75 average loss: 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  75%|███████▌  | 75/100 [41:17<14:07, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 75 current AUC: 0.9959 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0574]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.0574]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:28,  1.05it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.11s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.11s/it, train_loss=0.00339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.00339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.0543] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:33,  1.23s/it, train_loss=0.0543]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:33,  1.23s/it, train_loss=0.00865]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:31,  1.21s/it, train_loss=0.00865]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:31,  1.21s/it, train_loss=0.00302]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.00302]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.00996]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.19s/it, train_loss=0.00996]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.19s/it, train_loss=0.00119]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.00119]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.00994]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.00994]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.13s/it, train_loss=0.011]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.08s/it, train_loss=0.011]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.08s/it, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.02s/it, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.02s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.01it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:18,  1.01it/s, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.03s/it, train_loss=0.00254]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.03s/it, train_loss=0.0192] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.03s/it, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.03s/it, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.00it/s, train_loss=0.00413]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:15,  1.00it/s, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.05s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.05s/it, train_loss=0.0147] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.06s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.06s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.03s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.03s/it, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.05s/it, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.05s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.0255]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.00529]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.00s/it, train_loss=0.00529]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.00s/it, train_loss=0.021]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.05s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.05s/it, train_loss=0.00939]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.01it/s, train_loss=0.00939]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:06,  1.01it/s, train_loss=0.0497] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.0497]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.027] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.03s/it, train_loss=0.027]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.03s/it, train_loss=0.00352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.01it/s, train_loss=0.00352]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.01it/s, train_loss=0.0117] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.04s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.04s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.02s/it, train_loss=0.00799]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.00204]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.01it/s, train_loss=0.00204]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.01it/s, train_loss=0.0265] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.24it/s, train_loss=0.0265]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76 average loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  76%|███████▌  | 76/100 [41:50<13:26, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 76 current AUC: 0.9969 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.02it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.02it/s, train_loss=0.00558]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:30,  1.07s/it, train_loss=0.00558]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:30,  1.07s/it, train_loss=0.00522]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.04s/it, train_loss=0.00522]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.04s/it, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.04it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.04it/s, train_loss=0.0032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:24,  1.04it/s, train_loss=0.0032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:24,  1.04it/s, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:24,  1.02it/s, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:24,  1.02it/s, train_loss=0.0287] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:24,  1.04s/it, train_loss=0.0287]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:24,  1.04s/it, train_loss=0.183] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.05s/it, train_loss=0.183]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.05s/it, train_loss=0.00422]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.04s/it, train_loss=0.00422]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.0477] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.02it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:19,  1.04it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.04it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:17,  1.07it/s, train_loss=0.00931]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:17,  1.07it/s, train_loss=0.00714]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:16,  1.09it/s, train_loss=0.00714]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.09it/s, train_loss=0.107]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:15,  1.08it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:15,  1.08it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:15,  1.03it/s, train_loss=0.0823]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.03it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:15<00:14,  1.01it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.00773]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:16<00:14,  1.02s/it, train_loss=0.00773]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.02s/it, train_loss=0.0123] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:17<00:13,  1.05s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.05s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.13s/it, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:12,  1.11s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.11s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:11,  1.15s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.15s/it, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:10,  1.19s/it, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.19s/it, train_loss=0.0357] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:09,  1.19s/it, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.19s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:08,  1.14s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.14s/it, train_loss=0.00335]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.12s/it, train_loss=0.00335]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.12s/it, train_loss=0.0101] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.11s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.11s/it, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.09s/it, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.09s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.16s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.16s/it, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.17s/it, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.17s/it, train_loss=0.00452]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.16s/it, train_loss=0.00452]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.16s/it, train_loss=0.00341]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.12it/s, train_loss=0.00341]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77 average loss: 0.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  77%|███████▋  | 77/100 [42:23<12:47, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 77 current AUC: 0.9976 current accuracy: 0.9503 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00391]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.00it/s, train_loss=0.00391]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:29,  1.00it/s, train_loss=0.00519]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.00519]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.0041] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.16s/it, train_loss=0.0041]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.16s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.00538]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.06s/it, train_loss=0.00538]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.06s/it, train_loss=0.0015] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.0015]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.03s/it, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.03s/it, train_loss=0.00172]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.05s/it, train_loss=0.00172]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.00251]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:20,  1.01it/s, train_loss=0.00251]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:20,  1.01it/s, train_loss=0.066]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:19,  1.04it/s, train_loss=0.066]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:19,  1.04it/s, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.05it/s, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:18,  1.05it/s, train_loss=0.00836]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:16,  1.08it/s, train_loss=0.00836]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:16,  1.08it/s, train_loss=0.014]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.00s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.00s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.07s/it, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.07s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:15,  1.04s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:15,  1.04s/it, train_loss=0.00394]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.10s/it, train_loss=0.00394]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.0501] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:15,  1.18s/it, train_loss=0.0501]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.18s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.15s/it, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:13,  1.18s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.18s/it, train_loss=0.00223]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.16s/it, train_loss=0.00223]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.16s/it, train_loss=0.00719]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:11,  1.23s/it, train_loss=0.00719]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.23s/it, train_loss=0.00449]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.20s/it, train_loss=0.00449]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.20s/it, train_loss=0.015]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:08,  1.19s/it, train_loss=0.015]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.19s/it, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:07,  1.20s/it, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.20s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.16s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.16s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.15s/it, train_loss=0.101]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.12s/it, train_loss=0.101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.12s/it, train_loss=0.0049]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.13s/it, train_loss=0.0049]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.09s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.09s/it, train_loss=0.00904]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.16it/s, train_loss=0.00904]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78 average loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  78%|███████▊  | 78/100 [42:57<12:18, 33.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 78 current AUC: 0.9954 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00194]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.14s/it, train_loss=0.00194]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.14s/it, train_loss=0.0331] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.21s/it, train_loss=0.0331]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.21s/it, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.00763]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:32,  1.22s/it, train_loss=0.00763]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:32,  1.22s/it, train_loss=0.225]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:33,  1.30s/it, train_loss=0.225]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:33,  1.30s/it, train_loss=0.0201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:29,  1.17s/it, train_loss=0.0201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.17s/it, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:25,  1.06s/it, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.05s/it, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.05s/it, train_loss=0.00659]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.04s/it, train_loss=0.00659]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:22,  1.04s/it, train_loss=0.00194]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.00194]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.00376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.18s/it, train_loss=0.00376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.18s/it, train_loss=0.0109] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.18s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.18s/it, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.15s/it, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.15s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.13s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.13s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.12s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:17,  1.12s/it, train_loss=0.00392]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:15,  1.06s/it, train_loss=0.00392]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:15,  1.06s/it, train_loss=0.0167] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.12s/it, train_loss=0.00247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.15s/it, train_loss=0.00247]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:14,  1.15s/it, train_loss=0.0265] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.25s/it, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:14,  1.25s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.24s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.24s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.24s/it, train_loss=0.035]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:12,  1.24s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:11,  1.25s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:11,  1.25s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.13s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.13s/it, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.05s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.05s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.00it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:05,  1.00it/s, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.08s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.08s/it, train_loss=0.0477] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.14s/it, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.14s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.27s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.27s/it, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.20s/it, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.20s/it, train_loss=0.00958]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.00958]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.00673]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.10it/s, train_loss=0.00673]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79 average loss: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  79%|███████▉  | 79/100 [43:32<11:57, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 79 current AUC: 0.9961 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.004]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.05it/s, train_loss=0.004]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.05it/s, train_loss=0.0029]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0029]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.00it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:27,  1.00it/s, train_loss=0.00305]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.09s/it, train_loss=0.00305]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.09s/it, train_loss=0.0142] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.0435]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.18s/it, train_loss=0.02]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.18s/it, train_loss=0.00876]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.00876]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.15s/it, train_loss=0.00206]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.00206]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.04s/it, train_loss=0.00448]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:21,  1.04s/it, train_loss=0.17]   \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.16s/it, train_loss=0.17]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.16s/it, train_loss=0.00713]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.00713]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.0211] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.14s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.00995]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.16s/it, train_loss=0.00995]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.16s/it, train_loss=0.00453]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:19,  1.19s/it, train_loss=0.00453]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:19,  1.19s/it, train_loss=0.0157] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.014] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.16s/it, train_loss=0.014]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.16s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.00999]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.0481] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.00932]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.19s/it, train_loss=0.00932]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:13,  1.19s/it, train_loss=0.0269] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.19s/it, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.19s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.20s/it, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.20s/it, train_loss=0.0297] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.19s/it, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.19s/it, train_loss=0.00643]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.22s/it, train_loss=0.00643]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.22s/it, train_loss=0.0391] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.15s/it, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.15s/it, train_loss=0.00495]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.10s/it, train_loss=0.00495]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.10s/it, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.12s/it, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.12s/it, train_loss=0.149]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.10s/it, train_loss=0.149]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.10s/it, train_loss=0.00828]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.07s/it, train_loss=0.00828]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.07s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.03s/it, train_loss=0.00404]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.03s/it, train_loss=0.00778]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.19it/s, train_loss=0.00778]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 average loss: 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 80/100 [44:07<11:27, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 80 current AUC: 0.9960 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00855]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.15s/it, train_loss=0.00855]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.15s/it, train_loss=0.0253] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.18s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.18s/it, train_loss=0.000811]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.15s/it, train_loss=0.000811]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.15s/it, train_loss=0.0749]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.15s/it, train_loss=0.0749]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.15s/it, train_loss=0.0035]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.15s/it, train_loss=0.0035]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.15s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.14s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.14s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.09s/it, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.09s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.09s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.09s/it, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.09s/it, train_loss=0.00269]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.00269]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.05s/it, train_loss=0.00638]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.00638]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.00231]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.11s/it, train_loss=0.00231]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.11s/it, train_loss=0.0479] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.09s/it, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.09s/it, train_loss=0.00764]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.00764]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0091] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.07s/it, train_loss=0.0091]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.07s/it, train_loss=0.00696]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.00696]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.10s/it, train_loss=0.0252] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.17s/it, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.17s/it, train_loss=0.071] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.20s/it, train_loss=0.071]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.20s/it, train_loss=0.00587]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.17s/it, train_loss=0.00587]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.17s/it, train_loss=0.00319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.17s/it, train_loss=0.00319]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.17s/it, train_loss=0.0115] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.15s/it, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.15s/it, train_loss=0.00694]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.12s/it, train_loss=0.00694]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.12s/it, train_loss=0.0187] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.10s/it, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.13s/it, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.13s/it, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.18s/it, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.18s/it, train_loss=0.00835]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.19s/it, train_loss=0.00835]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.19s/it, train_loss=0.125]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.16s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.16s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.09s/it, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.09s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.00344]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.00703]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.25it/s, train_loss=0.00703]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81 average loss: 0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  81%|████████  | 81/100 [44:42<10:53, 34.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 81 current AUC: 0.9950 current accuracy: 0.9006 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00398]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:28,  1.04it/s, train_loss=0.00398]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:28,  1.04it/s, train_loss=0.064]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.09it/s, train_loss=0.064]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.09it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.09it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.09it/s, train_loss=0.00655]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:23,  1.14it/s, train_loss=0.00655]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:23,  1.14it/s, train_loss=0.00334]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.00it/s, train_loss=0.00334]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:25,  1.00it/s, train_loss=0.0133] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:25,  1.00s/it, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.00s/it, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:23,  1.03it/s, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.03it/s, train_loss=0.00265]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:23,  1.01s/it, train_loss=0.00265]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.01s/it, train_loss=0.00168]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:22,  1.02s/it, train_loss=0.00168]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.0112] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:21,  1.02s/it, train_loss=0.0112]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.02s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:10<00:20,  1.00s/it, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.00s/it, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:11<00:18,  1.00it/s, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:18,  1.00it/s, train_loss=0.00878]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:12<00:18,  1.01s/it, train_loss=0.00878]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.01s/it, train_loss=0.0655] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:13<00:16,  1.03it/s, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:16,  1.03it/s, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:14<00:16,  1.02s/it, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.02s/it, train_loss=0.155] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.09s/it, train_loss=0.155]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.09s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:15,  1.12s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:14,  1.13s/it, train_loss=0.00806]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.13s/it, train_loss=0.00503]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:13,  1.09s/it, train_loss=0.00503]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.09s/it, train_loss=0.0525] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:12,  1.12s/it, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.12s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:11,  1.12s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:11,  1.12s/it, train_loss=0.0302] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:10,  1.13s/it, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:10,  1.13s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.08s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.0106] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:07,  1.07s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.07s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.03s/it, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.03s/it, train_loss=0.0206] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.05s/it, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.05s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.05s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.05s/it, train_loss=0.00398]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:03,  1.03s/it, train_loss=0.00398]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.03s/it, train_loss=0.0305] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.05s/it, train_loss=0.0305]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.05s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.11s/it, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:01,  1.11s/it, train_loss=0.0434]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.12it/s, train_loss=0.0434]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 average loss: 0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 82/100 [45:14<10:08, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 82 current AUC: 0.9884 current accuracy: 0.8882 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:39,  1.33s/it, train_loss=0.00279]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:39,  1.33s/it, train_loss=0.0203] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.15s/it, train_loss=0.0203]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.15s/it, train_loss=0.00968]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.09s/it, train_loss=0.00968]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.09s/it, train_loss=0.0652] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.01s/it, train_loss=0.0054]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.04s/it, train_loss=0.0054]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.04s/it, train_loss=0.00669]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.10s/it, train_loss=0.00669]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.10s/it, train_loss=0.053]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.14s/it, train_loss=0.053]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.14s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.07s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.07s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.01s/it, train_loss=0.00447]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.08s/it, train_loss=0.00447]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.08s/it, train_loss=0.00957]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.11s/it, train_loss=0.00957]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.11s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.17s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.17s/it, train_loss=0.13]   \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.13]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.13s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.16s/it, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.16s/it, train_loss=0.00615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.00615]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.19s/it, train_loss=0.00147]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.17s/it, train_loss=0.00147]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.17s/it, train_loss=0.0311] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.17s/it, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.17s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:12,  1.20s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.20s/it, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.14s/it, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.14s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.11s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.11s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.12s/it, train_loss=0.00456]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.12s/it, train_loss=0.0177] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.13s/it, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.13s/it, train_loss=0.0682]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0682]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.08s/it, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.08s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.11s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.16s/it, train_loss=0.0159] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.17s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.17s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.10it/s, train_loss=0.00637]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83 average loss: 0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  83%|████████▎ | 83/100 [45:49<09:39, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 83 current AUC: 0.9938 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.30s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.30s/it, train_loss=0.00365]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.20s/it, train_loss=0.00365]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.20s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.13s/it, train_loss=0.00315]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.13s/it, train_loss=0.0106] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.16s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.16s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.16s/it, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:30,  1.16s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.00169]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.00377]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.10s/it, train_loss=0.00377]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.00203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.00203]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.00181]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:23,  1.09s/it, train_loss=0.0389] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:22,  1.09s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.05s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.05s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.09s/it, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.09s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.05s/it, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:18,  1.05s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.02s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:16,  1.05s/it, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:16,  1.05s/it, train_loss=0.00182]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.10s/it, train_loss=0.00182]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.10s/it, train_loss=0.063]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.17s/it, train_loss=0.063]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.0028]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.09s/it, train_loss=0.0028]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.09s/it, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.15s/it, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.15s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.00341]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.00341]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.07s/it, train_loss=0.0104] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.03s/it, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.03s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.0335]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.09s/it, train_loss=0.025] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.025]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.12s/it, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.309] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.13s/it, train_loss=0.309]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.13s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.11s/it, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.0081]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.0081]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.15s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.0865]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.06it/s, train_loss=0.0865]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84 average loss: 0.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  84%|████████▍ | 84/100 [46:23<09:06, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 84 current AUC: 0.9910 current accuracy: 0.8882 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00714]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:36,  1.22s/it, train_loss=0.00714]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:36,  1.22s/it, train_loss=0.00746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:36,  1.27s/it, train_loss=0.00746]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:36,  1.27s/it, train_loss=0.0452] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.14s/it, train_loss=0.0452]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.14s/it, train_loss=0.00648]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.00648]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0126] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:30,  1.19s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:30,  1.19s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.15s/it, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.15s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.10s/it, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:26,  1.10s/it, train_loss=0.0355]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0355]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:24,  1.08s/it, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.05s/it, train_loss=0.00755]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.01s/it, train_loss=0.00755]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.01s/it, train_loss=0.0144] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.01s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.01s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.12s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.12s/it, train_loss=0.0082]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.08s/it, train_loss=0.0082]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.08s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.11s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.11s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.15s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.17s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.17s/it, train_loss=0.033] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.17s/it, train_loss=0.033]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.17s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.14s/it, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.14s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.18s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.18s/it, train_loss=0.00721]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:13,  1.22s/it, train_loss=0.00721]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:13,  1.22s/it, train_loss=0.031]  \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.24s/it, train_loss=0.031]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:12,  1.24s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.18s/it, train_loss=0.0693]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.18s/it, train_loss=0.152] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.14s/it, train_loss=0.152]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.14s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.09s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.03s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.03s/it, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.01it/s, train_loss=0.0443]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:04,  1.01it/s, train_loss=0.00362]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.02it/s, train_loss=0.00362]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.02it/s, train_loss=0.0103] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.01it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:02,  1.01it/s, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.02s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.03s/it, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.03s/it, train_loss=0.00484]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.00484]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85 average loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  85%|████████▌ | 85/100 [46:58<08:32, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 85 current AUC: 0.9922 current accuracy: 0.8696 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00223]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.00s/it, train_loss=0.00223]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.00s/it, train_loss=0.0342] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:32,  1.11s/it, train_loss=0.0342]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:32,  1.11s/it, train_loss=0.00933]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.04s/it, train_loss=0.00933]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.04s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.01s/it, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.05s/it, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.05s/it, train_loss=0.125]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.01s/it, train_loss=0.125]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.01s/it, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.02it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:23,  1.02it/s, train_loss=0.0036]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.11s/it, train_loss=0.0036]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.11s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.10s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.10s/it, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.16s/it, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.16s/it, train_loss=0.144] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.12s/it, train_loss=0.144]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.13s/it, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.16s/it, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.16s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.13s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.13s/it, train_loss=0.00704]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.14s/it, train_loss=0.00704]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.14s/it, train_loss=0.0351] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.17s/it, train_loss=0.0351]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.17s/it, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:16,  1.20s/it, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.20s/it, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.20s/it, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.20s/it, train_loss=0.00236]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:15,  1.26s/it, train_loss=0.00236]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:15,  1.26s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.18s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.18s/it, train_loss=0.0988] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.19s/it, train_loss=0.0988]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.19s/it, train_loss=0.00791]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.14s/it, train_loss=0.00791]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.14s/it, train_loss=0.0436] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:09,  1.13s/it, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.13s/it, train_loss=0.112] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.11s/it, train_loss=0.112]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.11s/it, train_loss=0.023]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.023]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.07s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.07s/it, train_loss=0.00532]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.07s/it, train_loss=0.0226] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.09s/it, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.09s/it, train_loss=0.029] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.15s/it, train_loss=0.029]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.15s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.12s/it, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.12s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.15s/it, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.15s/it, train_loss=0.0576]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.09it/s, train_loss=0.0576]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86 average loss: 0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|████████▌ | 86/100 [47:32<08:01, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 86 current AUC: 0.9944 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.05s/it, train_loss=0.0745]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:31,  1.05s/it, train_loss=0.00827]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.01it/s, train_loss=0.00827]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:28,  1.01it/s, train_loss=0.0113] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.06s/it, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.06s/it, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.0916]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.00767]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.04it/s, train_loss=0.00767]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.04it/s, train_loss=0.00735]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.00735]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.0278] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:23,  1.01it/s, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:23,  1.01it/s, train_loss=0.00792]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:23,  1.00s/it, train_loss=0.00792]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:23,  1.00s/it, train_loss=0.00328]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.06s/it, train_loss=0.00328]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.06s/it, train_loss=0.021]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:21,  1.05s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:21,  1.05s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.12s/it, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.12s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:22,  1.17s/it, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.184] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.19s/it, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.19s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.22s/it, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.22s/it, train_loss=0.00913]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:19,  1.20s/it, train_loss=0.00913]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:19,  1.20s/it, train_loss=0.0408] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.18s/it, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.18s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.11s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.11s/it, train_loss=0.039] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.06s/it, train_loss=0.039]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.06s/it, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.07s/it, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.07s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.15s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:12,  1.25s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:12,  1.25s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:11,  1.31s/it, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:11,  1.31s/it, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:10,  1.30s/it, train_loss=0.02]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:10,  1.30s/it, train_loss=0.00942]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.25s/it, train_loss=0.00942]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.25s/it, train_loss=0.0337] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:07,  1.19s/it, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:07,  1.19s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.10s/it, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.10s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.04s/it, train_loss=0.00327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.04s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.02s/it, train_loss=0.00554]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.02s/it, train_loss=0.0161] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.08s/it, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.08s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.02s/it, train_loss=0.103]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:33<00:00,  1.20it/s, train_loss=0.103]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87 average loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  87%|████████▋ | 87/100 [48:07<07:26, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 87 current AUC: 0.9968 current accuracy: 0.9441 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00533]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.18s/it, train_loss=0.00533]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.18s/it, train_loss=0.0163] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.18s/it, train_loss=0.0163]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.18s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.06s/it, train_loss=0.00517]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.06s/it, train_loss=0.0655] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:29,  1.07s/it, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:29,  1.07s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.04s/it, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.04s/it, train_loss=0.201] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:27,  1.09s/it, train_loss=0.201]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:27,  1.09s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0808] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.106] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.12s/it, train_loss=0.106]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.12s/it, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.09s/it, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.09s/it, train_loss=0.00309]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.08s/it, train_loss=0.00309]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:21,  1.08s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.19s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.19s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.20s/it, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.20s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:20,  1.18s/it, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.18s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.15s/it, train_loss=0.18]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:18,  1.21s/it, train_loss=0.18]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:18,  1.21s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:17,  1.26s/it, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:17,  1.26s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.23s/it, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.23s/it, train_loss=0.00446]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:14,  1.20s/it, train_loss=0.00446]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:14,  1.20s/it, train_loss=0.0188] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.15s/it, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.14s/it, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:11,  1.14s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.12s/it, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.14s/it, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.14s/it, train_loss=0.0338]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.11s/it, train_loss=0.0338]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.11s/it, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.04s/it, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.04s/it, train_loss=0.0604] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.11s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.11s/it, train_loss=0.0213] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.12s/it, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.12s/it, train_loss=0.00863]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.14s/it, train_loss=0.00863]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.14s/it, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.14s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.14s/it, train_loss=0.0705]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.08it/s, train_loss=0.0705]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88 average loss: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|████████▊ | 88/100 [48:42<06:55, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 88 current AUC: 0.9945 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.16s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.23s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.23s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.17s/it, train_loss=0.00921]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.17s/it, train_loss=0.0267] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:31,  1.17s/it, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:31,  1.17s/it, train_loss=0.0918]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.0918]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.09s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.16s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.16s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.16s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:27,  1.16s/it, train_loss=0.00482]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.21s/it, train_loss=0.00482]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.21s/it, train_loss=0.0123] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.24s/it, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:12<00:27,  1.24s/it, train_loss=0.044] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:26,  1.26s/it, train_loss=0.044]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:13<00:26,  1.26s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:24,  1.24s/it, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:24,  1.24s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.20s/it, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.20s/it, train_loss=0.00444]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.18s/it, train_loss=0.00444]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:21,  1.18s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.14s/it, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:19,  1.14s/it, train_loss=0.0059] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0059]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:17,  1.09s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.12s/it, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.12s/it, train_loss=0.0184] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.13s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.05s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.05s/it, train_loss=0.124] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.11s/it, train_loss=0.124]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.11s/it, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.07s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.05s/it, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.05s/it, train_loss=0.00451]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.08s/it, train_loss=0.00451]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.08s/it, train_loss=0.00178]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.13s/it, train_loss=0.00178]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.13s/it, train_loss=0.00607]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.16s/it, train_loss=0.00607]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.16s/it, train_loss=0.031]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.15s/it, train_loss=0.031]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.15s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.08s/it, train_loss=0.129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.08s/it, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.05s/it, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.05s/it, train_loss=0.052]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.052]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.03s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.06s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.06s/it, train_loss=0.021]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.07s/it, train_loss=0.021]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.07s/it, train_loss=0.0504]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.16it/s, train_loss=0.0504]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89 average loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  89%|████████▉ | 89/100 [49:17<06:21, 34.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 89 current AUC: 0.9955 current accuracy: 0.8944 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:37,  1.26s/it, train_loss=0.0447]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:37,  1.26s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:33,  1.16s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:33,  1.16s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:30,  1.10s/it, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:30,  1.10s/it, train_loss=0.00825]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:30,  1.14s/it, train_loss=0.00825]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:30,  1.14s/it, train_loss=0.00577]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.11s/it, train_loss=0.00577]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.11s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.04s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.04s/it, train_loss=0.0239] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:26,  1.09s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:26,  1.09s/it, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.15s/it, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:26,  1.15s/it, train_loss=0.102] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.14s/it, train_loss=0.102]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.14s/it, train_loss=0.0214]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:24,  1.15s/it, train_loss=0.0214]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.00564]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.05s/it, train_loss=0.00564]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:20,  1.05s/it, train_loss=0.0955] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.11s/it, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:21,  1.11s/it, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:20,  1.14s/it, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.15s/it, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.15s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.15s/it, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.15s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.15s/it, train_loss=0.0375]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.15s/it, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.21s/it, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.21s/it, train_loss=0.0706] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.19s/it, train_loss=0.0706]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.19s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.16s/it, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.16s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.12s/it, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.12s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.12s/it, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.12s/it, train_loss=0.238] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.13s/it, train_loss=0.238]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.13s/it, train_loss=0.0556]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.0556]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.10s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.09s/it, train_loss=0.0063]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.08s/it, train_loss=0.0063]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.08s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.12s/it, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.12s/it, train_loss=0.055] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.18s/it, train_loss=0.055]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.18s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.22s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.22s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.16s/it, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.16s/it, train_loss=0.215] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.18s/it, train_loss=0.215]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.18s/it, train_loss=0.0526]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.03it/s, train_loss=0.0526]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90 average loss: 0.0449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 90/100 [49:52<05:48, 34.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 90 current AUC: 0.9947 current accuracy: 0.8882 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.0159]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.03s/it, train_loss=0.00591]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.03it/s, train_loss=0.00591]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.03it/s, train_loss=0.0294] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.06it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.06it/s, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.02it/s, train_loss=0.00698]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:26,  1.02it/s, train_loss=0.0272] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:25,  1.04it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:25,  1.04it/s, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:25,  1.03s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.403]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.05s/it, train_loss=0.403]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.05s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.02s/it, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.02s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.08s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.08s/it, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:20,  1.04s/it, train_loss=0.0068]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:20,  1.04s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:21,  1.13s/it, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:21,  1.13s/it, train_loss=0.00255]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.11s/it, train_loss=0.00255]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.11s/it, train_loss=0.0462] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.04s/it, train_loss=0.0462]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:17,  1.04s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:15,  1.00it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:15,  1.00it/s, train_loss=0.029] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:14,  1.01it/s, train_loss=0.029]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:14,  1.01it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:13,  1.05it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:13,  1.05it/s, train_loss=0.013] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:12,  1.08it/s, train_loss=0.013]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:12,  1.08it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:11,  1.05it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:11,  1.05it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.03it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.03it/s, train_loss=0.00991]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.02it/s, train_loss=0.00991]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.02it/s, train_loss=0.0263] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.01s/it, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.01s/it, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:23<00:08,  1.01s/it, train_loss=0.0206]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.01s/it, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:24<00:06,  1.03it/s, train_loss=0.00889]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:06,  1.03it/s, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:25<00:06,  1.00s/it, train_loss=0.00314]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.00s/it, train_loss=0.137]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:26<00:05,  1.06s/it, train_loss=0.137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.06s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:27<00:04,  1.02s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.02s/it, train_loss=0.021] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:28<00:02,  1.03it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.03it/s, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:29<00:01,  1.04it/s, train_loss=0.00408]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.04it/s, train_loss=0.0496] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.0496]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:30<00:00,  1.07it/s, train_loss=0.0813]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:30<00:00,  1.31it/s, train_loss=0.0813]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91 average loss: 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  91%|█████████ | 91/100 [50:23<05:03, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 91 current AUC: 0.9898 current accuracy: 0.8323 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:35,  1.17s/it, train_loss=0.0467]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:35,  1.17s/it, train_loss=0.00948]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.08s/it, train_loss=0.00948]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.08s/it, train_loss=0.0403] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:29,  1.05s/it, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:29,  1.05s/it, train_loss=0.00795]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.00795]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.0392] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.13s/it, train_loss=0.0392]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:29,  1.13s/it, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.13s/it, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.13s/it, train_loss=0.0805]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.07s/it, train_loss=0.0805]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.07s/it, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:22,  1.01it/s, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:22,  1.01it/s, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.0378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0378]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.11s/it, train_loss=0.0503]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.08s/it, train_loss=0.0503]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.08s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:19,  1.05s/it, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:19,  1.05s/it, train_loss=0.0738] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.05s/it, train_loss=0.0738]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.05s/it, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.03s/it, train_loss=0.11]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.03s/it, train_loss=0.00596]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.09s/it, train_loss=0.00596]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.09s/it, train_loss=0.0262] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.13s/it, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.13s/it, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.15s/it, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.15s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.13s/it, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.13s/it, train_loss=0.0363]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:12,  1.15s/it, train_loss=0.0363]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.15s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.10s/it, train_loss=0.0757]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.10s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.03s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.03s/it, train_loss=0.082] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.09s/it, train_loss=0.082]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.09s/it, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.04s/it, train_loss=0.0858]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.04s/it, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:05,  1.02it/s, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.02it/s, train_loss=0.0265] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.03it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.03it/s, train_loss=0.122] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.01s/it, train_loss=0.122]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.01s/it, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:02,  1.01it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.01it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:01,  1.05it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.05it/s, train_loss=0.00726]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.07it/s, train_loss=0.00726]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:31<00:00,  1.07it/s, train_loss=0.267]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:31<00:00,  1.28it/s, train_loss=0.267]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 average loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  92%|█████████▏| 92/100 [50:56<04:27, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 92 current AUC: 0.9900 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:29,  1.03it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:29,  1.03it/s, train_loss=0.0117] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.07it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.07it/s, train_loss=0.00794]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s, train_loss=0.00794]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:25,  1.11it/s, train_loss=0.0143] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:24,  1.10it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:24,  1.10it/s, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:27,  1.06s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.06s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.12s/it, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.12s/it, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:29,  1.21s/it, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:29,  1.21s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:28,  1.24s/it, train_loss=0.0352]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:28,  1.24s/it, train_loss=0.0158]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.22s/it, train_loss=0.0158]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:26,  1.22s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.11s/it, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.11s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.09s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.06s/it, train_loss=0.00535]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.00535]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.04s/it, train_loss=0.0578] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:17,  1.02s/it, train_loss=0.0578]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:17,  1.02s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.08s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.08s/it, train_loss=0.00361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.03s/it, train_loss=0.00361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:14,  1.03s/it, train_loss=0.0107] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.01s/it, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.01s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.05s/it, train_loss=0.00459]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:12,  1.05s/it, train_loss=0.0598] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0598]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.04s/it, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.04s/it, train_loss=0.0914] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.07s/it, train_loss=0.0914]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.07s/it, train_loss=0.131] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.06s/it, train_loss=0.131]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.06s/it, train_loss=0.00966]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.10s/it, train_loss=0.00966]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.10s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.11s/it, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.11s/it, train_loss=0.0461] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.15s/it, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.15s/it, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.15s/it, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.15s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.06s/it, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.06s/it, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.02s/it, train_loss=0.0458]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.02s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.0651]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.386] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.21it/s, train_loss=0.386]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93 average loss: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  93%|█████████▎| 93/100 [51:29<03:53, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 93 current AUC: 0.9950 current accuracy: 0.9379 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00182]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.27s/it, train_loss=0.00182]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.27s/it, train_loss=0.0174] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:34,  1.17s/it, train_loss=0.0174]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:34,  1.17s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:32,  1.18s/it, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:32,  1.18s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:33,  1.22s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:06<00:33,  1.22s/it, train_loss=0.096] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:31,  1.23s/it, train_loss=0.096]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:31,  1.23s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.16s/it, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:28,  1.16s/it, train_loss=0.0044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.17s/it, train_loss=0.0044]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.17s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.09s/it, train_loss=0.00656]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:25,  1.09s/it, train_loss=0.0151] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:24,  1.13s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.12s/it, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.12s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:23,  1.16s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:23,  1.16s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.10s/it, train_loss=0.173]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:20,  1.10s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:19,  1.08s/it, train_loss=0.0395]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.08s/it, train_loss=0.00642]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.00642]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:18,  1.08s/it, train_loss=0.00332]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.13s/it, train_loss=0.00332]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.13s/it, train_loss=0.0928] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.09s/it, train_loss=0.0928]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:16,  1.09s/it, train_loss=0.267] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.08s/it, train_loss=0.267]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.08s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:13,  1.04s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:13,  1.04s/it, train_loss=0.111] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:11,  1.00it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:11,  1.00it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.03s/it, train_loss=0.192]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.03s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.02s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.02s/it, train_loss=0.0333] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:08,  1.01it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:08,  1.01it/s, train_loss=0.139] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.03s/it, train_loss=0.139]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.03s/it, train_loss=0.0073]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.09s/it, train_loss=0.0073]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.09s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.10s/it, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.10s/it, train_loss=0.24]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.08s/it, train_loss=0.24]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.08s/it, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.02s/it, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.02s/it, train_loss=0.0555]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.03it/s, train_loss=0.0555]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.03it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.04it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:01,  1.04it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.03it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.03it/s, train_loss=0.053] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.20it/s, train_loss=0.053]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 average loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  94%|█████████▍| 94/100 [52:03<03:20, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 94 current AUC: 0.9909 current accuracy: 0.8447 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:30,  1.02s/it, train_loss=0.0516]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:30,  1.02s/it, train_loss=0.0222]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.07s/it, train_loss=0.0222]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.07s/it, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.04it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.04it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:26,  1.01it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:26,  1.01it/s, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:26,  1.00s/it, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.00s/it, train_loss=0.00434]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.00434]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.0233] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:27,  1.13s/it, train_loss=0.0233]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:27,  1.13s/it, train_loss=0.0369]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:26,  1.14s/it, train_loss=0.0369]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:26,  1.14s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:23,  1.09s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:23,  1.09s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:22,  1.05s/it, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:22,  1.05s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:21,  1.07s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.07s/it, train_loss=0.00385]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.06s/it, train_loss=0.00385]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.06s/it, train_loss=0.008]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:19,  1.09s/it, train_loss=0.008]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.09s/it, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.14s/it, train_loss=0.0321]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.14s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.13s/it, train_loss=0.017] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.10s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.10s/it, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.11s/it, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:13,  1.16s/it, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.16s/it, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:11,  1.08s/it, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.08s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:10,  1.06s/it, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:10,  1.06s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:23<00:09,  1.04s/it, train_loss=0.00471]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=0.0262] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.08s/it, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.017] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.08s/it, train_loss=0.017]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.08s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.02s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.02s/it, train_loss=0.28]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:04,  1.00it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.00it/s, train_loss=0.0741]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:03,  1.00it/s, train_loss=0.0741]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.00it/s, train_loss=0.0975]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.03s/it, train_loss=0.0975]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.03s/it, train_loss=0.0667]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.09s/it, train_loss=0.0667]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.09s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.0502]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.05s/it, train_loss=0.0119]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.21it/s, train_loss=0.0119]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95 average loss: 0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  95%|█████████▌| 95/100 [52:36<02:46, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 95 current AUC: 0.9918 current accuracy: 0.8820 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00407]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.00407]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.00483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:26,  1.08it/s, train_loss=0.00483]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:26,  1.08it/s, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:27,  1.01it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:27,  1.01it/s, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:27,  1.01s/it, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.01s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:26,  1.03s/it, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:26,  1.03s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:26,  1.07s/it, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:26,  1.07s/it, train_loss=0.199] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.199]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:25,  1.10s/it, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:25,  1.10s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:22,  1.03s/it, train_loss=0.00418]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:22,  1.03s/it, train_loss=0.056]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.10s/it, train_loss=0.056]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.10s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:11<00:22,  1.11s/it, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.11s/it, train_loss=0.00875]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:12<00:20,  1.07s/it, train_loss=0.00875]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:20,  1.07s/it, train_loss=0.00342]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:13<00:18,  1.02s/it, train_loss=0.00342]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:18,  1.02s/it, train_loss=0.0917] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:14<00:18,  1.08s/it, train_loss=0.0917]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.08s/it, train_loss=0.00584]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:15<00:17,  1.08s/it, train_loss=0.00584]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.08s/it, train_loss=0.00368]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:16<00:16,  1.08s/it, train_loss=0.00368]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.08s/it, train_loss=0.012]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:17<00:14,  1.05s/it, train_loss=0.012]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:14,  1.05s/it, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:18<00:13,  1.02s/it, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:13,  1.02s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:19<00:12,  1.03s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:20<00:12,  1.03s/it, train_loss=0.0293] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:20<00:10,  1.00it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:21<00:10,  1.00it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:21<00:09,  1.00it/s, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:22<00:09,  1.00it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:22<00:09,  1.06s/it, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.06s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:24<00:08,  1.10s/it, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.10s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:25<00:07,  1.06s/it, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.06s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:26<00:06,  1.06s/it, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.06s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:27<00:05,  1.15s/it, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.15s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:28<00:04,  1.18s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.18s/it, train_loss=0.00861]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:29<00:03,  1.15s/it, train_loss=0.00861]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.15s/it, train_loss=0.0194] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:30<00:02,  1.13s/it, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.13s/it, train_loss=0.00754]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.00754]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.14s/it, train_loss=0.00197]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.10it/s, train_loss=0.00197]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 average loss: 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  96%|█████████▌| 96/100 [53:09<02:13, 33.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 96 current AUC: 0.9958 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.0184]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.16s/it, train_loss=0.00367]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:28,  1.03it/s, train_loss=0.00367]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:28,  1.03it/s, train_loss=0.0497] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:26,  1.07it/s, train_loss=0.0497]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:26,  1.07it/s, train_loss=0.021] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:25,  1.05it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:25,  1.05it/s, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:23,  1.10it/s, train_loss=0.00476]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:23,  1.10it/s, train_loss=0.0288] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:25,  1.03s/it, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:25,  1.03s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:25,  1.06s/it, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:25,  1.06s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.07s/it, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.07s/it, train_loss=0.00281]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.12s/it, train_loss=0.00281]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.12s/it, train_loss=0.00853]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:24,  1.15s/it, train_loss=0.00853]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:24,  1.15s/it, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:24,  1.25s/it, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:24,  1.25s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:23,  1.25s/it, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:23,  1.25s/it, train_loss=0.012]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.21s/it, train_loss=0.012]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.21s/it, train_loss=0.00245]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:19,  1.12s/it, train_loss=0.00245]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:19,  1.12s/it, train_loss=0.00581]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:18,  1.14s/it, train_loss=0.00581]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.14s/it, train_loss=0.0247] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:16,  1.13s/it, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:16,  1.13s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:18<00:15,  1.12s/it, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.12s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:19<00:14,  1.13s/it, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:14,  1.13s/it, train_loss=0.0053]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.12s/it, train_loss=0.0053]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.12s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.13s/it, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.13s/it, train_loss=0.00248]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.15s/it, train_loss=0.00248]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.15s/it, train_loss=0.00603]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:10,  1.12s/it, train_loss=0.00603]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.12s/it, train_loss=0.038]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:08,  1.08s/it, train_loss=0.038]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.08s/it, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.03s/it, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.03s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:06,  1.07s/it, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.07s/it, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:05,  1.04s/it, train_loss=0.0093]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.04s/it, train_loss=0.00765]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:04,  1.06s/it, train_loss=0.00765]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.06s/it, train_loss=0.0134] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:02,  1.02it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:02,  1.02it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:01,  1.05it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:01,  1.05it/s, train_loss=0.0161] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:00,  1.06it/s, train_loss=0.0151]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.34it/s, train_loss=0.0151]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97 average loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  97%|█████████▋| 97/100 [53:42<01:39, 33.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 97 current AUC: 0.9930 current accuracy: 0.9068 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00586]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:34,  1.13s/it, train_loss=0.00586]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:34,  1.13s/it, train_loss=0.00406]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:35,  1.22s/it, train_loss=0.00406]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:35,  1.22s/it, train_loss=0.0247] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:34,  1.23s/it, train_loss=0.0247]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:34,  1.23s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:34,  1.27s/it, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:34,  1.27s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:29,  1.14s/it, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:07<00:29,  1.14s/it, train_loss=0.00372]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:30,  1.22s/it, train_loss=0.00372]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:30,  1.22s/it, train_loss=0.00221]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.19s/it, train_loss=0.00221]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.19s/it, train_loss=0.00287]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.00287]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:25,  1.18s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:25,  1.18s/it, train_loss=0.0688] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:23,  1.13s/it, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.114] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:21,  1.10s/it, train_loss=0.114]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:21,  1.10s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.17s/it, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.17s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:19,  1.11s/it, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:19,  1.11s/it, train_loss=0.0347] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.11s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:18,  1.11s/it, train_loss=0.00138]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.17s/it, train_loss=0.00138]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.17s/it, train_loss=0.00496]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.16s/it, train_loss=0.00496]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.16s/it, train_loss=0.00762]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.19s/it, train_loss=0.00762]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:21<00:16,  1.19s/it, train_loss=0.00137]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.23s/it, train_loss=0.00137]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:15,  1.23s/it, train_loss=0.00565]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.13s/it, train_loss=0.00565]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:23<00:13,  1.13s/it, train_loss=0.0391] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:11,  1.08s/it, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:11,  1.08s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.04s/it, train_loss=0.0668]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.04s/it, train_loss=0.018] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:10,  1.13s/it, train_loss=0.018]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:10,  1.13s/it, train_loss=0.00664]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:09,  1.15s/it, train_loss=0.00664]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:09,  1.15s/it, train_loss=0.00308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:08,  1.17s/it, train_loss=0.00308]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:08,  1.17s/it, train_loss=0.00766]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.15s/it, train_loss=0.00766]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.15s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.11s/it, train_loss=0.00208]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.11s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.10s/it, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:32<00:04,  1.10s/it, train_loss=0.0328] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.11s/it, train_loss=0.0328]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:33<00:03,  1.11s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.09s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:34<00:02,  1.09s/it, train_loss=0.00288]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.03s/it, train_loss=0.00288]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.03s/it, train_loss=0.0394] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.18it/s, train_loss=0.0394]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98 average loss: 0.0184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  98%|█████████▊| 98/100 [54:17<01:07, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 98 current AUC: 0.9927 current accuracy: 0.9193 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0026]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:27,  1.08it/s, train_loss=0.0026]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:27,  1.08it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:27,  1.07it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:27,  1.07it/s, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:28,  1.00s/it, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:28,  1.00s/it, train_loss=0.0347] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:27,  1.03s/it, train_loss=0.0347]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:27,  1.03s/it, train_loss=0.00405]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:28,  1.09s/it, train_loss=0.00405]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:28,  1.09s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:28,  1.15s/it, train_loss=0.00635]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:07<00:28,  1.15s/it, train_loss=0.0144] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:07<00:28,  1.18s/it, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.18s/it, train_loss=0.00142]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:08<00:24,  1.08s/it, train_loss=0.00142]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:24,  1.08s/it, train_loss=0.0153] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:09<00:24,  1.13s/it, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:24,  1.13s/it, train_loss=0.00253]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:10<00:23,  1.13s/it, train_loss=0.00253]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:23,  1.13s/it, train_loss=0.00353]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:22,  1.14s/it, train_loss=0.00353]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:13<00:22,  1.14s/it, train_loss=0.0289] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:13<00:22,  1.20s/it, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.20s/it, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:14<00:21,  1.20s/it, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:21,  1.20s/it, train_loss=0.0082] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:15<00:18,  1.10s/it, train_loss=0.0082]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:18,  1.10s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:16<00:17,  1.11s/it, train_loss=0.00241]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:17,  1.11s/it, train_loss=0.0373] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:17<00:17,  1.15s/it, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.15s/it, train_loss=0.00292]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:16,  1.20s/it, train_loss=0.00292]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:16,  1.20s/it, train_loss=0.0367] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:15,  1.18s/it, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:21<00:15,  1.18s/it, train_loss=0.00222]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:21<00:13,  1.11s/it, train_loss=0.00222]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.11s/it, train_loss=0.0021] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:12,  1.10s/it, train_loss=0.0021]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:23<00:12,  1.10s/it, train_loss=0.0023]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:23<00:11,  1.10s/it, train_loss=0.0023]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:11,  1.10s/it, train_loss=0.00858]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:24<00:09,  1.04s/it, train_loss=0.00858]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.04s/it, train_loss=0.0081] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:25<00:07,  1.01it/s, train_loss=0.0081]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:07,  1.01it/s, train_loss=0.00514]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:26<00:07,  1.01s/it, train_loss=0.00514]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.01s/it, train_loss=0.00337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:27<00:05,  1.00it/s, train_loss=0.00337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:05,  1.00it/s, train_loss=0.0107] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:28<00:04,  1.04it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:04,  1.04it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:29<00:03,  1.04it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:03,  1.04it/s, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:30<00:03,  1.01s/it, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.01s/it, train_loss=0.0148] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:31<00:02,  1.03s/it, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.03s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.00173]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:32<00:01,  1.07s/it, train_loss=0.0801] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:32<00:00,  1.17it/s, train_loss=0.0801]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 average loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  99%|█████████▉| 99/100 [54:51<00:33, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 99 current AUC: 0.9952 current accuracy: 0.9379 best AUC: 0.9984 at epoch: 61\n",
      "----------\n",
      "epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:01<?, ?it/s, train_loss=0.00614]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:38,  1.29s/it, train_loss=0.00614]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:02<00:38,  1.29s/it, train_loss=0.00501]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:31,  1.09s/it, train_loss=0.00501]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:03<00:31,  1.09s/it, train_loss=0.00198]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:31,  1.14s/it, train_loss=0.00198]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:04<00:31,  1.14s/it, train_loss=0.0325] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:28,  1.06s/it, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:05<00:28,  1.06s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:27,  1.06s/it, train_loss=0.00363]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:06<00:27,  1.06s/it, train_loss=0.118]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:06<00:29,  1.19s/it, train_loss=0.118]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:08<00:29,  1.19s/it, train_loss=0.016]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:08<00:28,  1.20s/it, train_loss=0.016]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:09<00:28,  1.20s/it, train_loss=0.00259]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:09<00:27,  1.19s/it, train_loss=0.00259]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:10<00:27,  1.19s/it, train_loss=0.00982]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:10<00:27,  1.27s/it, train_loss=0.00982]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:11<00:27,  1.27s/it, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:11<00:25,  1.19s/it, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:12<00:25,  1.19s/it, train_loss=0.00084]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:12<00:24,  1.20s/it, train_loss=0.00084]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:14<00:24,  1.20s/it, train_loss=0.0925] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:14<00:22,  1.19s/it, train_loss=0.0925]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:15<00:22,  1.19s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:15<00:20,  1.14s/it, train_loss=0.00211]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:16<00:20,  1.14s/it, train_loss=0.0122] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:16<00:20,  1.18s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:17<00:20,  1.18s/it, train_loss=0.00796]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:17<00:18,  1.16s/it, train_loss=0.00796]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:18<00:18,  1.16s/it, train_loss=0.0146] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:18<00:17,  1.19s/it, train_loss=0.0146]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:19<00:17,  1.19s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:19<00:15,  1.14s/it, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:20<00:15,  1.14s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:20<00:14,  1.11s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:22<00:14,  1.11s/it, train_loss=0.0122] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.13s/it, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:22<00:13,  1.13s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:22<00:11,  1.07s/it, train_loss=0.00959]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:24<00:11,  1.07s/it, train_loss=0.00866]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:24<00:10,  1.08s/it, train_loss=0.00866]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:25<00:10,  1.08s/it, train_loss=0.00144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:25<00:09,  1.05s/it, train_loss=0.00144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:26<00:09,  1.05s/it, train_loss=0.0089] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:26<00:08,  1.04s/it, train_loss=0.0089]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:27<00:08,  1.04s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:27<00:07,  1.03s/it, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:28<00:07,  1.03s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:28<00:06,  1.02s/it, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:29<00:06,  1.02s/it, train_loss=0.143]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:29<00:05,  1.00s/it, train_loss=0.143]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:30<00:05,  1.00s/it, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:30<00:04,  1.01s/it, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:31<00:04,  1.01s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:31<00:03,  1.16s/it, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:32<00:03,  1.16s/it, train_loss=0.0276] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:32<00:02,  1.15s/it, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:33<00:02,  1.15s/it, train_loss=0.00772]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:33<00:01,  1.14s/it, train_loss=0.00772]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:34<00:01,  1.14s/it, train_loss=0.197]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:34<00:00,  1.12it/s, train_loss=0.197]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 average loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 100/100 [55:25<00:00, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 100 current AUC: 0.9918 current accuracy: 0.9130 best AUC: 0.9984 at epoch: 61\n",
      "train completed, best_metric: 0.9984 at epoch: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=False)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            print('1')\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model_3d.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_3d.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "example = []\n",
    "example_preds = []\n",
    "example_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'].to(device),\n",
    "            test_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "        if len(example) < 10:\n",
    "            example.append(test_images)\n",
    "            example_preds.append(pred)\n",
    "            example_labels.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9857    1.0000    0.9928        69\n",
      "           1     0.6618    0.6618    0.6618        68\n",
      "           2     0.7170    0.5507    0.6230        69\n",
      "           3     0.4872    0.5846    0.5315        65\n",
      "           4     0.5738    0.5385    0.5556        65\n",
      "           5     1.0000    0.9697    0.9846        66\n",
      "           6     0.9333    1.0000    0.9655        28\n",
      "           7     1.0000    1.0000    1.0000        21\n",
      "           8     1.0000    1.0000    1.0000        21\n",
      "           9     0.9014    0.9275    0.9143        69\n",
      "          10     0.9315    0.9855    0.9577        69\n",
      "\n",
      "    accuracy                         0.8049       610\n",
      "   macro avg     0.8356    0.8380    0.8352       610\n",
      "weighted avg     0.8066    0.8049    0.8038       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=True, spatial_dims=3, n_input_channels=1, \n",
    "                 feed_forward=False, shortcut_type='A', bias_downsample=True).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.00005)\n",
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avgpool = nn.Sequential(\n",
    "    model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, n_classes)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]         \u001b[A\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.18 GiB is free. Process 188468 has 3.55 GiB memory in use. Process 188453 has 2.65 GiB memory in use. Process 188434 has 16.31 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 3.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/overrides.py:1621\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1616\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1617\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1621\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1418\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1418\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.18 GiB is free. Process 188468 has 3.55 GiB memory in use. Process 188453 has 2.65 GiB memory in use. Process 188434 has 16.31 GiB memory in use. Of the allocated memory 11.55 GiB is allocated by PyTorch, and 3.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=False)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            print('1')\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model_3d_pretrained.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_3d_pretrained.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "example = []\n",
    "example_preds = []\n",
    "example_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'].to(device),\n",
    "            test_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "        if len(example) < 10:\n",
    "            example.append(test_images)\n",
    "            example_preds.append(pred)\n",
    "            example_labels.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
