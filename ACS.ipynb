{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d01957-48a4-4352-99f9-87422811892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2441\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: cf815ed4e44a5b8ce67e894ab0bc2765279a1a59\n",
      "MONAI __file__: /mnt/hdd/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.17.1\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.46.2\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import shutil\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import decollate_batch, DataLoader, Dataset\n",
    "from monai.metrics import ROCAUCMetric\n",
    "import monai.networks.nets as nets\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    RandGaussianNoise,\n",
    "    RandAdjustContrast,\n",
    "    ScaleIntensity, \n",
    "    Transform,\n",
    "    ToTensor,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713b5ffa-0bf3-4b1f-a64e-e0b7590b0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/hdd/marina/.medmnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca83c47-d6f7-46ef-a8cb-8d03dd114aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n",
      "Using downloaded and verified file: /mnt/hdd/marina/.medmnist/organmnist3d_64.npz\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'organmnist3d'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', download=download, size=64)\n",
    "val_dataset = DataClass(split='val', download=download, size=64)\n",
    "test_dataset = DataClass(split='test', download=download, size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8828564-efdb-4aba-b313-36d2f7244767",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),\n",
    "        RandGaussianNoise(prob=0.5, mean=0.0, std=0.05),\n",
    "        RandAdjustContrast(gamma=(0.7, 1.3), prob=0.5),\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        ToTensor(),\n",
    "        EnsureType(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    ToTensor(),\n",
    "    EnsureType(),\n",
    "])\n",
    "\n",
    "y_pred_trans = Compose([Activations(softmax=True)])\n",
    "y_trans = Compose([AsDiscrete(to_onehot=n_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b8ba64-1956-40db-aac7-112e83eb90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _3D_Dataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return {'images': data, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3023b6-8159-476d-925e-ea35296bf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ = _3D_Dataset(train_dataset, transform=train_transforms)\n",
    "val_dataset_ = _3D_Dataset(val_dataset, transform=val_transforms)\n",
    "test_dataset_ = _3D_Dataset(test_dataset, transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5390cf1b-f23f-4f67-b754-ffa9670437fd",
   "metadata": {},
   "source": [
    "import torch\n",
    "from acsconv.operators import ACSConv, SoftACSConv\n",
    "B, C_in, D, H, W = (1, 3, 64, 64, 64)\n",
    "x = torch.rand(B, C_in, D, H, W)\n",
    "# ACSConv to process 3D volumnes\n",
    "conv = ACSConv(in_channels=3, out_channels=11, kernel_size=3, padding=1)\n",
    "out = conv(x)\n",
    "# # SoftACSConv to process 3D volumnes\n",
    "# conv = SoftACSConv(in_channels=3, out_channels=10, kernel_size=3, padding=1)\n",
    "# out = conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f0824b-c677-4252-8803-9f0d7a0a9da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from torchvision.models import resnet18\n",
    "from monai.networks.nets import DenseNet121, resnet, resnet18\n",
    "from acsconv.converters import ACSConverter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model_2d is a standard pytorch 2D model\n",
    "model_2d = resnet18(pretrained=False, spatial_dims=3, n_input_channels=1, num_classes=n_classes).to(device)\n",
    "# model_2d = resnet18(pretrained=True)\n",
    "\n",
    "model_3d = ACSConverter(model_2d).to(device)\n",
    "# once converted, model_3d is using ACSConv and capable of processing 3D volumes.\n",
    "# model_3d.fc = nn.Linear(model_3d.fc.in_features, n_classes)\n",
    "# model_3d = model_3d.to(device)\n",
    "print(model_3d)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_3d.parameters(), 0.00005)\n",
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefa2d64-8196-4719-a577-4c29a65091fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=2.44]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=2.44]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=2.4] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=2.4]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=2.36]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.27it/s, train_loss=2.36]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.27it/s, train_loss=2.26]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=2.26]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=2.1] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=2.1]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=2.28]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=2.28]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=2.4] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=2.4]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=1.89]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=1.89]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=1.8] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=1.8]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=1.74]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:15,  1.31it/s, train_loss=1.74]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:15,  1.31it/s, train_loss=1.71]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.31it/s, train_loss=1.71]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.31it/s, train_loss=1.68]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.34it/s, train_loss=1.68]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.34it/s, train_loss=1.76]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.30it/s, train_loss=1.76]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.30it/s, train_loss=1.73]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:12,  1.32it/s, train_loss=1.73]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:12,  1.32it/s, train_loss=1.59]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=1.59]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=1.57]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.30it/s, train_loss=1.57]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.30it/s, train_loss=1.63]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=1.63]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=1.4] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:13<00:10,  1.29it/s, train_loss=1.4]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.29it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.27it/s, train_loss=1.48]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=1.55]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=1.55]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=1.45]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=1.45]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=1.29]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=1.29]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=1.17]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=1.17]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.25it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=1.38]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=1.38]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=1.13]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=1.1] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=1.1]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=1.26]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=1.26]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.98]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.27it/s, train_loss=0.98]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=1.32]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=1.09]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.60it/s, train_loss=1.09]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average loss: 1.6469\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [00:25<42:17, 25.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.5883 current accuracy: 0.0932 best AUC: 0.5883 at epoch: 1\n",
      "----------\n",
      "epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=1.14]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=1.14]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=1.33]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=1.33]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=1.34]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.7] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.7]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=1.09]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=1.09]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=1.29]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=1.29]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.941]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.941]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=1.25] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=1.25]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=1.24]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=1.24]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.802]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.802]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=1.01] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=1.01]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.847]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.847]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.713]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.713]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=1.42] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=1.42]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.879]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.879]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.889]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.889]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=1.12] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.20it/s, train_loss=1.12]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.20it/s, train_loss=0.825]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.21it/s, train_loss=0.825]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.21it/s, train_loss=0.788]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.788]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.821]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.821]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.798]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.798]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=1.21] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=1.21]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=1.35]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=1.35]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.68]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.68]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.718]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.718]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.905]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.905]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=1.05] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=1.05]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.794]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.794]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.889]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.889]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.696]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.696]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average loss: 0.9873\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [00:51<42:19, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.9709 current accuracy: 0.6273 best AUC: 0.9709 at epoch: 2\n",
      "----------\n",
      "epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.949]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.949]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.907]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.907]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.673]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.673]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.88] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.88]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.869]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.869]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.76] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.76]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.759]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.759]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.569]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.569]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.697]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.697]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.659]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.659]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.771]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.771]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.587]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.587]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.939]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:12,  1.34it/s, train_loss=0.939]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:12,  1.34it/s, train_loss=0.827]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.827]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.669]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.669]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.821]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.821]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.605]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.605]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.653]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.20it/s, train_loss=0.653]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.20it/s, train_loss=0.534]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.20it/s, train_loss=0.534]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.20it/s, train_loss=1.07] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=1.07]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.21it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s, train_loss=0.538]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.538]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.769]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.769]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.832]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.832]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.558]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.558]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.574]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.573]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.573]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.714]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.714]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.812]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.812]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 average loss: 0.7132\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 3/100 [01:17<42:06, 26.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 3 current AUC: 0.9724 current accuracy: 0.5714 best AUC: 0.9724 at epoch: 3\n",
      "----------\n",
      "epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.454]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.688]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.688]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.62] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.62]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.632]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.632]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.749]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.749]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.665]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.665]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.968]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.968]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.548]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.564]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.564]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.534]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.534]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.636]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.636]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.637]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.637]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.593]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.593]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.69] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.69]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=1.36]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=1.36]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=1.02]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=1.02]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.705]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.705]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.511]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.511]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.565]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.601]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.601]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=0.654]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.654]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.685]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.685]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.566]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.615]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.615]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.618]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.618]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.516]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.516]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.736]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.736]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 average loss: 0.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 4/100 [01:42<40:52, 25.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 4 current AUC: 0.8953 current accuracy: 0.3478 best AUC: 0.9724 at epoch: 3\n",
      "----------\n",
      "epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.723]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.723]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.745]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.745]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.737]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.737]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.506]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.513]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.513]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.656]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.899]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:19,  1.20it/s, train_loss=0.899]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:19,  1.20it/s, train_loss=0.481]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.481]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.841]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.841]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.671]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.671]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.693]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.693]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.705]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.705]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.658]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.658]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.6]  \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.6]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.54]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.54]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.49]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.49]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.591]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.591]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.657]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.657]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.692]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.692]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.8]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.8]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.377]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.42] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.42]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.987]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.987]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.577]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.577]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.418]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.418]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.456]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.456]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 average loss: 0.6055\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 5/100 [02:09<40:52, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 5 current AUC: 0.9826 current accuracy: 0.7143 best AUC: 0.9826 at epoch: 5\n",
      "----------\n",
      "epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.445]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.445]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.619]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.619]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.681]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.681]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.32it/s, train_loss=0.512]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.32it/s, train_loss=0.497]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:18,  1.33it/s, train_loss=0.497]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:18,  1.33it/s, train_loss=0.513]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:17,  1.35it/s, train_loss=0.513]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:17,  1.35it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.34it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.34it/s, train_loss=0.97] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:16,  1.32it/s, train_loss=0.97]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.32it/s, train_loss=0.479]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=0.479]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.459]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.48] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.48]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:13,  1.22it/s, train_loss=0.476]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.691]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.21it/s, train_loss=0.691]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s, train_loss=0.39] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.522]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.448]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.448]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.509]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.509]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.381]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.478]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.478]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.769]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.769]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.479]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.479]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.424]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 average loss: 0.4965\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 6/100 [02:35<40:36, 25.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 6 current AUC: 0.9853 current accuracy: 0.7267 best AUC: 0.9853 at epoch: 6\n",
      "----------\n",
      "epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.561]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.561]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.491]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.491]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.43] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.43]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.755]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.755]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.573]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.573]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.477]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.477]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.371]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.627]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.627]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.508]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.384]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.20it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.20it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.517]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.517]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.22it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.354]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.59] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.59]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.489]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.572]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.572]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.403]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.606]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.606]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.551]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 average loss: 0.4540\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 7/100 [03:01<40:21, 26.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 7 current AUC: 0.9890 current accuracy: 0.8385 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.374]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.374]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.365]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.4]  \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.4]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.462]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.462]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.602]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.602]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.399]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.527]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.21it/s, train_loss=0.527]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.291]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.559]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.361]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.474]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.348]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.348]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.21it/s, train_loss=0.551]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.21it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.562]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.562]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.519]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=0.588]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.588]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.28it/s, train_loss=0.487]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.487]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.652]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.652]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.318]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.569]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.569]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.73] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.73]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 average loss: 0.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 8/100 [03:26<39:23, 25.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 8 current AUC: 0.9877 current accuracy: 0.8323 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.17it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.17it/s, train_loss=0.537]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.537]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.23it/s, train_loss=0.629]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.23it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.599]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.439]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.514]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.514]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.438]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.438]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.41] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.41]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.337]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.414]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.414]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.412]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.19it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.19it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.20it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.20it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.475]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.488]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.22it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.493]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.493]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.21it/s, train_loss=0.372]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.21it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.883]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.883]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 average loss: 0.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 9/100 [03:51<38:47, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 9 current AUC: 0.9874 current accuracy: 0.7019 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.351]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.455]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.445]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.19it/s, train_loss=0.445]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.19it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.21it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.21it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:19,  1.20it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:19,  1.20it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.396]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.449]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.602]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.602]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.444]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.444]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.545]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.545]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.484]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.521]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.521]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.408]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.564]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.564]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.304]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.28it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.411]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.411]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.32] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.32]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.373]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.421]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.407]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.407]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 average loss: 0.3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 10/100 [04:16<38:04, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 10 current AUC: 0.9828 current accuracy: 0.7950 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.344]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.344]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.328]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.435]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.435]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.464]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:15,  1.32it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:15,  1.32it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.31it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.31it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.30it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.30it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.358]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.355]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.355]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.27it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.30it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.30it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.30it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.30it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.30it/s, train_loss=0.386]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.30it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:17<00:06,  1.32it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.32it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.29it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.555]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.555]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.27it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.26it/s, train_loss=0.236]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.54it/s, train_loss=0.319]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 average loss: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 11/100 [04:41<37:14, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 11 current AUC: 0.9874 current accuracy: 0.8447 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.16it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.16it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.17it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.17it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.366]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.496]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:19,  1.21it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:19,  1.21it/s, train_loss=0.462]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.462]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.65] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.65]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.325]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:12,  1.25it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.516]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.516]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.26it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.507]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.545]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.545]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.332]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.546]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.546]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 average loss: 0.3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 12/100 [05:06<36:49, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 12 current AUC: 0.9887 current accuracy: 0.8571 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.401] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.401]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.298]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.653]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.653]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.385]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.47] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.31it/s, train_loss=0.47]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.31it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.424]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.297]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.392]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.30it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.30it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.30it/s, train_loss=0.329]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.30it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.61it/s, train_loss=0.394]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 average loss: 0.3127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 13/100 [05:30<36:14, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 13 current AUC: 0.9809 current accuracy: 0.7888 best AUC: 0.9890 at epoch: 7\n",
      "----------\n",
      "epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.463]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.345]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.451]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.451]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.409]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.343]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.284]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.281]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.349]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.349]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.367]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.295]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.28] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.28]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.235]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.3]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.3]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.238]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0957]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 average loss: 0.2725\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 14/100 [05:57<36:22, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 14 current AUC: 0.9941 current accuracy: 0.7950 best AUC: 0.9941 at epoch: 14\n",
      "----------\n",
      "epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.276]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.405]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.20it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.20it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.434]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.248]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.468]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.468]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.275]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.422]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.348]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.348]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.249]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.505]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.505]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 average loss: 0.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 15/100 [06:22<35:45, 25.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 15 current AUC: 0.9912 current accuracy: 0.7640 best AUC: 0.9941 at epoch: 14\n",
      "----------\n",
      "epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.296]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.436]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.436]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.309]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.271]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.33] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.27it/s, train_loss=0.33]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.27it/s, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.214] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.30it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.30it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.30it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.30it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.29it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.29it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.344]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.344]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:09,  1.22it/s, train_loss=0.314]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.21it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.21it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.452]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.452]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.198]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 average loss: 0.2587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|█▌        | 16/100 [06:46<35:08, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 16 current AUC: 0.9855 current accuracy: 0.7081 best AUC: 0.9941 at epoch: 14\n",
      "----------\n",
      "epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.364]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.334]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.334]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.34] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.34]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.425]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.299]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.471]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.222]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 average loss: 0.2350\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 17/100 [07:13<35:08, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 17 current AUC: 0.9981 current accuracy: 0.9193 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.34it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.34it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.287]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.256]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.266]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.669]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.669]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.19it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.19it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.397]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.397]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.303]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.162]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 average loss: 0.2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  18%|█▊        | 18/100 [07:37<34:31, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 18 current AUC: 0.9922 current accuracy: 0.8261 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.27it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.27it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.30it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.30it/s, train_loss=0.433]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.433]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.258]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.258]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.29it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.29it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.27it/s, train_loss=0.24] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.28it/s, train_loss=0.543]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.311]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.14] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.29it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.24it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.22it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.437]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.437]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 average loss: 0.2403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  19%|█▉        | 19/100 [08:02<33:51, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 19 current AUC: 0.9914 current accuracy: 0.8634 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.335]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.389]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.336]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.394]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.15] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.267]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.27] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.27]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.195]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 average loss: 0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 20/100 [08:27<33:19, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 20 current AUC: 0.9963 current accuracy: 0.9193 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.18] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.322]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.22it/s, train_loss=0.218]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.319]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.346]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.222]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.326]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.0985]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.219] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.219]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.42] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.42]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 average loss: 0.2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  21%|██        | 21/100 [08:52<32:51, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 21 current AUC: 0.9830 current accuracy: 0.7081 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.261]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.252]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.398]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.3]  \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.3]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.395]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.395]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.283]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.431]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0464]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.359] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.359]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.339]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.342]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.186]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.277]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.162]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 average loss: 0.2260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|██▏       | 22/100 [09:17<32:27, 24.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 22 current AUC: 0.9926 current accuracy: 0.8199 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.338]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.31it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.31it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.242]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.279]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.095]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.095]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.39] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.39]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.142] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.292]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.251]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.173]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 average loss: 0.1937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 23/100 [09:41<31:55, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 23 current AUC: 0.9971 current accuracy: 0.8323 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.294]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.313]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.375]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.18]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.18]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.286]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0968]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.108]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 average loss: 0.1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 24/100 [10:06<31:32, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 24 current AUC: 0.9893 current accuracy: 0.8509 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.0902]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.30it/s, train_loss=0.0902]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.30it/s, train_loss=0.181] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.33it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.33it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.31it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.31it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:17,  1.29it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=0.191]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.31it/s, train_loss=0.383]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.31it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.212]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.26it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.245]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.278]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:09,  1.25it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.369] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.369]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:17<00:06,  1.31it/s, train_loss=0.263]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.31it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.31it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.31it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.28it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.416]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:21<00:02,  1.26it/s, train_loss=0.416]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.28it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.58it/s, train_loss=0.2]\u001b[A\n",
      "                                                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 average loss: 0.1955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 25/100 [10:31<30:54, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 25 current AUC: 0.9764 current accuracy: 0.7391 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.25it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.25it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.185]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.289]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:17,  1.23it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.216]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.273]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0944]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.0944]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.19]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.19]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.165]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 average loss: 0.1626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|██▌       | 26/100 [10:56<30:31, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 26 current AUC: 0.9925 current accuracy: 0.7950 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.16it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.16it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.327]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.202]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.0885]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0885]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.176] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0933]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.0933]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.214]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.23] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.379]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0996]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.173] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.21it/s, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.21it/s, train_loss=0.201] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.201]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.243]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.0649]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.0649]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.334] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.334]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 average loss: 0.1571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 27/100 [11:21<30:11, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 27 current AUC: 0.9930 current accuracy: 0.9193 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0321]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.0321]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.169] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.181] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.0859]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0859]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0868]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.107] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.264]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.163]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.257]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.246]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.376]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0997]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0997]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 average loss: 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|██▊       | 28/100 [11:45<29:47, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 28 current AUC: 0.9874 current accuracy: 0.8571 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.29it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.29it/s, train_loss=0.156] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.27it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.27it/s, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.166] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.152]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0856]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.155] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.244]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.0661]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.0661]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.167] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.233]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0633]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.113] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.20it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.20it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0508]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.315] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.315]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 average loss: 0.1428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  29%|██▉       | 29/100 [12:10<29:24, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 29 current AUC: 0.9789 current accuracy: 0.8509 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0774]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0774]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0971]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0971]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.177] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.11] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.29it/s, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.0875]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0964]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.23]  \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.23]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.123] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0691]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.149] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.226]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0737]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.206] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.206]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0972]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0534]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0534]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0524]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0524]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.203] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.203]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0946]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.0946]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.141]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 average loss: 0.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 30/100 [12:35<28:59, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 30 current AUC: 0.9954 current accuracy: 0.9255 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.0747]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.0747]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.088] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.088]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.374]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.21it/s, train_loss=0.374]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:20,  1.19it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:20,  1.19it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.22it/s, train_loss=0.0418]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0418]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0472]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.0472]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.0956]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0956]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.148] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.305]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0636]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0636]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.184] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.28it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.237]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0939]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.0855]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0626]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0626]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.176] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0749]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0749]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.261] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.261]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 average loss: 0.1249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  31%|███       | 31/100 [13:00<28:35, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 31 current AUC: 0.9961 current accuracy: 0.8820 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.0653]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.0653]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.037] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.037]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.0767]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:20,  1.30it/s, train_loss=0.0739]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.0739]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.272] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.272]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.30it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.30it/s, train_loss=0.0463]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:16,  1.30it/s, train_loss=0.0463]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.207]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.26it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.282] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.282]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.143] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.15]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.15]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.2] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.22it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0536]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0536]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.29]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.204]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.204]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 average loss: 0.1204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|███▏      | 32/100 [13:25<28:09, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 32 current AUC: 0.9903 current accuracy: 0.8261 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0947]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.0947]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.0337]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.0337]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.30it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.30it/s, train_loss=0.124] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.124]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0771]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:21,  1.23it/s, train_loss=0.0771]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.128] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0852]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.151] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.208]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0638]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0638]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.082]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.082]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.239]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0643]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.164] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.132] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.22it/s, train_loss=0.148]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.19it/s, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.19it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.20it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.20it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.20it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.29] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.29]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 average loss: 0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 33/100 [13:50<27:48, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 33 current AUC: 0.9953 current accuracy: 0.8509 best AUC: 0.9981 at epoch: 17\n",
      "----------\n",
      "epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.139]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.0818]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.29it/s, train_loss=0.0818]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.29it/s, train_loss=0.0841]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0841]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.132] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.259] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:17,  1.23it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.21it/s, train_loss=0.0548]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.21it/s, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.21it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.159] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0884]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0884]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.107] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0541]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.0541]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0854]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0517]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.0517]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.147] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.078] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.078]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.179]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.143]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 average loss: 0.1094\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|███▍      | 34/100 [14:16<27:47, 25.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 34 current AUC: 0.9983 current accuracy: 0.9441 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0707]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.0707]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.0898]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.0898]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.184] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.107] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.0786]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.205]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0921]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.27it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.27it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.098] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.098]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.197]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.0534]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.0534]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.057] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.057]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0913]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0913]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.165] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0879]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.234] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.234]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0771]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.0771]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.211] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.211]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.16] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0619]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.0619]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0917]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0917]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 average loss: 0.1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 35/100 [14:41<27:14, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 35 current AUC: 0.9928 current accuracy: 0.8820 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.32it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.32it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.168]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.0455]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0455]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:06<00:16,  1.30it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:16,  1.30it/s, train_loss=0.227] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0549]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.596] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.596]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.0919]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0901]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.164] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.301]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.0954]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.27it/s, train_loss=0.232]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.0446]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.57it/s, train_loss=0.0446]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 average loss: 0.1434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 36/100 [15:05<26:38, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 36 current AUC: 0.9948 current accuracy: 0.9068 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.213] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.228]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0541]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0541]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.217] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.29it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.29it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0694]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0694]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0494]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0988]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0988]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0378]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0378]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.127] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.079]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.079]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=0.0998]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.0998]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.29it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.29it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0638]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0638]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.0826]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0826]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 average loss: 0.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 37/100 [15:30<26:10, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 37 current AUC: 0.9934 current accuracy: 0.8944 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.17it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.17it/s, train_loss=0.0781]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.0781]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0285]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0285]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.217] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.217]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.055]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.055]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.076]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.076]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.198] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.0864]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0425]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0425]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0763]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0763]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0357]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.113] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0387]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.0387]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.177] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.177]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0233]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.0233]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.173] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.173]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.041] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.109]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 average loss: 0.0896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|███▊      | 38/100 [15:55<25:46, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 38 current AUC: 0.9937 current accuracy: 0.9006 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0615]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.0615]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0699]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.0699]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.0371]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.0371]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.195] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.195]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0418]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.0418]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.209] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.209]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.0613]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.20it/s, train_loss=0.0613]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.20it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:15,  1.20it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:15,  1.20it/s, train_loss=0.199] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.21it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.21it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0986]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.0289]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.29it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.151]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.2]  \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00918]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.00918]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.055]  \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.055]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.0151]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.335] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.335]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 average loss: 0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  39%|███▉      | 39/100 [16:20<25:21, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 39 current AUC: 0.9970 current accuracy: 0.9379 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.274]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.147] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0336]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.044] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.044]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.187] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.187]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0546]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.0546]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.132] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.20it/s, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.20it/s, train_loss=0.0561]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.0561]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.22it/s, train_loss=0.0453]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.0453]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0255]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0255]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0792]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0792]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.25it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0807]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0848]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0686]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.102]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 average loss: 0.0847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 40/100 [16:45<25:00, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 40 current AUC: 0.9926 current accuracy: 0.8509 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.183] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.0614]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.20it/s, train_loss=0.0614]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.20it/s, train_loss=0.0387]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.23it/s, train_loss=0.0387]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.23it/s, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0226]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.02]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.02]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.0417]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.0417]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.019]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.019]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0454]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.141] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0287]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0287]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0544]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0955]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.189] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.122] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.122]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.20it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0825]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0825]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 average loss: 0.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  41%|████      | 41/100 [17:10<24:35, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 41 current AUC: 0.9894 current accuracy: 0.8137 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.0316]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.0316]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.27it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0673]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0673]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.29it/s, train_loss=0.0518]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.29it/s, train_loss=0.0753]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0753]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0233]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.0233]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.058] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.22it/s, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.0778]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0778]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.023] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.023]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0588]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0588]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.0479]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.0506]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0506]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.048] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.029] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.029]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.063] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.101]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 average loss: 0.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|████▏     | 42/100 [17:35<24:11, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 42 current AUC: 0.9950 current accuracy: 0.9006 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0121]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.0121]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.07] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.07]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.0813]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0813]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.142] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0832]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.011] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.149]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.0286] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.27it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.27it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.0849]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.0748]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.164] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0975]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0975]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0186]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.11]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 average loss: 0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 43/100 [18:00<23:44, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 43 current AUC: 0.9975 current accuracy: 0.9441 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.25it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.25it/s, train_loss=0.0579]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0579]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.0725]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.143] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0472]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.0472]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.046] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.069]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.069]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.29it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.268] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.31it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.31it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0538]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.0539]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.30it/s, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.30it/s, train_loss=0.0842]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.0842]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.30it/s, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.26it/s, train_loss=0.0525]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.171] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:22<00:01,  1.25it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.251] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:23<00:00,  1.55it/s, train_loss=0.251]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 average loss: 0.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|████▍     | 44/100 [18:25<23:13, 24.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 44 current AUC: 0.9822 current accuracy: 0.8447 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.07]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.21it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.21it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0781]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0781]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0948]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.285] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.285]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.254]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.28it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.28it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.11]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0614]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.0614]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.356] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.356]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0902]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.113] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.113]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 average loss: 0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 45/100 [18:50<22:49, 24.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 45 current AUC: 0.9696 current accuracy: 0.6149 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.171]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.125] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.125]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.0333]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0384]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0682]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.0682]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.28it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.28it/s, train_loss=0.192] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0537]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.0537]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.14]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0778]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.0778]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.0505]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0505]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.088] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.088]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.0655]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.0418]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:09,  1.22it/s, train_loss=0.0418]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.0516]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.031] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.123] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.1]   \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0414]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0835]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0267]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 average loss: 0.0764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|████▌     | 46/100 [19:15<22:24, 24.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 46 current AUC: 0.9941 current accuracy: 0.9255 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.0427]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.0693]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0693]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0821]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0821]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0599]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0859]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0859]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.164] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.164]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0434]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.0434]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.044] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.044]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0567]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0567]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.0694]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0694]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0279]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0279]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.131] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.131]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.1]  \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.1]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.0576]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.0576]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0207]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0765]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.0765]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0396]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0521]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0935]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0935]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.0306]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 average loss: 0.0633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 47/100 [19:40<21:58, 24.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 47 current AUC: 0.9962 current accuracy: 0.8944 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.032]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.032]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.00825]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.00825]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.0205] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.0709]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0625]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0745]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.07]  \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.07]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.00628]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0353]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.0353]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0566]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.0566]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.0615]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.162] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.28it/s, train_loss=0.162]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0784]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.118] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.262]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.123] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0434]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0434]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.144] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.144]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.189]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.189]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 average loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|████▊     | 48/100 [20:04<21:31, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 48 current AUC: 0.9863 current accuracy: 0.8385 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.015]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.015]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.023] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.023]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.17] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.074] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.074]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.00455]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.00455]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.118]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.00911]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.00911]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0245] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.21it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.21it/s, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.0527]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.153] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.0715]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.0715]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0658]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0658]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0482]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.0659]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.012] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.012]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.302]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.0403]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0473]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 average loss: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  49%|████▉     | 49/100 [20:29<21:08, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 49 current AUC: 0.9852 current accuracy: 0.8758 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.096]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0444]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.28it/s, train_loss=0.0444]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.28it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.0222]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0222]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.29it/s, train_loss=0.155] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.28it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.0583] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.0309]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0827]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.23it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:06,  1.29it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:06,  1.29it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.0853]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.134] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.181]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.155]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0492]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.0492]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.0269]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0269]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 average loss: 0.0651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 50/100 [20:54<20:41, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 50 current AUC: 0.9942 current accuracy: 0.9193 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0612]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.0612]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.0367]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.19it/s, train_loss=0.0367]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.19it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.20it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.20it/s, train_loss=0.0568]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.20it/s, train_loss=0.0568]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.20it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.00695]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.00695]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0291] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.0291]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.22it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.175] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.27it/s, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.29it/s, train_loss=0.0532]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.29it/s, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.00513]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.00513]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0284] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0337]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.04]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.04]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0462]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0462]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0371]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0371]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 average loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  51%|█████     | 51/100 [21:19<20:18, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 51 current AUC: 0.9937 current accuracy: 0.9379 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0326]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.0326]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.0755]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.00992]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.00992]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.0132] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.0288]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.028] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.24it/s, train_loss=0.028]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.24it/s, train_loss=0.0772]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0772]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.0676]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.019] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.019]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.13] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.13]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.00958]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.00958]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0293] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.00787]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.00787]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.21it/s, train_loss=0.0159] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.20it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.20it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.012] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.012]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.26it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0565]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.0359]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.00715]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.00715]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0712]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.0712]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 average loss: 0.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|█████▏    | 52/100 [21:44<19:58, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 52 current AUC: 0.9957 current accuracy: 0.9379 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0788]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.0788]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0355]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.0355]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.137] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0278]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.00569]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.01]   \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.01]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.00502]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.00502]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0171] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.174] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.174]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0531]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0684]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0684]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0489]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.156] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0696]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0696]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0485]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.0485]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0466]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0466]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.039] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.039]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.00359]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.00359]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0574] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0574]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0268]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 average loss: 0.0480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 53/100 [22:09<19:35, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 53 current AUC: 0.9952 current accuracy: 0.9317 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0561]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.0561]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.0752]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0509]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0366]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0208]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.2]   \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.00492]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.00492]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.0256] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0606]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.018] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.00396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.00396]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.0284] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.081] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.081]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.011] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.00564]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.00564]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.19]   \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.19]\u001b[A\n",
      "                                                                                  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 average loss: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|█████▍    | 54/100 [22:34<19:10, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 54 current AUC: 0.9935 current accuracy: 0.8882 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.138]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.00634]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.00634]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0182] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.00688]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.00688]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0129] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0492]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0492]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0172]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0398]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.0629]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0635]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0635]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.0303]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.0303]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.0385]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.24]  \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.20it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.20it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0372]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.0397]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.306] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.306]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.142]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.29it/s, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.29it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.0451]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.29it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.407] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.407]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 average loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 55/100 [22:59<18:43, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 55 current AUC: 0.9944 current accuracy: 0.9068 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0129]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.0129]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0764]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.147] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.093] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0196]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.00771]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.00771]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.134]  \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.0476]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.0476]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.0522]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.0522]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.04]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.04]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.022] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.022]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.165] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.0448]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.184] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.184]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0471]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.146] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.022]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.022]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 average loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|█████▌    | 56/100 [23:24<18:18, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 56 current AUC: 0.9929 current accuracy: 0.8820 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.111] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0503]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0503]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0422]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.0422]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.0389]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0075]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.0075]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.034] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.034]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.00661]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.21it/s, train_loss=0.00661]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s, train_loss=0.0237] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.021] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.011] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.029]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.029]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.00341]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.00341]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0257] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.07]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.07]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.147]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 average loss: 0.0467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  57%|█████▋    | 57/100 [23:49<17:53, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 57 current AUC: 0.9973 current accuracy: 0.9379 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.32it/s, train_loss=0.0254]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.32it/s, train_loss=0.02]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.02]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.194] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.24it/s, train_loss=0.194]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.00947]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.00947]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.00885]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.00885]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0673] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0673]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.315] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.315]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.159]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.127]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.0677]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.0497]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.20it/s, train_loss=0.0497]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.20it/s, train_loss=0.0802]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.0802]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.20it/s, train_loss=0.00725]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.21it/s, train_loss=0.00725]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s, train_loss=0.0419] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.21it/s, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.21it/s, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.0299]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.0831]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0831]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.253] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.253]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0812]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0275]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.0994]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0994]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.123] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.078]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.078]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0601]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.076] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.076]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0295]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0295]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.261] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.261]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 average loss: 0.0813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  58%|█████▊    | 58/100 [24:14<17:29, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 58 current AUC: 0.9944 current accuracy: 0.9193 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.08]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.21it/s, train_loss=0.08]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.21it/s, train_loss=0.0519]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.0519]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0481]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0493]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.0781]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0781]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.053] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.053]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.317]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.0674]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0674]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.146] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.146]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.058] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.056] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.056]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.0277]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.193] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.193]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.133]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0392]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0392]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.21]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0325]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.175] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.175]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0756]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.0756]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0113]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.0113]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59 average loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  59%|█████▉    | 59/100 [24:39<17:05, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 59 current AUC: 0.9925 current accuracy: 0.8944 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0714]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0714]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:19,  1.30it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:19,  1.30it/s, train_loss=0.0729]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.0729]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.0535]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.0535]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.0363]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.28it/s, train_loss=0.0363]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.28it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0519]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=0.0519]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.28it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.28it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.2]   \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:13,  1.23it/s, train_loss=0.2]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0334]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0334]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0524]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0524]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.196] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.196]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.25it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0411]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.0411]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.21it/s, train_loss=0.0184]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.0617]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.50it/s, train_loss=0.0617]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 average loss: 0.0493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 60/100 [25:04<16:39, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 60 current AUC: 0.9947 current accuracy: 0.8820 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.0641]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.0641]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.032] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.032]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.172] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0199]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.147] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.0806]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.017] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.00294]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0243] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.032] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.032]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.21it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.123] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.123]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.0627]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.128] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0393]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.154] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.154]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.50it/s, train_loss=0.0225]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61 average loss: 0.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  61%|██████    | 61/100 [25:29<16:16, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 61 current AUC: 0.9906 current accuracy: 0.8696 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.16it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.16it/s, train_loss=0.0375]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.21it/s, train_loss=0.0375]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.21it/s, train_loss=0.0364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0364]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0486]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.103] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.103]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0326]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0566]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.0566]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.00953]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.24it/s, train_loss=0.00953]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.24it/s, train_loss=0.0241] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.061] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.061]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0926]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.16]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.16]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.00609]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.00609]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.0906] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0906]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.20it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.20it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.21it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.21it/s, train_loss=0.00371]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.00371]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.24it/s, train_loss=0.0368] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.134] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0463]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0463]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0139]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.0163]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0331]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.0685]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.00421]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.00421]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0692] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.262] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.262]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62 average loss: 0.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▏   | 62/100 [25:54<15:52, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 62 current AUC: 0.9918 current accuracy: 0.8944 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.013]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.013]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.00278]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.00278]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0123] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0586]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.00769]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.00769]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.019]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.019]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.00749]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.00749]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0314] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0314]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0828]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.00973]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.00973]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0401] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.20it/s, train_loss=0.0401]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.20it/s, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.0635]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0635]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0588]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0588]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0529]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0595]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.00518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.00518]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0731] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0731]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0407]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0768]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0768]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0059]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0059]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0358]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.0441]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0533]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.0063]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0063]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.0632]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63 average loss: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  63%|██████▎   | 63/100 [26:19<15:26, 25.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 63 current AUC: 0.9876 current accuracy: 0.8634 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.012] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.012]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.00697]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.00697]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.0883] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0883]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.29it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.29it/s, train_loss=0.0554]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.29it/s, train_loss=0.0554]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.29it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.0318]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.008] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.008]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00626]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.00626]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.048]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.039] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.24it/s, train_loss=0.039]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.00782]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.00782]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.00226]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.00226]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0399] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.29it/s, train_loss=0.0399]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.29it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.0631]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.0631]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.029] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.029]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.101]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.0426]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00954]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.00954]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0219] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.013] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.013]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.101] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.101]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 average loss: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  64%|██████▍   | 64/100 [26:44<14:59, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 64 current AUC: 0.9937 current accuracy: 0.9255 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00533]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.00533]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0226] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.0226]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.20it/s, train_loss=0.0449]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.20it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0744]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.20it/s, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.20it/s, train_loss=0.143] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.143]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.00684]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.00684]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.00927]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.00927]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.0118] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.00607]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.00607]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.114]  \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0678]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.00637]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0189] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.0354]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.182] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.28it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.28it/s, train_loss=0.00549]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.00549]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0132] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.0616]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0888]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0888]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00407]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.00407]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.00668]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.00668]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.0236] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0341]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0104]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 average loss: 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 65/100 [27:09<14:34, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 65 current AUC: 0.9950 current accuracy: 0.8944 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0669]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.20it/s, train_loss=0.0669]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.20it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0046]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0046]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.00763]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.00763]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.014]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0513]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.182] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.182]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.00685]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.00685]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.00381]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.00381]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0107] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.26it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.014] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.00419]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.00419]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.00621]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.00621]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.0162] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0161]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.00829]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.00829]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0478] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0478]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0837]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0528]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.01]  \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.01]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0276]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.00273]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.00273]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.0641] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0641]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.137] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.137]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.00715]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.00715]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0229] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.00668]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.25it/s, train_loss=0.00668]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0731] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0731]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66 average loss: 0.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  66%|██████▌   | 66/100 [27:34<14:08, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 66 current AUC: 0.9929 current accuracy: 0.8820 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0374]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.0374]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.033]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.033]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.178]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.30it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.30it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:17,  1.29it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:17,  1.29it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0343]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0465]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.00745]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.00745]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.00631]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.00479]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.00479]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.0137] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.147] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.147]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.00156]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.00156]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.071]  \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.071]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.00451]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.00451]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0495] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0495]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.0024] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0024]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.26it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.0712]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0712]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.116] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.116]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0452]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.0452]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 average loss: 0.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 67/100 [27:59<13:42, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 67 current AUC: 0.9964 current accuracy: 0.9317 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0178]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0178]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.17it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.17it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:24,  1.16it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:24,  1.16it/s, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.18it/s, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.18it/s, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.00972]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:21,  1.20it/s, train_loss=0.0424] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.21it/s, train_loss=0.0356]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.21it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.22it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:17,  1.22it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.00808]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.00808]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.0133] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0133]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.00291]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.00291]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:12,  1.23it/s, train_loss=0.0284] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0284]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.21it/s, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.21it/s, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.20it/s, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.20it/s, train_loss=0.0759] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.21it/s, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.21it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.22it/s, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:18<00:08,  1.24it/s, train_loss=0.0112] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0112]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.00432]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.00432]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0808] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0808]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.0818]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0818]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.26it/s, train_loss=0.114] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.114]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:22<00:03,  1.25it/s, train_loss=0.00652]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.00652]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0148] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.26it/s, train_loss=0.169] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.169]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0507]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0507]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 average loss: 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  68%|██████▊   | 68/100 [28:24<13:20, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 68 current AUC: 0.9960 current accuracy: 0.9317 best AUC: 0.9983 at epoch: 34\n",
      "----------\n",
      "epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00319]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.00319]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.0125] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.0095]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0095]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.00409]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.00409]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0121] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0121]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0447]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0221]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.29it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.29it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.00765]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.00765]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0174] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.28it/s, train_loss=0.00419]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.00419]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.0268] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.00879]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.29it/s, train_loss=0.00879]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.29it/s, train_loss=0.0187] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.31it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.31it/s, train_loss=0.00331]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.32it/s, train_loss=0.00331]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.32it/s, train_loss=0.0071] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:14<00:08,  1.34it/s, train_loss=0.0071]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:08,  1.34it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.31it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.31it/s, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.27it/s, train_loss=0.0889]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.27it/s, train_loss=0.00804]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.00804]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0099] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.0099]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.20it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.20it/s, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:05,  1.19it/s, train_loss=0.0239]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:05,  1.19it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.19it/s, train_loss=0.0122]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.19it/s, train_loss=0.00686]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.00686]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.00667]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.00667]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0367] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0367]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.0874]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.51it/s, train_loss=0.0874]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 average loss: 0.0199\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  69%|██████▉   | 69/100 [28:50<13:05, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 69 current AUC: 0.9989 current accuracy: 0.9565 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.00625]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.00625]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.0191] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.00975]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.00975]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0297] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0267]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.00277]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.00277]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0429] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0033]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.0033]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.0782]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0603]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0413]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.00741]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.00741]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.00475]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.00475]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.118]  \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.118]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.0515]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.0515]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.00567]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.00567]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.21it/s, train_loss=0.0127] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.00761]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.00761]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.104]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.00974]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.00974]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.0573] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.22it/s, train_loss=0.00378]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.00378]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70 average loss: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 70/100 [29:15<12:38, 25.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 70 current AUC: 0.9882 current accuracy: 0.8944 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0321]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.17it/s, train_loss=0.0321]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.17it/s, train_loss=0.00289]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.00289]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.0555] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0555]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.23it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.23it/s, train_loss=0.0106] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.00808]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.00808]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.0137] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.00811]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.00811]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0242] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0374]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0038]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0038]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.00317]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.00317]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0225] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0387]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0387]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.00952]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.00952]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.00917]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.00917]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.0461] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.00784]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.00784]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.255]  \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0327]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0327]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0542]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0064]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.29it/s, train_loss=0.0064]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.29it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0514]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.0098]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.00551]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.00551]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0409] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.0409]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0135]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71 average loss: 0.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  71%|███████   | 71/100 [29:40<12:09, 25.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 71 current AUC: 0.9948 current accuracy: 0.8944 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.00464]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.00464]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.0339] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.00723]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.00723]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0328] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.0328]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00333]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.00333]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.00891]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.27it/s, train_loss=0.00891]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.27it/s, train_loss=0.00396]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.00396]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0293] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.0246]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.00513]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.00513]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.0262] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.00313]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.00313]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0604] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0604]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.00757]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.0135] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.00258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.00258]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.00942]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.00942]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.0799] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0799]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0758]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0758]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.00844]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.00844]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72 average loss: 0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  72%|███████▏  | 72/100 [30:05<11:42, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 72 current AUC: 0.9976 current accuracy: 0.9379 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.23it/s, train_loss=0.0368]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.23it/s, train_loss=0.0752]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.19it/s, train_loss=0.0752]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.19it/s, train_loss=0.102] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.102]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.157] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0157] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.107] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.107]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.27it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.27it/s, train_loss=0.037] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.037]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.28it/s, train_loss=0.0141]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.29it/s, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.29it/s, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.32it/s, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.32it/s, train_loss=0.016]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.31it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:09,  1.31it/s, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.0344] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.0344]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.00919]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.00919]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0169] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0169]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0261]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.24it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0788]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.0788]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0869]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0814]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.0814]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 average loss: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  73%|███████▎  | 73/100 [30:30<11:15, 25.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 73 current AUC: 0.9973 current accuracy: 0.9317 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.00935]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.00935]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.0274] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.0526]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0303] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0303]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.00384]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.00384]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.0834] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0834]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0882]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.0882]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00505]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.00505]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.00586]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.00586]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0135] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.27it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0573]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.0728]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0728]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0282]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.00323]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.00323]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0887] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.0887]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0313]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.00316]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0165] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.20it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.20it/s, train_loss=0.156] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.0094]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0094]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.017] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.017]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74 average loss: 0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  74%|███████▍  | 74/100 [30:55<10:51, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 74 current AUC: 0.9962 current accuracy: 0.9441 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00427]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.00427]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.17]   \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.0978]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.00478]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.23it/s, train_loss=0.00478]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.23it/s, train_loss=0.11]   \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.11]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.00811]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.00811]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.0258] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.21it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.21it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.046] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00782]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.00782]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.0138] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0138]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0721]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0721]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.22it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.22it/s, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.0634]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.21it/s, train_loss=0.129] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.141]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.0175]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0175]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.25it/s, train_loss=0.117] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.117]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.00259]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.00259]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0283] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0283]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0087]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0087]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.22it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.00977]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.00977]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0118] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.28it/s, train_loss=0.0183]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0183]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.135]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75 average loss: 0.0501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  75%|███████▌  | 75/100 [31:20<10:26, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 75 current AUC: 0.9892 current accuracy: 0.7950 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.0583]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.00644]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:21,  1.33it/s, train_loss=0.00644]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:21,  1.33it/s, train_loss=0.0716] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0716]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.307] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.307]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.0769]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0769]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.0654]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.136] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.136]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.227]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0597]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.108] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.108]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.0957]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.112] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.112]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.229]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.00843]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.00843]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.26it/s, train_loss=0.0253] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.00749]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.00749]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0166] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.0317]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0681]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.043] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.043]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.00897]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.00897]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.26it/s, train_loss=0.0473] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0717]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0717]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76 average loss: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  76%|███████▌  | 76/100 [31:45<10:00, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 76 current AUC: 0.9966 current accuracy: 0.8944 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.24]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.0213]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.31it/s, train_loss=0.0213]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.31it/s, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.29it/s, train_loss=0.0656]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.29it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.225] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:21,  1.23it/s, train_loss=0.225]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.00817]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.00817]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.111]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.111]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0558]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0097]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0097]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0923]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.0923]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0231]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.0217]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.0386]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0553]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.022] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.022]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.113]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.0412]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.014] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0166]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.00979]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.00979]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.00868]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.00868]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0252] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0252]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.21it/s, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.21it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.0534]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.20it/s, train_loss=0.0534]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.20it/s, train_loss=0.119] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.119]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.20it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.20it/s, train_loss=0.158]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.20it/s, train_loss=0.0799]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.49it/s, train_loss=0.0799]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77 average loss: 0.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  77%|███████▋  | 77/100 [32:10<09:36, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 77 current AUC: 0.9945 current accuracy: 0.9255 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0215]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:26,  1.13it/s, train_loss=0.0215]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:26,  1.13it/s, train_loss=0.00979]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.21it/s, train_loss=0.00979]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.21it/s, train_loss=0.0241] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0241]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.00724]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.00724]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.0115] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.00822]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.00389]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.00389]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.0293] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.28it/s, train_loss=0.0797]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.27it/s, train_loss=0.0797]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.27it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.28it/s, train_loss=0.0361]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.28it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0268]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.00684]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.00684]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.00318]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.00318]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.0177] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.00668]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.00668]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.22it/s, train_loss=0.0475] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0475]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.153] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.26it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.26it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.00201]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.00201]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.28it/s, train_loss=0.0102] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.00721]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.00721]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0048] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.0048]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.0071]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.0071]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.00469]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.00469]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.00945]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.00945]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.369]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.369]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78 average loss: 0.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  78%|███████▊  | 78/100 [32:35<09:09, 24.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 78 current AUC: 0.9963 current accuracy: 0.9317 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00474]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.00474]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.018]  \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.23it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.23it/s, train_loss=0.0773]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0773]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0492]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0492]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.00965]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.115]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0125]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.027] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.027]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.128] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.128]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.132]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0272]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0572]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0637]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.0637]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.014] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.25] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.25]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.29it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.188]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.0628]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0433]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.013] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.013]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.0866]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.29it/s, train_loss=0.0477]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.29it/s, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0154]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0862]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.0862]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.0298]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.0298]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79 average loss: 0.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  79%|███████▉  | 79/100 [33:00<08:42, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 79 current AUC: 0.9946 current accuracy: 0.8571 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.015]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.015]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.0694]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.0694]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.0824]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0824]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0191]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.00557]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.00557]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.0215] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.0215]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.00671]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.00919]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.00919]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.00444]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:17,  1.23it/s, train_loss=0.00444]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0442] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0442]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0237]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.00867]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.00867]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.0227] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.00914]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.00914]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0613] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0613]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.00153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.00153]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0152] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0152]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.0377]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.0075]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0075]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.0167]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0307]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.00371]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00371]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0113] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0234]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.00537]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.00537]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0134] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0134]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0113]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0113]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 average loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 80/100 [33:25<08:18, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 80 current AUC: 0.9961 current accuracy: 0.9441 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.31it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.31it/s, train_loss=0.0537]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.0537]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.00695]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.26it/s, train_loss=0.00695]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.26it/s, train_loss=0.0176] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.0546]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.0546]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.22it/s, train_loss=0.0943]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0943]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.057] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.22it/s, train_loss=0.057]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.22it/s, train_loss=0.00462]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.00462]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.0103] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.0103]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.011] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.28it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.199]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.0216]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0959]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.00998]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.00998]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.00672]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.00672]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.00588]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.00588]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0316] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.115] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.115]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0974]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0974]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.145] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.26it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s, train_loss=0.00818]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00818]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0609] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.0609]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0897]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.172] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.172]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.00744]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.00744]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0265] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0508]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.0508]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81 average loss: 0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  81%|████████  | 81/100 [33:50<07:54, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 81 current AUC: 0.9883 current accuracy: 0.8199 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.24it/s, train_loss=0.00366]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.00366]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.00734]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.00734]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0144] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0213]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.21it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0365]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.22it/s, train_loss=0.0365]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.22it/s, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.0703] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0703]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0263]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.23it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.093] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.093]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.161] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0679]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.0679]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.23it/s, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0453]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.22it/s, train_loss=0.0453]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.0408]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.00481]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.21it/s, train_loss=0.00481]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.21it/s, train_loss=0.166]  \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:09,  1.21it/s, train_loss=0.00402]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.00402]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.21it/s, train_loss=0.0312] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.00439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.00439]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0243] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0311]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.27it/s, train_loss=0.0328]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0328]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.0293]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.027] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.027]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.00448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.00448]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0171] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0171]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 average loss: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|████████▏ | 82/100 [34:15<07:30, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 82 current AUC: 0.9973 current accuracy: 0.9379 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.00277]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.00277]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.00818]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.23it/s, train_loss=0.00818]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.23it/s, train_loss=0.0394] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.24it/s, train_loss=0.0394]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.24it/s, train_loss=0.126] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.126]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.00384]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.00384]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.00389]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.00389]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.00299]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.00299]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.00414]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.00414]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.00166]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00166]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0319] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0319]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0061]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.0061]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.00219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.23it/s, train_loss=0.00219]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.23it/s, train_loss=0.00349]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.00349]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.0198] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.29it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.29it/s, train_loss=0.0026]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:10,  1.28it/s, train_loss=0.0026]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:10,  1.28it/s, train_loss=0.05]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.05]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.00681]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0302] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.00509]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.00509]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.0376] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.27it/s, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.27it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.28it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.28it/s, train_loss=0.00834]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.00834]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.29it/s, train_loss=0.0164] \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.30it/s, train_loss=0.0164]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.30it/s, train_loss=0.00371]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.00371]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0108] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.00616]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.00616]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.00218]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.00218]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0302] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.28it/s, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.034] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.60it/s, train_loss=0.034]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83 average loss: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  83%|████████▎ | 83/100 [34:40<07:03, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 83 current AUC: 0.9963 current accuracy: 0.9317 best AUC: 0.9989 at epoch: 69\n",
      "----------\n",
      "epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00996]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.00996]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.00393]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.00393]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0222] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.24it/s, train_loss=0.0222]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.24it/s, train_loss=0.0033]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0033]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.00224]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.00224]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.00268]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.00268]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.0264] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0264]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.00911]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.00911]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0432] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.0432]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.0872]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.0872]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.00864]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.00864]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.166]  \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.0885]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.25it/s, train_loss=0.0885]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.156] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.26it/s, train_loss=0.156]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.26it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.00353]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.00353]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0129] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.0798]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:09,  1.21it/s, train_loss=0.0522]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.0522]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.031] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0174]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0218]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.0145]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0419]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.00908]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.00908]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.0116] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0302]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.476] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.476]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84 average loss: 0.0497\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  84%|████████▍ | 84/100 [35:06<06:45, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model\n",
      "current epoch: 84 current AUC: 0.9993 current accuracy: 0.9441 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0255]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.0255]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.00742]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.00742]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.00803]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.28it/s, train_loss=0.00803]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.28it/s, train_loss=0.0329] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.0329]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.26it/s, train_loss=0.0925]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0925]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.0936]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.0936]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.75]  \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.26it/s, train_loss=0.75]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.26it/s, train_loss=0.0064]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.0064]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0273]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0342]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0589]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0589]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0232]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0232]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0461]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.0203]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.441] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.441]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.21] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.21]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.166]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.198]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0962]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0962]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.018] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.27it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.27it/s, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0949]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0636]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0636]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.035] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.035]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.22it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.22it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.25it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.0327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0327]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.00955]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0658] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.27it/s, train_loss=0.0658]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0101]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85 average loss: 0.0913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  85%|████████▌ | 85/100 [35:31<06:17, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 85 current AUC: 0.9893 current accuracy: 0.8820 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00887]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.20it/s, train_loss=0.00887]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.20it/s, train_loss=0.0801] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.24it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.24it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.259] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.259]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:21,  1.25it/s, train_loss=0.12] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.23it/s, train_loss=0.352]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.23it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.21it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.21it/s, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.0382]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0741]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0741]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.161] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.161]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.26it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.25it/s, train_loss=0.0189]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.25it/s, train_loss=0.00689]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.00689]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0286] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.27it/s, train_loss=0.0286]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.27it/s, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0428]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.069] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.069]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.0423]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0915]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.099] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.099]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0688]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0746]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0722]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0639]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.22it/s, train_loss=0.0117]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.22it/s, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0113]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.025] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.025]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.00978]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0524] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0524]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.25it/s, train_loss=0.0294]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.55it/s, train_loss=0.0294]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86 average loss: 0.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|████████▌ | 86/100 [35:56<05:51, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 86 current AUC: 0.9937 current accuracy: 0.9255 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00723]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:24,  1.22it/s, train_loss=0.00723]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:24,  1.22it/s, train_loss=0.00507]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.00507]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.00395]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.00395]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.0391] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.23it/s, train_loss=0.0391]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.23it/s, train_loss=0.00899]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.00899]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.22it/s, train_loss=0.083]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.21it/s, train_loss=0.083]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.21it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:20,  1.19it/s, train_loss=0.0332]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:20,  1.19it/s, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.0557]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.24it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:16,  1.24it/s, train_loss=0.00894]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.00894]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.21it/s, train_loss=0.00526]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.00526]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0132] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0404]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.046] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:12,  1.23it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0148]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0736]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.0736]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0469]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.21it/s, train_loss=0.0469]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.21it/s, train_loss=0.046] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.046]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.24it/s, train_loss=0.00345]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.00345]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.00936]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.00567]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.00567]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0225] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.0225]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.00928]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00928]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.24it/s, train_loss=0.0306] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0306]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.026] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.24it/s, train_loss=0.026]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.24it/s, train_loss=0.00485]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.25it/s, train_loss=0.00485]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.25it/s, train_loss=0.145]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.145]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.00912]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.00912]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.29it/s, train_loss=0.126]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.126]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87 average loss: 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  87%|████████▋ | 87/100 [36:21<05:26, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 87 current AUC: 0.9955 current accuracy: 0.9255 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0091]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:21,  1.39it/s, train_loss=0.0091]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:21,  1.39it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.27it/s, train_loss=0.0179]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.27it/s, train_loss=0.00352]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:21,  1.32it/s, train_loss=0.00352]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:21,  1.32it/s, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.00438]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.26it/s, train_loss=0.00674]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:21,  1.24it/s, train_loss=0.00674]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.24it/s, train_loss=0.0185] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.0185]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.0089]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.24it/s, train_loss=0.0089]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.24it/s, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.0324]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.22it/s, train_loss=0.00808]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.00808]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0259] \u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.0259]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.033] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.033]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.22it/s, train_loss=0.0131]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.22it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0127]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.24it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.0569]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0238]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0205]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.011] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.041]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.025] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.025]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.00915]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.25it/s, train_loss=0.00915]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.25it/s, train_loss=0.00305]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00305]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0112] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.0112]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.00735]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.00735]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.134]  \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.0743]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0743]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.021] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0097]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.0097]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88 average loss: 0.0236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|████████▊ | 88/100 [36:46<05:00, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 88 current AUC: 0.9963 current accuracy: 0.9317 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00309]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.00309]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.00368]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.00368]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.0173] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.0173]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.00737]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.00737]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0219] \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.25it/s, train_loss=0.0219]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.25it/s, train_loss=0.0283]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.28it/s, train_loss=0.0283]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.28it/s, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.00728]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0077] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.25it/s, train_loss=0.0077]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.00281]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.26it/s, train_loss=0.00281]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.26it/s, train_loss=0.00599]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.00599]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.00342]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.00342]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.29it/s, train_loss=0.00391]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.00391]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.28it/s, train_loss=0.104]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.0783]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.00727]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0988] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0988]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.24it/s, train_loss=0.0424]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.00184]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.00499]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.0687] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.25it/s, train_loss=0.0687]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.25it/s, train_loss=0.038] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.038]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.00537]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.00537]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.00347]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.00347]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.00551]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.00551]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.00311]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.00311]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.0673] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.0673]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.20it/s, train_loss=0.0765]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.0765]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.21it/s, train_loss=0.0831]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.0831]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89 average loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  89%|████████▉ | 89/100 [37:11<04:35, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 89 current AUC: 0.9933 current accuracy: 0.9193 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.19it/s, train_loss=0.0101]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.19it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.00709]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.00709]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.0244] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.19it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.19it/s, train_loss=0.0347]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.0347]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.22it/s, train_loss=0.0155]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.22it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.21it/s, train_loss=0.0114]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.21it/s, train_loss=0.109] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.21it/s, train_loss=0.109]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.21it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0381]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:16,  1.25it/s, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.00597]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.24it/s, train_loss=0.00743]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.24it/s, train_loss=0.0147] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0147]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.00787]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.00787]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.00526]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.00526]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:12,  1.26it/s, train_loss=0.00595]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.00595]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.0965] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.0965]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.021] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.26it/s, train_loss=0.021]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.26it/s, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.28it/s, train_loss=0.0049]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.29it/s, train_loss=0.0049]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.29it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.28it/s, train_loss=0.0345]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.28it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.29it/s, train_loss=0.00985]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.00985]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0429] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.26it/s, train_loss=0.0429]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.22it/s, train_loss=0.0281]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.22it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.0128]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.23it/s, train_loss=0.0312] \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0312]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.00977]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.00977]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90 average loss: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 90/100 [37:36<04:10, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 90 current AUC: 0.9934 current accuracy: 0.9068 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00578]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.00578]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.0137] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.0137]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.00197]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.00197]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.0054] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0054]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.00541]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.241]  \u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.30it/s, train_loss=0.241]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.30it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.0297]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.00691]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.00691]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.00194]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.00194]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.0088] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.0088]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.00518]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.00518]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.0732] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0732]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0469]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.24it/s, train_loss=0.0469]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.24it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.0142]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.23it/s, train_loss=0.0046]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0046]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.00372]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.00372]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0287] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0287]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.24it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.24it/s, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0194]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.22it/s, train_loss=0.00279]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s, train_loss=0.0211] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.22it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.22it/s, train_loss=0.00225]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.00225]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.017]  \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0316]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.26it/s, train_loss=0.0406]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.0406]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0846]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.0846]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.27it/s, train_loss=0.0245]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s, train_loss=0.06]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.06]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.25it/s, train_loss=0.00682]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.00682]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.27it/s, train_loss=0.112]  \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.112]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91 average loss: 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  91%|█████████ | 91/100 [38:01<03:45, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 91 current AUC: 0.9922 current accuracy: 0.9006 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.00377]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.20it/s, train_loss=0.00377]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.20it/s, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.00578]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.00531]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.00531]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.00474]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.00474]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.00689]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.19it/s, train_loss=0.00689]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.19it/s, train_loss=0.0258] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.21it/s, train_loss=0.0258]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.21it/s, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.23it/s, train_loss=0.0153]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.23it/s, train_loss=0.00705]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.00705]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.0165] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0165]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.25it/s, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0577]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.0692]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.063] \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.063]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.192]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.27it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.0779]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.28it/s, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.25it/s, train_loss=0.00839]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.25it/s, train_loss=0.00606]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.25it/s, train_loss=0.00606]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.25it/s, train_loss=0.00231]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.00231]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.031]  \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.052]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.052]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.24it/s, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.25it/s, train_loss=0.0551]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.0563]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.00608]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.00608]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.0507] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.0507]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0253]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.23it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0794]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.25it/s, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.25it/s, train_loss=0.121] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.121]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.20it/s, train_loss=0.0431]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.20it/s, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0274]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.372] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.52it/s, train_loss=0.372]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 average loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  92%|█████████▏| 92/100 [38:26<03:20, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 92 current AUC: 0.9955 current accuracy: 0.9441 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.30it/s, train_loss=0.00491]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.30it/s, train_loss=0.0425] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.28it/s, train_loss=0.0425]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.28it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.27it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.27it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0388]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0504]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.0504]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.27it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.27it/s, train_loss=0.00893]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.00893]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.22]   \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.22]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.00739]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.00739]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.0816] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.24it/s, train_loss=0.0816]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.0229]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.0622]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0622]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.17]  \u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:13,  1.29it/s, train_loss=0.17]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:13,  1.29it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.0227]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.27it/s, train_loss=0.0303]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.0303]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0034]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.0034]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.0159]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.25it/s, train_loss=0.0195]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0204]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0362]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0792]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0792]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0086]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0086]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.0759]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.29it/s, train_loss=0.055] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:18<00:05,  1.30it/s, train_loss=0.055]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.30it/s, train_loss=0.0708]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.27it/s, train_loss=0.0708]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.27it/s, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.27it/s, train_loss=0.0322]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.27it/s, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0126]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.105] \u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.24it/s, train_loss=0.105]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.24it/s, train_loss=0.094]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.094]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.23it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.24it/s, train_loss=0.0474]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0459]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.0459]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93 average loss: 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  93%|█████████▎| 93/100 [38:51<02:54, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 93 current AUC: 0.9968 current accuracy: 0.9379 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0437]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.26it/s, train_loss=0.0437]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.26it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.0244]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0123]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.176] \u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.22it/s, train_loss=0.176]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.22it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.031]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.24it/s, train_loss=0.027]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.027]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.0456]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.0228]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.12]  \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.12]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0171]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.25it/s, train_loss=0.0211]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.25it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0373]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.25it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.25it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.0511]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.25it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.24it/s, train_loss=0.0652]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.24it/s, train_loss=0.129] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.00332]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.24it/s, train_loss=0.00332]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.24it/s, train_loss=0.0106] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.27it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.27it/s, train_loss=0.00696]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.00696]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.00309]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.25it/s, train_loss=0.00309]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.25it/s, train_loss=0.0515] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.0515]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.0129]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0469]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0469]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.28it/s, train_loss=0.00403]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.30it/s, train_loss=0.00403]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.30it/s, train_loss=0.0339] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.30it/s, train_loss=0.0339]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.30it/s, train_loss=0.00747]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.31it/s, train_loss=0.00747]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.31it/s, train_loss=0.0304] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0304]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.28it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.0242]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.57it/s, train_loss=0.0242]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 average loss: 0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  94%|█████████▍| 94/100 [39:15<02:29, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 94 current AUC: 0.9916 current accuracy: 0.8447 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00822]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:22,  1.33it/s, train_loss=0.00822]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:22,  1.33it/s, train_loss=0.0239] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:22,  1.26it/s, train_loss=0.0239]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:22,  1.26it/s, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.00926]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.00401]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.00401]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.045]  \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.25it/s, train_loss=0.045]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.25it/s, train_loss=0.00298]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.24it/s, train_loss=0.00298]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.24it/s, train_loss=0.00907]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.23it/s, train_loss=0.00907]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.23it/s, train_loss=0.00239]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.25it/s, train_loss=0.00239]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.25it/s, train_loss=0.00924]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.27it/s, train_loss=0.00924]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.27it/s, train_loss=0.0585] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0585]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.0168]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.25it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.25it/s, train_loss=0.00884]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.00884]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.00908]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.00908]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.25it/s, train_loss=0.014]  \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.26it/s, train_loss=0.014]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.26it/s, train_loss=0.00722]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.27it/s, train_loss=0.00722]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.27it/s, train_loss=0.00664]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.26it/s, train_loss=0.00664]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.26it/s, train_loss=0.007]  \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.28it/s, train_loss=0.007]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.28it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.223]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.25it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.25it/s, train_loss=0.0146]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.00539]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.00539]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0144] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.24it/s, train_loss=0.0144]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.24it/s, train_loss=0.00577]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.00577]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0445] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.22it/s, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.00477]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.167]  \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.24it/s, train_loss=0.167]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.24it/s, train_loss=0.0647]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.23it/s, train_loss=0.0647]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.23it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.0156]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.258] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.258]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95 average loss: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  95%|█████████▌| 95/100 [39:40<02:04, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 95 current AUC: 0.9951 current accuracy: 0.9068 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00963]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.00963]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.00481]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.00481]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.0223] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.25it/s, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.25it/s, train_loss=0.00458]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.00458]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0817] \u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.0817]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0473]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:18,  1.27it/s, train_loss=0.0249]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:18,  1.27it/s, train_loss=0.129] \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.129]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0094]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:18,  1.22it/s, train_loss=0.0094]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:18,  1.22it/s, train_loss=0.036] \u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.036]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.20it/s, train_loss=0.0376]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.20it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.22it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.22it/s, train_loss=0.0066]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0066]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.165] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.165]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.26it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.017]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.25it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.018]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0425]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.20it/s, train_loss=0.0425]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.20it/s, train_loss=0.00479]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.21it/s, train_loss=0.00479]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.21it/s, train_loss=0.0116] \u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.0116]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.22it/s, train_loss=0.0632]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.22it/s, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.00846]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.25it/s, train_loss=0.0292] \u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0292]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.0541]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.27it/s, train_loss=0.0541]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.27it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0197]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.00383]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.00383]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.25it/s, train_loss=0.0124] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0124]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.22it/s, train_loss=0.0251]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.22it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.24it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.24it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0878]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.207] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.56it/s, train_loss=0.207]\u001b[A\n",
      "                                                                                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 average loss: 0.0397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  96%|█████████▌| 96/100 [40:06<01:39, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 96 current AUC: 0.9962 current accuracy: 0.9068 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.00521]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:25,  1.18it/s, train_loss=0.00521]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:25,  1.18it/s, train_loss=0.0512] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:24,  1.18it/s, train_loss=0.0512]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:24,  1.18it/s, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0256]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.0235]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.0594]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0594]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.23it/s, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.26it/s, train_loss=0.0223]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.26it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.0262]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.22]  \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.26it/s, train_loss=0.22]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.26it/s, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.26it/s, train_loss=0.0666]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.26it/s, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.0137]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.28it/s, train_loss=0.0888]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.27it/s, train_loss=0.0888]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.27it/s, train_loss=0.255] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.27it/s, train_loss=0.255]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.27it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.26it/s, train_loss=0.00528]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.26it/s, train_loss=0.0104] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0104]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.26it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.27it/s, train_loss=0.0187]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.27it/s, train_loss=0.0743]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.25it/s, train_loss=0.0743]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.25it/s, train_loss=0.00693]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.24it/s, train_loss=0.00693]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.24it/s, train_loss=0.0149] \u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0149]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.0182]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.23it/s, train_loss=0.0635]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0635]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.25it/s, train_loss=0.0664]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:07,  1.26it/s, train_loss=0.0664]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:07,  1.26it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.23it/s, train_loss=0.0115]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.23it/s, train_loss=0.104] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.24it/s, train_loss=0.104]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.24it/s, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.27it/s, train_loss=0.0162]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.27it/s, train_loss=0.14]  \u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.14]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.25it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.25it/s, train_loss=0.0193]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.25it/s, train_loss=0.0055]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.27it/s, train_loss=0.0055]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.27it/s, train_loss=0.00749]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.26it/s, train_loss=0.00749]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.26it/s, train_loss=0.0467] \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0467]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.25it/s, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.26it/s, train_loss=0.0552]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.26it/s, train_loss=0.00624]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.58it/s, train_loss=0.00624]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97 average loss: 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  97%|█████████▋| 97/100 [40:30<01:14, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 97 current AUC: 0.9970 current accuracy: 0.9193 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.28it/s, train_loss=0.0108]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.28it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.26it/s, train_loss=0.0198]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.26it/s, train_loss=0.135] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.26it/s, train_loss=0.135]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.26it/s, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.0436]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.27it/s, train_loss=0.00403]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.28it/s, train_loss=0.00403]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.28it/s, train_loss=0.00442]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:19,  1.28it/s, train_loss=0.00442]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:19,  1.28it/s, train_loss=0.00875]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.25it/s, train_loss=0.00875]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.25it/s, train_loss=0.058]  \u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.24it/s, train_loss=0.058]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.24it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0735]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.24it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:07<00:16,  1.30it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.30it/s, train_loss=0.00525]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:15,  1.30it/s, train_loss=0.00525]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:15,  1.30it/s, train_loss=0.0111] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:14,  1.29it/s, train_loss=0.0111]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:14,  1.29it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.0438]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.28it/s, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:10<00:13,  1.29it/s, train_loss=0.0209]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.29it/s, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:11<00:12,  1.29it/s, train_loss=0.0202]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:12,  1.29it/s, train_loss=0.183] \u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:11,  1.28it/s, train_loss=0.183]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:11,  1.28it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.27it/s, train_loss=0.048]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.27it/s, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.26it/s, train_loss=0.0266]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.26it/s, train_loss=0.00836]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.00836]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.23it/s, train_loss=0.0271] \u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:15<00:08,  1.23it/s, train_loss=0.0271]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.23it/s, train_loss=0.0119]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.23it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0662]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.106] \u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.23it/s, train_loss=0.106]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.23it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.0136]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.22it/s, train_loss=0.00841]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:19<00:04,  1.23it/s, train_loss=0.00841]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.23it/s, train_loss=0.0107] \u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:04,  1.23it/s, train_loss=0.0107]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:04,  1.23it/s, train_loss=0.012] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.21it/s, train_loss=0.012]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.21it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.21it/s, train_loss=0.0192]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.21it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.21it/s, train_loss=0.00506]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.21it/s, train_loss=0.00443]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:23<00:00,  1.23it/s, train_loss=0.00443]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.23it/s, train_loss=0.0375] \u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.53it/s, train_loss=0.0375]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98 average loss: 0.0342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  98%|█████████▊| 98/100 [40:55<00:49, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 98 current AUC: 0.9976 current accuracy: 0.9317 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0679]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.27it/s, train_loss=0.0679]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.27it/s, train_loss=0.016] \u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.25it/s, train_loss=0.016]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.25it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:22,  1.22it/s, train_loss=0.0102]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:22,  1.22it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:21,  1.25it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:03<00:20,  1.27it/s, train_loss=0.0236]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:20,  1.27it/s, train_loss=0.00269]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:04<00:20,  1.25it/s, train_loss=0.00269]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:20,  1.25it/s, train_loss=0.00272]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:19,  1.26it/s, train_loss=0.00272]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:19,  1.26it/s, train_loss=0.00796]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.27it/s, train_loss=0.00796]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.27it/s, train_loss=0.0212] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.25it/s, train_loss=0.0212]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.25it/s, train_loss=0.0593]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.0593]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:17,  1.23it/s, train_loss=0.00514]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:08<00:16,  1.24it/s, train_loss=0.00514]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.24it/s, train_loss=0.0176] \u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.26it/s, train_loss=0.0176]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.26it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.24it/s, train_loss=0.0265]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.24it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:13,  1.23it/s, train_loss=0.0106]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:13,  1.23it/s, train_loss=0.0365]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.0365]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.23it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:12<00:12,  1.22it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.22it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.0445]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.23it/s, train_loss=0.0243]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.23it/s, train_loss=0.00244]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.00244]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.00606]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.00606]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0132] \u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:16<00:08,  1.24it/s, train_loss=0.0132]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0075]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:17<00:07,  1.25it/s, train_loss=0.0075]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.25it/s, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.26it/s, train_loss=0.00375]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.26it/s, train_loss=0.0157] \u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.23it/s, train_loss=0.0157]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.23it/s, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.0181]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00732]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:20<00:03,  1.28it/s, train_loss=0.00732]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.28it/s, train_loss=0.0105] \u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.26it/s, train_loss=0.0105]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.26it/s, train_loss=0.00557]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.23it/s, train_loss=0.00557]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.23it/s, train_loss=0.051]  \u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.22it/s, train_loss=0.051]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.22it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.24it/s, train_loss=0.00449]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.54it/s, train_loss=0.00449]\u001b[A\n",
      "                                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 average loss: 0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  99%|█████████▉| 99/100 [41:20<00:24, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 99 current AUC: 0.9973 current accuracy: 0.9379 best AUC: 0.9993 at epoch: 84\n",
      "----------\n",
      "epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Training Batches:   0%|          | 0/31 [00:00<?, ?it/s, train_loss=0.0811]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:00<00:23,  1.25it/s, train_loss=0.0811]\u001b[A\n",
      "Training Batches:   3%|▎         | 1/31 [00:01<00:23,  1.25it/s, train_loss=0.00468]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:01<00:23,  1.22it/s, train_loss=0.00468]\u001b[A\n",
      "Training Batches:   6%|▋         | 2/31 [00:02<00:23,  1.22it/s, train_loss=0.0269] \u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:02<00:23,  1.21it/s, train_loss=0.0269]\u001b[A\n",
      "Training Batches:  10%|▉         | 3/31 [00:03<00:23,  1.21it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:03<00:22,  1.21it/s, train_loss=0.0118]\u001b[A\n",
      "Training Batches:  13%|█▎        | 4/31 [00:04<00:22,  1.21it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:04<00:21,  1.20it/s, train_loss=0.0143]\u001b[A\n",
      "Training Batches:  16%|█▌        | 5/31 [00:05<00:21,  1.20it/s, train_loss=0.00519]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:21,  1.18it/s, train_loss=0.00519]\u001b[A\n",
      "Training Batches:  19%|█▉        | 6/31 [00:05<00:21,  1.18it/s, train_loss=0.0177] \u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:05<00:20,  1.19it/s, train_loss=0.0177]\u001b[A\n",
      "Training Batches:  23%|██▎       | 7/31 [00:06<00:20,  1.19it/s, train_loss=0.00562]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:06<00:18,  1.22it/s, train_loss=0.00562]\u001b[A\n",
      "Training Batches:  26%|██▌       | 8/31 [00:07<00:18,  1.22it/s, train_loss=0.0624] \u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:07<00:17,  1.23it/s, train_loss=0.0624]\u001b[A\n",
      "Training Batches:  29%|██▉       | 9/31 [00:08<00:17,  1.23it/s, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:08<00:16,  1.24it/s, train_loss=0.0224]\u001b[A\n",
      "Training Batches:  32%|███▏      | 10/31 [00:09<00:16,  1.24it/s, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0698]\u001b[A\n",
      "Training Batches:  35%|███▌      | 11/31 [00:09<00:16,  1.23it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:09<00:15,  1.23it/s, train_loss=0.0109]\u001b[A\n",
      "Training Batches:  39%|███▊      | 12/31 [00:10<00:15,  1.23it/s, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:10<00:14,  1.21it/s, train_loss=0.00678]\u001b[A\n",
      "Training Batches:  42%|████▏     | 13/31 [00:11<00:14,  1.21it/s, train_loss=0.0966] \u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:11<00:14,  1.21it/s, train_loss=0.0966]\u001b[A\n",
      "Training Batches:  45%|████▌     | 14/31 [00:12<00:14,  1.21it/s, train_loss=0.011] \u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:12<00:13,  1.22it/s, train_loss=0.011]\u001b[A\n",
      "Training Batches:  48%|████▊     | 15/31 [00:13<00:13,  1.22it/s, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.0257]\u001b[A\n",
      "Training Batches:  52%|█████▏    | 16/31 [00:13<00:12,  1.23it/s, train_loss=0.153] \u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:13<00:11,  1.23it/s, train_loss=0.153]\u001b[A\n",
      "Training Batches:  55%|█████▍    | 17/31 [00:14<00:11,  1.23it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:14<00:10,  1.22it/s, train_loss=0.0135]\u001b[A\n",
      "Training Batches:  58%|█████▊    | 18/31 [00:15<00:10,  1.22it/s, train_loss=0.0085]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:15<00:09,  1.22it/s, train_loss=0.0085]\u001b[A\n",
      "Training Batches:  61%|██████▏   | 19/31 [00:16<00:09,  1.22it/s, train_loss=0.0417]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:16<00:08,  1.23it/s, train_loss=0.0417]\u001b[A\n",
      "Training Batches:  65%|██████▍   | 20/31 [00:17<00:08,  1.23it/s, train_loss=0.0578]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:17<00:08,  1.24it/s, train_loss=0.0578]\u001b[A\n",
      "Training Batches:  68%|██████▊   | 21/31 [00:18<00:08,  1.24it/s, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s, train_loss=0.0248]\u001b[A\n",
      "Training Batches:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:18<00:06,  1.22it/s, train_loss=0.0346]\u001b[A\n",
      "Training Batches:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:19<00:05,  1.24it/s, train_loss=0.00297]\u001b[A\n",
      "Training Batches:  77%|███████▋  | 24/31 [00:20<00:05,  1.24it/s, train_loss=0.00659]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:20<00:04,  1.24it/s, train_loss=0.00659]\u001b[A\n",
      "Training Batches:  81%|████████  | 25/31 [00:21<00:04,  1.24it/s, train_loss=0.00174]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.00174]\u001b[A\n",
      "Training Batches:  84%|████████▍ | 26/31 [00:21<00:03,  1.25it/s, train_loss=0.00287]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:21<00:03,  1.28it/s, train_loss=0.00287]\u001b[A\n",
      "Training Batches:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s, train_loss=0.00922]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:22<00:02,  1.28it/s, train_loss=0.00922]\u001b[A\n",
      "Training Batches:  90%|█████████ | 28/31 [00:23<00:02,  1.28it/s, train_loss=0.00347]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:23<00:01,  1.27it/s, train_loss=0.00347]\u001b[A\n",
      "Training Batches:  94%|█████████▎| 29/31 [00:24<00:01,  1.27it/s, train_loss=0.134]  \u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.134]\u001b[A\n",
      "Training Batches:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s, train_loss=0.0801]\u001b[A\n",
      "Training Batches: 100%|██████████| 31/31 [00:24<00:00,  1.59it/s, train_loss=0.0801]\u001b[A\n",
      "                                                                                    \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 average loss: 0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 100/100 [41:46<00:00, 25.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "current epoch: 100 current AUC: 0.9978 current accuracy: 0.9006 best AUC: 0.9993 at epoch: 84\n",
      "train completed, best_metric: 0.9993 at epoch: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model_3d.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    batch_iter = tqdm(train_loader, desc=\"Training Batches\", leave=False)\n",
    "    \n",
    "    for batch_data in batch_iter:\n",
    "        step += 1\n",
    "        images, labels = batch_data['images'].to(device), batch_data['label'][:, 0].type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_3d(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        batch_iter.set_postfix(train_loss=loss.item())\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model_3d.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = (\n",
    "                    val_data['images'].to(device),\n",
    "                    val_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "                )\n",
    "                y_pred = torch.cat([y_pred, model_3d(val_images)], dim=0)\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "            print('1')\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            if result > best_metric:\n",
    "                best_metric = result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model_3d.state_dict(), os.path.join(root_dir, \"best_metric_model_ACS.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\n",
    "                f\" best AUC: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba3b85cf-d1ee-4a66-9bd1-b6d0d587d803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587932b-f721-46e4-b09e-d0c742e57b66",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d793d3c7-7da7-48cb-9f85-b50d34cc0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model_ACS.pth\"), weights_only=True))\n",
    "model_3d.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "example = []\n",
    "example_preds = []\n",
    "example_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data['images'].to(device),\n",
    "            test_data['label'][:, 0].type(torch.LongTensor).to(device),\n",
    "        )\n",
    "        pred = model_3d(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "        if len(example) < 10:\n",
    "            example.append(test_images)\n",
    "            example_preds.append(pred)\n",
    "            example_labels.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd00169-64b9-45f8-909b-4964b5b57bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        69\n",
      "           1     0.5773    0.8235    0.6788        68\n",
      "           2     0.6667    0.5217    0.5854        69\n",
      "           3     0.5775    0.6308    0.6029        65\n",
      "           4     0.6333    0.5846    0.6080        65\n",
      "           5     0.9839    0.9242    0.9531        66\n",
      "           6     1.0000    1.0000    1.0000        28\n",
      "           7     1.0000    1.0000    1.0000        21\n",
      "           8     1.0000    1.0000    1.0000        21\n",
      "           9     0.9836    0.8696    0.9231        69\n",
      "          10     0.9697    0.9275    0.9481        69\n",
      "\n",
      "    accuracy                         0.8115       610\n",
      "   macro avg     0.8538    0.8438    0.8454       610\n",
      "weighted avg     0.8241    0.8115    0.8136       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=info['label'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e692de3-3486-4640-a0b9-d1d51baa2f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
